[
  {
    "objectID": "3D. Split out validation.html",
    "href": "3D. Split out validation.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "ds = wds.WebDataset(utils.shard_glob('../wolnelektury-wds2/wolnelektury-eqvad-000000.tar.gz'))\n\n\nfor s in ds: break\ns.keys()\n\ndict_keys(['__key__', '__url__', 'spk_emb.npy', 'vad.npy'])\n\n\n\nsplit_dataset('../wolnelektury-wds2/wolnelektury-eqvad-stoks-*.tar.gz', '../wolnelektury-wds2/validation-eqvad')\n\n['../wolnelektury-wds2/wolnelektury-eqvad-stoks-000014.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000008.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000010.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000004.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000011.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000003.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000002.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000007.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000013.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000005.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000009.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000000.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000006.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000012.tar.gz', '../wolnelektury-wds2/wolnelektury-eqvad-stoks-000001.tar.gz']\n{'../wolnelektury-wds2/validation-eqvad': &lt;webdataset.writer.TarWriter object&gt;}\nLooking for 520 samples...\n\n\n\n\n\n\n\n    \n      \n      100.00% [520/520 00:01&lt;00:00]\n    \n    \n\n\n\nsplit_dataset('whisperspeech-s2a-512c-tts-r/*.tar.gz', 's2a-dim64-ttsr-valfix')\n\n# writing s2a-dim64-ttsr-valfix/train-000000.tar.gz 0 0.0 GB 00350 00:13&lt;00:00]\n# writing s2a-dim64-ttsr-valfix/train-000001.tar.gz 400 0.0 GB 4000&lt;04:44]]\n# writing s2a-dim64-ttsr-valfix/train-000002.tar.gz 400 0.0 GB 8000&lt;02:47]\n# writing s2a-dim64-ttsr-valfix/train-000003.tar.gz 400 0.0 GB 12000&lt;02:12]\n# writing s2a-dim64-ttsr-valfix/train-000004.tar.gz 400 0.0 GB 16001&lt;02:01]\n# writing s2a-dim64-ttsr-valfix/train-000005.tar.gz 400 0.0 GB 20001&lt;01:45]\n# writing s2a-dim64-ttsr-valfix/train-000006.tar.gz 400 0.0 GB 24001&lt;01:39]\n# writing s2a-dim64-ttsr-valfix/train-000007.tar.gz 400 0.0 GB 28001&lt;01:35]\n# writing s2a-dim64-ttsr-valfix/train-000008.tar.gz 400 0.0 GB 32001&lt;01:32]\n# writing s2a-dim64-ttsr-valfix/train-000009.tar.gz 400 0.0 GB 36001&lt;01:26]\n# writing s2a-dim64-ttsr-valfix/train-000010.tar.gz 400 0.0 GB 40002&lt;01:24]\n# writing s2a-dim64-ttsr-valfix/train-000011.tar.gz 400 0.0 GB 44002&lt;01:22]\n# writing s2a-dim64-ttsr-valfix/train-000012.tar.gz 400 0.0 GB 48002&lt;01:21]\n# writing s2a-dim64-ttsr-valfix/train-000013.tar.gz 400 0.0 GB 52002&lt;01:19]\n# writing s2a-dim64-ttsr-valfix/train-000014.tar.gz 400 0.0 GB 56002&lt;01:18]\n# writing s2a-dim64-ttsr-valfix/train-000015.tar.gz 400 0.0 GB 60002&lt;01:17]\n# writing s2a-dim64-ttsr-valfix/train-000016.tar.gz 400 0.0 GB 64003&lt;01:16]\n# writing s2a-dim64-ttsr-valfix/train-000017.tar.gz 400 0.0 GB 68003&lt;01:15]\n# writing s2a-dim64-ttsr-valfix/train-000018.tar.gz 400 0.0 GB 72003&lt;01:15]\n# writing s2a-dim64-ttsr-valfix/train-000019.tar.gz 400 0.0 GB 76003&lt;01:14]\n# writing s2a-dim64-ttsr-valfix/train-000020.tar.gz 400 0.0 GB 80003&lt;01:13]\n# writing s2a-dim64-ttsr-valfix/train-000021.tar.gz 400 0.0 GB 84003&lt;01:13]\n# writing s2a-dim64-ttsr-valfix/train-000022.tar.gz 400 0.0 GB 88004&lt;01:12]\n# writing s2a-dim64-ttsr-valfix/train-000023.tar.gz 400 0.0 GB 92004&lt;01:11]\n# writing s2a-dim64-ttsr-valfix/train-000024.tar.gz 400 0.0 GB 96004&lt;01:11]\n# writing s2a-dim64-ttsr-valfix/train-000025.tar.gz 400 0.0 GB 10000&lt;01:10]\n# writing s2a-dim64-ttsr-valfix/train-000026.tar.gz 400 0.0 GB 104004&lt;01:10]\n# writing s2a-dim64-ttsr-valfix/train-000027.tar.gz 400 0.0 GB 108005&lt;01:09]\n# writing s2a-dim64-ttsr-valfix/train-000028.tar.gz 400 0.0 GB 112005&lt;01:09]\n# writing s2a-dim64-ttsr-valfix/train-000029.tar.gz 400 0.0 GB 116005&lt;01:09]\n# writing s2a-dim64-ttsr-valfix/train-000030.tar.gz 400 0.0 GB 12000\n# writing s2a-dim64-ttsr-valfix/train-000031.tar.gz 400 0.0 GB 124005&lt;01:08]\n# writing s2a-dim64-ttsr-valfix/train-000032.tar.gz 400 0.0 GB 128005&lt;01:08]\n# writing s2a-dim64-ttsr-valfix/train-000033.tar.gz 400 0.0 GB 132005&lt;01:07]\n# writing s2a-dim64-ttsr-valfix/train-000034.tar.gz 400 0.0 GB 136006&lt;01:07]\n# writing s2a-dim64-ttsr-valfix/train-000035.tar.gz 400 0.0 GB 140006&lt;01:07]\n# writing s2a-dim64-ttsr-valfix/train-000036.tar.gz 400 0.0 GB 144006&lt;01:07]\n# writing s2a-dim64-ttsr-valfix/train-000037.tar.gz 400 0.0 GB 148006&lt;01:06]\n# writing s2a-dim64-ttsr-valfix/train-000038.tar.gz 400 0.0 GB 152006&lt;01:06]\n# writing s2a-dim64-ttsr-valfix/train-000039.tar.gz 400 0.0 GB 156007&lt;01:06]\n# writing s2a-dim64-ttsr-valfix/train-000040.tar.gz 400 0.0 GB 160007&lt;01:05]\n# writing s2a-dim64-ttsr-valfix/train-000041.tar.gz 400 0.0 GB 1640007&lt;01:05]\n# writing s2a-dim64-ttsr-valfix/train-000042.tar.gz 400 0.0 GB 1680007&lt;01:05]\n# writing s2a-dim64-ttsr-valfix/train-000043.tar.gz 400 0.0 GB 1720007&lt;01:04]\n# writing s2a-dim64-ttsr-valfix/train-000044.tar.gz 400 0.0 GB 17600\n# writing s2a-dim64-ttsr-valfix/train-000045.tar.gz 400 0.0 GB 1800008&lt;01:04]\n# writing s2a-dim64-ttsr-valfix/train-000046.tar.gz 400 0.0 GB 1840008&lt;01:04]\n# writing s2a-dim64-ttsr-valfix/train-000047.tar.gz 400 0.0 GB 1880008&lt;01:03]\n# writing s2a-dim64-ttsr-valfix/train-000048.tar.gz 400 0.0 GB 1920008&lt;01:03]\n# writing s2a-dim64-ttsr-valfix/train-000049.tar.gz 400 0.0 GB 1960008&lt;01:03]\n# writing s2a-dim64-ttsr-valfix/train-000050.tar.gz 400 0.0 GB 2000009&lt;01:03]\n# writing s2a-dim64-ttsr-valfix/train-000051.tar.gz 400 0.0 GB 2040009&lt;01:02]\n# writing s2a-dim64-ttsr-valfix/train-000052.tar.gz 400 0.0 GB 2080009&lt;01:02]\n# writing s2a-dim64-ttsr-valfix/train-000053.tar.gz 400 0.0 GB 2120009&lt;01:02]\n# writing s2a-dim64-ttsr-valfix/train-000054.tar.gz 400 0.0 GB 21600\n# writing s2a-dim64-ttsr-valfix/train-000055.tar.gz 400 0.0 GB 2200009&lt;01:02]\n# writing s2a-dim64-ttsr-valfix/train-000056.tar.gz 400 0.0 GB 2240009&lt;01:01]\n# writing s2a-dim64-ttsr-valfix/train-000057.tar.gz 400 0.0 GB 2280010&lt;01:01]\n# writing s2a-dim64-ttsr-valfix/train-000058.tar.gz 400 0.0 GB 2320010&lt;01:01]\n# writing s2a-dim64-ttsr-valfix/train-000059.tar.gz 400 0.0 GB 2360010&lt;01:01]\n# writing s2a-dim64-ttsr-valfix/train-000060.tar.gz 400 0.0 GB 2400010&lt;01:00]\n# writing s2a-dim64-ttsr-valfix/train-000061.tar.gz 400 0.0 GB 2440010&lt;01:00]\n# writing s2a-dim64-ttsr-valfix/train-000062.tar.gz 400 0.0 GB 2480011&lt;01:00]\n# writing s2a-dim64-ttsr-valfix/train-000063.tar.gz 400 0.0 GB 2520011&lt;01:00]\n# writing s2a-dim64-ttsr-valfix/train-000064.tar.gz 400 0.0 GB 25600\n# writing s2a-dim64-ttsr-valfix/train-000065.tar.gz 400 0.0 GB 2600011&lt;00:59]\n# writing s2a-dim64-ttsr-valfix/train-000066.tar.gz 400 0.0 GB 2640011&lt;00:59]\n# writing s2a-dim64-ttsr-valfix/train-000067.tar.gz 400 0.0 GB 2680011&lt;00:59]\n# writing s2a-dim64-ttsr-valfix/train-000068.tar.gz 400 0.0 GB 2720012&lt;00:59]\n# writing s2a-dim64-ttsr-valfix/train-000069.tar.gz 400 0.0 GB 2760012&lt;00:59]\n# writing s2a-dim64-ttsr-valfix/train-000070.tar.gz 400 0.0 GB 2800012&lt;00:58]\n# writing s2a-dim64-ttsr-valfix/train-000071.tar.gz 400 0.0 GB 2840012&lt;00:58]\n# writing s2a-dim64-ttsr-valfix/train-000072.tar.gz 400 0.0 GB 2880012&lt;00:58]\n# writing s2a-dim64-ttsr-valfix/train-000073.tar.gz 400 0.0 GB 2920013&lt;00:58]\n# writing s2a-dim64-ttsr-valfix/train-000074.tar.gz 400 0.0 GB 29600\n# writing s2a-dim64-ttsr-valfix/train-000075.tar.gz 400 0.0 GB 3000013&lt;00:57]\n# writing s2a-dim64-ttsr-valfix/train-000076.tar.gz 400 0.0 GB 3040013&lt;00:57]\n# writing s2a-dim64-ttsr-valfix/train-000077.tar.gz 400 0.0 GB 3080013&lt;00:57]\n# writing s2a-dim64-ttsr-valfix/train-000078.tar.gz 400 0.0 GB 3120013&lt;00:57]\n# writing s2a-dim64-ttsr-valfix/train-000079.tar.gz 400 0.0 GB 3160014&lt;00:56]\n# writing s2a-dim64-ttsr-valfix/train-000080.tar.gz 400 0.0 GB 3200014&lt;00:56]\n# writing s2a-dim64-ttsr-valfix/train-000081.tar.gz 400 0.0 GB 3240014&lt;00:56]\n# writing s2a-dim64-ttsr-valfix/train-000082.tar.gz 400 0.0 GB 3280014&lt;00:56]\n# writing s2a-dim64-ttsr-valfix/train-000083.tar.gz 400 0.0 GB 33200\n# writing s2a-dim64-ttsr-valfix/train-000084.tar.gz 400 0.0 GB 3360014&lt;00:56]\n# writing s2a-dim64-ttsr-valfix/train-000085.tar.gz 400 0.0 GB 3400014&lt;00:55]\n# writing s2a-dim64-ttsr-valfix/train-000086.tar.gz 400 0.0 GB 3440015&lt;00:55]\n# writing s2a-dim64-ttsr-valfix/train-000087.tar.gz 400 0.0 GB 3480015&lt;00:55]\n# writing s2a-dim64-ttsr-valfix/train-000088.tar.gz 400 0.0 GB 3520015&lt;00:55]\n# writing s2a-dim64-ttsr-valfix/train-000089.tar.gz 400 0.0 GB 3560015&lt;00:54]\n# writing s2a-dim64-ttsr-valfix/train-000090.tar.gz 400 0.0 GB 3600015&lt;00:54]\n# writing s2a-dim64-ttsr-valfix/train-000091.tar.gz 400 0.0 GB 36400\n# writing s2a-dim64-ttsr-valfix/train-000092.tar.gz 400 0.0 GB 3680016&lt;00:54]\n# writing s2a-dim64-ttsr-valfix/train-000093.tar.gz 400 0.0 GB 3720016&lt;00:54]\n# writing s2a-dim64-ttsr-valfix/train-000094.tar.gz 400 0.0 GB 3760016&lt;00:53]\n# writing s2a-dim64-ttsr-valfix/train-000095.tar.gz 400 0.0 GB 3800016&lt;00:53]\n# writing s2a-dim64-ttsr-valfix/train-000096.tar.gz 400 0.0 GB 3840016&lt;00:53]\n# writing s2a-dim64-ttsr-valfix/train-000097.tar.gz 400 0.0 GB 3880017&lt;00:53]\n# writing s2a-dim64-ttsr-valfix/train-000098.tar.gz 400 0.0 GB 3920017&lt;00:52]\n# writing s2a-dim64-ttsr-valfix/train-000099.tar.gz 400 0.0 GB 3960017&lt;00:52]\n# writing s2a-dim64-ttsr-valfix/train-000100.tar.gz 400 0.0 GB 40000\n# writing s2a-dim64-ttsr-valfix/train-000101.tar.gz 400 0.0 GB 4040017&lt;00:52]\n# writing s2a-dim64-ttsr-valfix/train-000102.tar.gz 400 0.0 GB 4080017&lt;00:52]\n# writing s2a-dim64-ttsr-valfix/train-000103.tar.gz 400 0.0 GB 4120018&lt;00:52]\n# writing s2a-dim64-ttsr-valfix/train-000104.tar.gz 400 0.0 GB 4160018&lt;00:51]\n# writing s2a-dim64-ttsr-valfix/train-000105.tar.gz 400 0.0 GB 4200018&lt;00:51]\n# writing s2a-dim64-ttsr-valfix/train-000106.tar.gz 400 0.0 GB 4240018&lt;00:51]\n# writing s2a-dim64-ttsr-valfix/train-000107.tar.gz 400 0.0 GB 4280018&lt;00:51]\n# writing s2a-dim64-ttsr-valfix/train-000108.tar.gz 400 0.0 GB 43200\n# writing s2a-dim64-ttsr-valfix/train-000109.tar.gz 400 0.0 GB 4360019&lt;00:51]\n# writing s2a-dim64-ttsr-valfix/train-000110.tar.gz 400 0.0 GB 4400019&lt;00:50]\n# writing s2a-dim64-ttsr-valfix/train-000111.tar.gz 400 0.0 GB 4440019&lt;00:50]\n# writing s2a-dim64-ttsr-valfix/train-000112.tar.gz 400 0.0 GB 4480019&lt;00:50]\n# writing s2a-dim64-ttsr-valfix/train-000113.tar.gz 400 0.0 GB 4520019&lt;00:50]\n# writing s2a-dim64-ttsr-valfix/train-000114.tar.gz 400 0.0 GB 4560020&lt;00:50]\n# writing s2a-dim64-ttsr-valfix/train-000115.tar.gz 400 0.0 GB 4600020&lt;00:49]\n# writing s2a-dim64-ttsr-valfix/train-000116.tar.gz 400 0.0 GB 4640020&lt;00:49]\n# writing s2a-dim64-ttsr-valfix/train-000117.tar.gz 400 0.0 GB 46800\n# writing s2a-dim64-ttsr-valfix/train-000118.tar.gz 400 0.0 GB 4720020&lt;00:49]\n# writing s2a-dim64-ttsr-valfix/train-000119.tar.gz 400 0.0 GB 4760020&lt;00:49]\n# writing s2a-dim64-ttsr-valfix/train-000120.tar.gz 400 0.0 GB 4800021&lt;00:49]\n# writing s2a-dim64-ttsr-valfix/train-000121.tar.gz 400 0.0 GB 4840021&lt;00:48]\n# writing s2a-dim64-ttsr-valfix/train-000122.tar.gz 400 0.0 GB 4880021&lt;00:48]\n# writing s2a-dim64-ttsr-valfix/train-000123.tar.gz 400 0.0 GB 4920021&lt;00:48]\n# writing s2a-dim64-ttsr-valfix/train-000124.tar.gz 400 0.0 GB 4960021&lt;00:48]\n# writing s2a-dim64-ttsr-valfix/train-000125.tar.gz 400 0.0 GB 50000\n# writing s2a-dim64-ttsr-valfix/train-000126.tar.gz 400 0.0 GB 5040022&lt;00:48]\n# writing s2a-dim64-ttsr-valfix/train-000127.tar.gz 400 0.0 GB 5080022&lt;00:47]\n# writing s2a-dim64-ttsr-valfix/train-000128.tar.gz 400 0.0 GB 5120022&lt;00:47]\n# writing s2a-dim64-ttsr-valfix/train-000129.tar.gz 400 0.0 GB 5160022&lt;00:47]\n# writing s2a-dim64-ttsr-valfix/train-000130.tar.gz 400 0.0 GB 5200022&lt;00:47]\n# writing s2a-dim64-ttsr-valfix/train-000131.tar.gz 400 0.0 GB 5240022&lt;00:47]\n# writing s2a-dim64-ttsr-valfix/train-000132.tar.gz 400 0.0 GB 5280023&lt;00:46]\n# writing s2a-dim64-ttsr-valfix/train-000133.tar.gz 400 0.0 GB 53200\n# writing s2a-dim64-ttsr-valfix/train-000134.tar.gz 400 0.0 GB 5360023&lt;00:46]\n# writing s2a-dim64-ttsr-valfix/train-000135.tar.gz 400 0.0 GB 5400023&lt;00:46]\n# writing s2a-dim64-ttsr-valfix/train-000136.tar.gz 400 0.0 GB 5440023&lt;00:46]\n# writing s2a-dim64-ttsr-valfix/train-000137.tar.gz 400 0.0 GB 5480023&lt;00:46]\n# writing s2a-dim64-ttsr-valfix/train-000138.tar.gz 400 0.0 GB 5520024&lt;00:45]\n# writing s2a-dim64-ttsr-valfix/train-000139.tar.gz 400 0.0 GB 5560024&lt;00:45]\n# writing s2a-dim64-ttsr-valfix/train-000140.tar.gz 400 0.0 GB 5600024&lt;00:45]\n# writing s2a-dim64-ttsr-valfix/train-000141.tar.gz 400 0.0 GB 5640024&lt;00:45]\n# writing s2a-dim64-ttsr-valfix/train-000142.tar.gz 400 0.0 GB 56800\n# writing s2a-dim64-ttsr-valfix/train-000143.tar.gz 400 0.0 GB 5720024&lt;00:45]\n# writing s2a-dim64-ttsr-valfix/train-000144.tar.gz 400 0.0 GB 5760025&lt;00:44]\n# writing s2a-dim64-ttsr-valfix/train-000145.tar.gz 400 0.0 GB 5800025&lt;00:44]\n# writing s2a-dim64-ttsr-valfix/train-000146.tar.gz 400 0.0 GB 5840025&lt;00:44]\n# writing s2a-dim64-ttsr-valfix/train-000147.tar.gz 400 0.0 GB 5880025&lt;00:44]\n# writing s2a-dim64-ttsr-valfix/train-000148.tar.gz 400 0.0 GB 5920025&lt;00:44]\n# writing s2a-dim64-ttsr-valfix/train-000149.tar.gz 400 0.0 GB 5960026&lt;00:43]\n# writing s2a-dim64-ttsr-valfix/train-000150.tar.gz 400 0.0 GB 6000026&lt;00:43]\n# writing s2a-dim64-ttsr-valfix/train-000151.tar.gz 400 0.0 GB 60400\n# writing s2a-dim64-ttsr-valfix/train-000152.tar.gz 400 0.0 GB 6080026&lt;00:43]\n# writing s2a-dim64-ttsr-valfix/train-000153.tar.gz 400 0.0 GB 6120026&lt;00:43]\n# writing s2a-dim64-ttsr-valfix/train-000154.tar.gz 400 0.0 GB 6160026&lt;00:43]\n# writing s2a-dim64-ttsr-valfix/train-000155.tar.gz 400 0.0 GB 6200027&lt;00:42]\n# writing s2a-dim64-ttsr-valfix/train-000156.tar.gz 400 0.0 GB 6240027&lt;00:42]\n# writing s2a-dim64-ttsr-valfix/train-000157.tar.gz 400 0.0 GB 6280027&lt;00:42]\n# writing s2a-dim64-ttsr-valfix/train-000158.tar.gz 400 0.0 GB 6320027&lt;00:42]\n# writing s2a-dim64-ttsr-valfix/train-000159.tar.gz 400 0.0 GB 63600\n# writing s2a-dim64-ttsr-valfix/train-000160.tar.gz 400 0.0 GB 6400027&lt;00:42]\n# writing s2a-dim64-ttsr-valfix/train-000161.tar.gz 400 0.0 GB 6440028&lt;00:41]\n# writing s2a-dim64-ttsr-valfix/train-000162.tar.gz 400 0.0 GB 6480028&lt;00:41]\n# writing s2a-dim64-ttsr-valfix/train-000163.tar.gz 400 0.0 GB 6520028&lt;00:41]\n# writing s2a-dim64-ttsr-valfix/train-000164.tar.gz 400 0.0 GB 6560028&lt;00:41]\n# writing s2a-dim64-ttsr-valfix/train-000165.tar.gz 400 0.0 GB 6600028&lt;00:40]\n# writing s2a-dim64-ttsr-valfix/train-000166.tar.gz 400 0.0 GB 6640029&lt;00:40]\n# writing s2a-dim64-ttsr-valfix/train-000167.tar.gz 400 0.0 GB 66800\n# writing s2a-dim64-ttsr-valfix/train-000168.tar.gz 400 0.0 GB 6720029&lt;00:40]\n# writing s2a-dim64-ttsr-valfix/train-000169.tar.gz 400 0.0 GB 6760029&lt;00:40]\n# writing s2a-dim64-ttsr-valfix/train-000170.tar.gz 400 0.0 GB 6800029&lt;00:40]\n# writing s2a-dim64-ttsr-valfix/train-000171.tar.gz 400 0.0 GB 6840029&lt;00:39]\n# writing s2a-dim64-ttsr-valfix/train-000172.tar.gz 400 0.0 GB 6880030&lt;00:39]\n# writing s2a-dim64-ttsr-valfix/train-000173.tar.gz 400 0.0 GB 6920030&lt;00:39]\n# writing s2a-dim64-ttsr-valfix/train-000174.tar.gz 400 0.0 GB 6960030&lt;00:39]\n# writing s2a-dim64-ttsr-valfix/train-000175.tar.gz 400 0.0 GB 7000030&lt;00:39]\n# writing s2a-dim64-ttsr-valfix/train-000176.tar.gz 400 0.0 GB 70400\n# writing s2a-dim64-ttsr-valfix/train-000177.tar.gz 400 0.0 GB 7080030&lt;00:38]\n# writing s2a-dim64-ttsr-valfix/train-000178.tar.gz 400 0.0 GB 7120031&lt;00:38]\n# writing s2a-dim64-ttsr-valfix/train-000179.tar.gz 400 0.0 GB 7160031&lt;00:38]\n# writing s2a-dim64-ttsr-valfix/train-000180.tar.gz 400 0.0 GB 7200031&lt;00:38]\n# writing s2a-dim64-ttsr-valfix/train-000181.tar.gz 400 0.0 GB 7240031&lt;00:38]\n# writing s2a-dim64-ttsr-valfix/train-000182.tar.gz 400 0.0 GB 7280031&lt;00:37]\n# writing s2a-dim64-ttsr-valfix/train-000183.tar.gz 400 0.0 GB 7320032&lt;00:37]\n# writing s2a-dim64-ttsr-valfix/train-000184.tar.gz 400 0.0 GB 73600\n# writing s2a-dim64-ttsr-valfix/train-000185.tar.gz 400 0.0 GB 7400032&lt;00:37]\n# writing s2a-dim64-ttsr-valfix/train-000186.tar.gz 400 0.0 GB 7440032&lt;00:37]\n# writing s2a-dim64-ttsr-valfix/train-000187.tar.gz 400 0.0 GB 7480032&lt;00:37]\n# writing s2a-dim64-ttsr-valfix/train-000188.tar.gz 400 0.0 GB 7520032&lt;00:36]\n# writing███████████----------------------| 47.37% [75960/160350 00:33&lt;00:36] s2a-dim64-ttsr-valfix/train-000189.tar.gz 400 0.0 GB 75600\n# writing s2a-dim64-ttsr-valfix/train-000190.tar.gz 400 0.0 GB 7600033&lt;00:36]\n# writing s2a-dim64-ttsr-valfix/train-000191.tar.gz 400 0.0 GB 7640033&lt;00:36]\n# writing s2a-dim64-ttsr-valfix/train-000192.tar.gz 400 0.0 GB 76800\n# writing s2a-dim64-ttsr-valfix/train-000193.tar.gz 400 0.0 GB 7720033&lt;00:36]\n# writing s2a-dim64-ttsr-valfix/train-000194.tar.gz 400 0.0 GB 7760033&lt;00:35]\n# writing s2a-dim64-ttsr-valfix/train-000195.tar.gz 400 0.0 GB 7800034&lt;00:35]\n# writing s2a-dim64-ttsr-valfix/train-000196.tar.gz 400 0.0 GB 7840034&lt;00:35]\n# writing s2a-dim64-ttsr-valfix/train-000197.tar.gz 400 0.0 GB 7880034&lt;00:35]\n# writing s2a-dim64-ttsr-valfix/train-000198.tar.gz 400 0.0 GB 7920034&lt;00:35]\n# writing s2a-dim64-ttsr-valfix/train-000199.tar.gz 400 0.0 GB 7960034&lt;00:34]\n# writing s2a-dim64-ttsr-valfix/train-000200.tar.gz 400 0.0 GB 80000\n# writing s2a-dim64-ttsr-valfix/train-000201.tar.gz 400 0.0 GB 8040035&lt;00:34]\n# writing s2a-dim64-ttsr-valfix/train-000202.tar.gz 400 0.0 GB 8080035&lt;00:34]\n# writing s2a-dim64-ttsr-valfix/train-000203.tar.gz 400 0.0 GB 8120035&lt;00:34]\n# writing s2a-dim64-ttsr-valfix/train-000204.tar.gz 400 0.0 GB 8160035&lt;00:33]\n# writing s2a-dim64-ttsr-valfix/train-000205.tar.gz 400 0.0 GB 8200035&lt;00:33]\n# writing s2a-dim64-ttsr-valfix/train-000206.tar.gz 400 0.0 GB 8240035&lt;00:33]\n# writing s2a-dim64-ttsr-valfix/train-000207.tar.gz 400 0.0 GB 82800\n# writing s2a-dim64-ttsr-valfix/train-000208.tar.gz 400 0.0 GB 8320035&lt;00:33]\n# writing s2a-dim64-ttsr-valfix/train-000209.tar.gz 400 0.0 GB 8360036&lt;00:32]\n# writing s2a-dim64-ttsr-valfix/train-000210.tar.gz 400 0.0 GB 8400036&lt;00:32]\n# writing s2a-dim64-ttsr-valfix/train-000211.tar.gz 400 0.0 GB 8440036&lt;00:32]\n# writing s2a-dim64-ttsr-valfix/train-000212.tar.gz 400 0.0 GB 8480036&lt;00:32]\n# writing s2a-dim64-ttsr-valfix/train-000213.tar.gz 400 0.0 GB 8520036&lt;00:31]\n# writing s2a-dim64-ttsr-valfix/train-000214.tar.gz 400 0.0 GB 8560036&lt;00:31]\n# writing s2a-dim64-ttsr-valfix/train-000215.tar.gz 400 0.0 GB 86000\n# writing s2a-dim64-ttsr-valfix/train-000216.tar.gz 400 0.0 GB 8640036&lt;00:31]\n# writing s2a-dim64-ttsr-valfix/train-000217.tar.gz 400 0.0 GB 8680036&lt;00:31]\n# writing s2a-dim64-ttsr-valfix/train-000218.tar.gz 400 0.0 GB 8720037&lt;00:30]\n# writing s2a-dim64-ttsr-valfix/train-000219.tar.gz 400 0.0 GB 8760037&lt;00:30]\n# writing s2a-dim64-ttsr-valfix/train-000220.tar.gz 400 0.0 GB 8800037&lt;00:30]\n# writing s2a-dim64-ttsr-valfix/train-000221.tar.gz 400 0.0 GB 88400\n# writing s2a-dim64-ttsr-valfix/train-000222.tar.gz 400 0.0 GB 8880037&lt;00:30]\n# writing s2a-dim64-ttsr-valfix/train-000223.tar.gz 400 0.0 GB 8920037&lt;00:29]\n# writing s2a-dim64-ttsr-valfix/train-000224.tar.gz 400 0.0 GB 8960037&lt;00:29]\n# writing s2a-dim64-ttsr-valfix/train-000225.tar.gz 400 0.0 GB 9000037&lt;00:29]\n# writing s2a-dim64-ttsr-valfix/train-000226.tar.gz 400 0.0 GB 9040038&lt;00:29]\n# writing s2a-dim64-ttsr-valfix/train-000227.tar.gz 400 0.0 GB 9080038&lt;00:28]\n# writing s2a-dim64-ttsr-valfix/train-000228.tar.gz 400 0.0 GB 91200\n# writing s2a-dim64-ttsr-valfix/train-000229.tar.gz 400 0.0 GB 9160038&lt;00:28]\n# writing s2a-dim64-ttsr-valfix/train-000230.tar.gz 400 0.0 GB 9200038&lt;00:28]\n# writing s2a-dim64-ttsr-valfix/train-000231.tar.gz 400 0.0 GB 9240038&lt;00:28]\n# writing s2a-dim64-ttsr-valfix/train-000232.tar.gz 400 0.0 GB 9280038&lt;00:27]\n# writing s2a-dim64-ttsr-valfix/train-000233.tar.gz 400 0.0 GB 9320038&lt;00:27]\n# writing s2a-dim64-ttsr-valfix/train-000234.tar.gz 400 0.0 GB 93600\n# writing s2a-dim64-ttsr-valfix/train-000235.tar.gz 400 0.0 GB 9400039&lt;00:27]\n# writing s2a-dim64-ttsr-valfix/train-000236.tar.gz 400 0.0 GB 9440039&lt;00:27]\n# writing s2a-dim64-ttsr-valfix/train-000237.tar.gz 400 0.0 GB 9480039&lt;00:27]\n# writing s2a-dim64-ttsr-valfix/train-000238.tar.gz 400 0.0 GB 9520039&lt;00:26]\n# writing s2a-dim64-ttsr-valfix/train-000239.tar.gz 400 0.0 GB 9560039&lt;00:26]\n# writing s2a-dim64-ttsr-valfix/train-000240.tar.gz 400 0.0 GB 96000\n# writing s2a-dim64-ttsr-valfix/train-000241.tar.gz 400 0.0 GB 9640039&lt;00:26]\n# writing s2a-dim64-ttsr-valfix/train-000242.tar.gz 400 0.0 GB 9680039&lt;00:26]\n# writing s2a-dim64-ttsr-valfix/train-000243.tar.gz 400 0.0 GB 9720040&lt;00:25]\n# writing s2a-dim64-ttsr-valfix/train-000244.tar.gz 400 0.0 GB 9760040&lt;00:25]\n# writing s2a-dim64-ttsr-valfix/train-000245.tar.gz 400 0.0 GB 98000\n# writing s2a-dim64-ttsr-valfix/train-000246.tar.gz 400 0.0 GB 9840040&lt;00:25]\n# writing s2a-dim64-ttsr-valfix/train-000247.tar.gz 400 0.0 GB 9880040&lt;00:25]\n# writing s2a-dim64-ttsr-valfix/train-000248.tar.gz 400 0.0 GB 9920040&lt;00:24]\n# writing s2a-dim64-ttsr-valfix/train-000249.tar.gz 400 0.0 GB 9960040&lt;00:24]\n# writing s2a-dim64-ttsr-valfix/train-000250.tar.gz 400 0.0 GB 10000041&lt;00:24]\n# writing s2a-dim64-ttsr-valfix/train-000251.tar.gz 400 0.0 GB 100400\n# writing s2a-dim64-ttsr-valfix/train-000252.tar.gz 400 0.0 GB 10080041&lt;00:24]\n# writing s2a-dim64-ttsr-valfix/train-000253.tar.gz 400 0.0 GB 10120041&lt;00:23]\n# writing s2a-dim64-ttsr-valfix/train-000254.tar.gz 400 0.0 GB 10160041&lt;00:23]\n# writing s2a-dim64-ttsr-valfix/train-000255.tar.gz 400 0.0 GB 10200041&lt;00:23]\n# writing s2a-dim64-ttsr-valfix/train-000256.tar.gz 400 0.0 GB 102400\n# writing s2a-dim64-ttsr-valfix/train-000257.tar.gz 400 0.0 GB 10280041&lt;00:23]\n# writing s2a-dim64-ttsr-valfix/train-000258.tar.gz 400 0.0 GB 10320041&lt;00:23]\n# writing s2a-dim64-ttsr-valfix/train-000259.tar.gz 400 0.0 GB 10360042&lt;00:22]\n# writing s2a-dim64-ttsr-valfix/train-000260.tar.gz 400 0.0 GB 10400042&lt;00:22]\n# writing s2a-dim64-ttsr-valfix/train-000261.tar.gz 400 0.0 GB 10440042&lt;00:22]\n# writing s2a-dim64-ttsr-valfix/train-000262.tar.gz 400 0.0 GB 104800\n# writing s2a-dim64-ttsr-valfix/train-000263.tar.gz 400 0.0 GB 10520042&lt;00:22]\n# writing s2a-dim64-ttsr-valfix/train-000264.tar.gz 400 0.0 GB 10560042&lt;00:21]\n# writing s2a-dim64-ttsr-valfix/train-000265.tar.gz 400 0.0 GB 10600042&lt;00:21]\n# writing s2a-dim64-ttsr-valfix/train-000266.tar.gz 400 0.0 GB 10640042&lt;00:21]\n# writing s2a-dim64-ttsr-valfix/train-000267.tar.gz 400 0.0 GB 106800\n# writing s2a-dim64-ttsr-valfix/train-000268.tar.gz 400 0.0 GB 10720043&lt;00:21]\n# writing s2a-dim64-ttsr-valfix/train-000269.tar.gz 400 0.0 GB 10760043&lt;00:21]\n# writing s2a-dim64-ttsr-valfix/train-000270.tar.gz 400 0.0 GB 10800043&lt;00:20]\n# writing s2a-dim64-ttsr-valfix/train-000271.tar.gz 400 0.0 GB 10840043&lt;00:20]\n# writing s2a-dim64-ttsr-valfix/train-000272.tar.gz 400 0.0 GB 108800\n# writing s2a-dim64-ttsr-valfix/train-000273.tar.gz 400 0.0 GB 10920043&lt;00:20]\n# writing s2a-dim64-ttsr-valfix/train-000274.tar.gz 400 0.0 GB 10960043&lt;00:20]\n# writing s2a-dim64-ttsr-valfix/train-000275.tar.gz 400 0.0 GB 11000043&lt;00:19]\n# writing s2a-dim64-ttsr-valfix/train-000276.tar.gz 400 0.0 GB 11040044&lt;00:19]\n# writing s2a-dim64-ttsr-valfix/train-000277.tar.gz 400 0.0 GB 110800\n# writing s2a-dim64-ttsr-valfix/train-000278.tar.gz 400 0.0 GB 11120044&lt;00:19]\n# writing s2a-dim64-ttsr-valfix/train-000279.tar.gz 400 0.0 GB 11160044&lt;00:19]\n# writing s2a-dim64-ttsr-valfix/train-000280.tar.gz 400 0.0 GB 11200044&lt;00:19]\n# writing s2a-dim64-ttsr-valfix/train-000281.tar.gz 400 0.0 GB 11240044&lt;00:18]\n# writing s2a-dim64-ttsr-valfix/train-000282.tar.gz 400 0.0 GB 112800\n# writing s2a-dim64-ttsr-valfix/train-000283.tar.gz 400 0.0 GB 11320044&lt;00:18]\n# writing s2a-dim64-ttsr-valfix/train-000284.tar.gz 400 0.0 GB 11360045&lt;00:18]\n# writing s2a-dim64-ttsr-valfix/train-000285.tar.gz 400 0.0 GB 11400045&lt;00:18]\n# writing s2a-dim64-ttsr-valfix/train-000286.tar.gz 400 0.0 GB 114400\n# writing s2a-dim64-ttsr-valfix/train-000287.tar.gz 400 0.0 GB 11480045&lt;00:17]\n# writing s2a-dim64-ttsr-valfix/train-000288.tar.gz 400 0.0 GB 11520045&lt;00:17]\n# writing s2a-dim64-ttsr-valfix/train-000289.tar.gz 400 0.0 GB 11560045&lt;00:17]\n# writing s2a-dim64-ttsr-valfix/train-000290.tar.gz 400 0.0 GB 11600045&lt;00:17]\n# writing s2a-dim64-ttsr-valfix/train-000291.tar.gz 400 0.0 GB 116400\n# writing s2a-dim64-ttsr-valfix/train-000292.tar.gz 400 0.0 GB 11680045&lt;00:17]\n# writing s2a-dim64-ttsr-valfix/train-000293.tar.gz 400 0.0 GB 11720046&lt;00:16]\n# writing s2a-dim64-ttsr-valfix/train-000294.tar.gz 400 0.0 GB 11760046&lt;00:16]\n# writing s2a-dim64-ttsr-valfix/train-000295.tar.gz 400 0.0 GB 11800046&lt;00:16]\n# writing s2a-dim64-ttsr-valfix/train-000296.tar.gz 400 0.0 GB 118400\n# writing s2a-dim64-ttsr-valfix/train-000297.tar.gz 400 0.0 GB 11880046&lt;00:16]\n# writing s2a-dim64-ttsr-valfix/train-000298.tar.gz 400 0.0 GB 11920046&lt;00:15]\n# writing s2a-dim64-ttsr-valfix/train-000299.tar.gz 400 0.0 GB 11960046&lt;00:15]\n# writing s2a-dim64-ttsr-valfix/train-000300.tar.gz 400 0.0 GB 120000\n# writing s2a-dim64-ttsr-valfix/train-000301.tar.gz 400 0.0 GB 12040047&lt;00:15]\n# writing s2a-dim64-ttsr-valfix/train-000302.tar.gz 400 0.0 GB 12080047&lt;00:15]\n# writing s2a-dim64-ttsr-valfix/train-000303.tar.gz 400 0.0 GB 12120047&lt;00:15]\n# writing s2a-dim64-ttsr-valfix/train-000304.tar.gz 400 0.0 GB 12160047&lt;00:14]\n# writing s2a-dim64-ttsr-valfix/train-000305.tar.gz 400 0.0 GB 122000\n# writing s2a-dim64-ttsr-valfix/train-000306.tar.gz 400 0.0 GB 12240047&lt;00:14]\n# writing s2a-dim64-ttsr-valfix/train-000307.tar.gz 400 0.0 GB 12280047&lt;00:14]\n# writing s2a-dim64-ttsr-valfix/train-000308.tar.gz 400 0.0 GB 12320047&lt;00:14]\n# writing s2a-dim64-ttsr-valfix/train-000309.tar.gz 400 0.0 GB 12360048&lt;00:14]\n# writing s2a-dim64-ttsr-valfix/train-000310.tar.gz 400 0.0 GB 124000\n# writing s2a-dim64-ttsr-valfix/train-000311.tar.gz 400 0.0 GB 12440048&lt;00:13]\n# writing s2a-dim64-ttsr-valfix/train-000312.tar.gz 400 0.0 GB 12480048&lt;00:13]\n# writing s2a-dim64-ttsr-valfix/train-000313.tar.gz 400 0.0 GB 12520048&lt;00:13]\n# writing s2a-dim64-ttsr-valfix/train-000314.tar.gz 400 0.0 GB 125600\n# writing s2a-dim64-ttsr-valfix/train-000315.tar.gz 400 0.0 GB 12600048&lt;00:13]\n# writing s2a-dim64-ttsr-valfix/train-000316.tar.gz 400 0.0 GB 12640048&lt;00:13]\n# writing s2a-dim64-ttsr-valfix/train-000317.tar.gz 400 0.0 GB 12680049&lt;00:12]\n# writing s2a-dim64-ttsr-valfix/train-000318.tar.gz 400 0.0 GB 127200\n# writing s2a-dim64-ttsr-valfix/train-000319.tar.gz 400 0.0 GB 12760049&lt;00:12]\n# writing s2a-dim64-ttsr-valfix/train-000320.tar.gz 400 0.0 GB 12800049&lt;00:12]\n# writing s2a-dim64-ttsr-valfix/train-000321.tar.gz 400 0.0 GB 12840049&lt;00:12]\n# writing s2a-dim64-ttsr-valfix/train-000322.tar.gz 400 0.0 GB 12880049&lt;00:11]\n# writing s2a-dim64-ttsr-valfix/train-000323.tar.gz 400 0.0 GB 129200\n# writing s2a-dim64-ttsr-valfix/train-000324.tar.gz 400 0.0 GB 12960049&lt;00:11]\n# writing s2a-dim64-ttsr-valfix/train-000325.tar.gz 400 0.0 GB 13000050&lt;00:11]\n# writing s2a-dim64-ttsr-valfix/train-000326.tar.gz 400 0.0 GB 13040050&lt;00:11]\n# writing s2a-dim64-ttsr-valfix/train-000327.tar.gz 400 0.0 GB 130800\n# writing s2a-dim64-ttsr-valfix/train-000328.tar.gz 400 0.0 GB 13120050&lt;00:11]\n# writing s2a-dim64-ttsr-valfix/train-000329.tar.gz 400 0.0 GB 13160050&lt;00:10]\n# writing s2a-dim64-ttsr-valfix/train-000330.tar.gz 400 0.0 GB 13200050&lt;00:10]\n# writing s2a-dim64-ttsr-valfix/train-000331.tar.gz 400 0.0 GB 132400\n# writing s2a-dim64-ttsr-valfix/train-000332.tar.gz 400 0.0 GB 13280050&lt;00:10]\n# writing s2a-dim64-ttsr-valfix/train-000333.tar.gz 400 0.0 GB 13320050&lt;00:10]\n# writing s2a-dim64-ttsr-valfix/train-000334.tar.gz 400 0.0 GB 13360051&lt;00:10]\n# writing s2a-dim64-ttsr-valfix/train-000335.tar.gz 400 0.0 GB 13400051&lt;00:09]\n# writing s2a-dim64-ttsr-valfix/train-000336.tar.gz 400 0.0 GB 134400\n# writing s2a-dim64-ttsr-valfix/train-000337.tar.gz 400 0.0 GB 13480051&lt;00:09]\n# writing s2a-dim64-ttsr-valfix/train-000338.tar.gz 400 0.0 GB 13520051&lt;00:09]\n# writing s2a-dim64-ttsr-valfix/train-000339.tar.gz 400 0.0 GB 13560051&lt;00:09]\n# writing s2a-dim64-ttsr-valfix/train-000340.tar.gz 400 0.0 GB 136000\n# writing s2a-dim64-ttsr-valfix/train-000341.tar.gz 400 0.0 GB 13640051&lt;00:09]\n# writing s2a-dim64-ttsr-valfix/train-000342.tar.gz 400 0.0 GB 13680052&lt;00:08]\n# writing s2a-dim64-ttsr-valfix/train-000343.tar.gz 400 0.0 GB 13720052&lt;00:08]\n# writing s2a-dim64-ttsr-valfix/train-000344.tar.gz 400 0.0 GB 137600\n# writing s2a-dim64-ttsr-valfix/train-000345.tar.gz 400 0.0 GB 13800052&lt;00:08]\n# writing s2a-dim64-ttsr-valfix/train-000346.tar.gz 400 0.0 GB 13840052&lt;00:08]\n# writing s2a-dim64-ttsr-valfix/train-000347.tar.gz 400 0.0 GB 13880052&lt;00:08]\n# writing s2a-dim64-ttsr-valfix/train-000348.tar.gz 400 0.0 GB 139200\n# writing s2a-dim64-ttsr-valfix/train-000349.tar.gz 400 0.0 GB 13960052&lt;00:07]\n# writing s2a-dim64-ttsr-valfix/train-000350.tar.gz 400 0.0 GB 14000053&lt;00:07]\n# writing s2a-dim64-ttsr-valfix/train-000351.tar.gz 400 0.0 GB 14040053&lt;00:07]\n# writing s2a-dim64-ttsr-valfix/train-000352.tar.gz 400 0.0 GB 140800\n# writing s2a-dim64-ttsr-valfix/train-000353.tar.gz 400 0.0 GB 14120053&lt;00:07]\n# writing s2a-dim64-ttsr-valfix/train-000354.tar.gz 400 0.0 GB 14160053&lt;00:06]\n# writing s2a-dim64-ttsr-valfix/train-000355.tar.gz 400 0.0 GB 14200053&lt;00:06]\n# writing s2a-dim64-ttsr-valfix/train-000356.tar.gz 400 0.0 GB 142400\n# writing s2a-dim64-ttsr-valfix/train-000357.tar.gz 400 0.0 GB 14280053&lt;00:06]\n# writing s2a-dim64-ttsr-valfix/train-000358.tar.gz 400 0.0 GB 14320054&lt;00:06]\n# writing s2a-dim64-ttsr-valfix/train-000359.tar.gz 400 0.0 GB 14360054&lt;00:06]\n# writing s2a-dim64-ttsr-valfix/train-000360.tar.gz 400 0.0 GB 14400054&lt;00:05]\n# writing s2a-dim64-ttsr-valfix/train-000361.tar.gz 400 0.0 GB 144400\n# writing s2a-dim64-ttsr-valfix/train-000362.tar.gz 400 0.0 GB 14480054&lt;00:05]\n# writing s2a-dim64-ttsr-valfix/train-000363.tar.gz 400 0.0 GB 14520054&lt;00:05]\n# writing s2a-dim64-ttsr-valfix/train-000364.tar.gz 400 0.0 GB 14560054&lt;00:05]\n# writing s2a-dim64-ttsr-valfix/train-000365.tar.gz 400 0.0 GB 146000\n# writing s2a-dim64-ttsr-valfix/train-000366.tar.gz 400 0.0 GB 14640055&lt;00:05]\n# writing s2a-dim64-ttsr-valfix/train-000367.tar.gz 400 0.0 GB 14680055&lt;00:04]\n# writing s2a-dim64-ttsr-valfix/train-000368.tar.gz 400 0.0 GB 14720055&lt;00:04]\n# writing s2a-dim64-ttsr-valfix/train-000369.tar.gz 400 0.0 GB 147600\n# writing s2a-dim64-ttsr-valfix/train-000370.tar.gz 400 0.0 GB 14800055&lt;00:04]\n# writing s2a-dim64-ttsr-valfix/train-000371.tar.gz 400 0.0 GB 14840055&lt;00:04]\n# writing s2a-dim64-ttsr-valfix/train-000372.tar.gz 400 0.0 GB 14880055&lt;00:04]\n# writing s2a-dim64-ttsr-valfix/train-000373.tar.gz 400 0.0 GB 149200\n# writing s2a-dim64-ttsr-valfix/train-000374.tar.gz 400 0.0 GB 14960056&lt;00:03]\n# writing s2a-dim64-ttsr-valfix/train-000375.tar.gz 400 0.0 GB 15000056&lt;00:03]\n# writing s2a-dim64-ttsr-valfix/train-000376.tar.gz 400 0.0 GB 15040056&lt;00:03]\n# writing s2a-dim64-ttsr-valfix/train-000377.tar.gz 400 0.0 GB 150800\n# writing s2a-dim64-ttsr-valfix/train-000378.tar.gz 400 0.0 GB 15120056&lt;00:03]\n# writing s2a-dim64-ttsr-valfix/train-000379.tar.gz 400 0.0 GB 15160056&lt;00:03]\n# writing s2a-dim64-ttsr-valfix/train-000380.tar.gz 400 0.0 GB 15200056&lt;00:02]\n# writing s2a-dim64-ttsr-valfix/train-000381.tar.gz 400 0.0 GB 152400\n# writing s2a-dim64-ttsr-valfix/train-000382.tar.gz 400 0.0 GB 15280057&lt;00:02]\n# writing s2a-dim64-ttsr-valfix/train-000383.tar.gz 400 0.0 GB 15320057&lt;00:02]\n# writing s2a-dim64-ttsr-valfix/train-000384.tar.gz 400 0.0 GB 15360057&lt;00:02]\n# writing s2a-dim64-ttsr-valfix/train-000385.tar.gz 400 0.0 GB 154000\n# writing s2a-dim64-ttsr-valfix/train-000386.tar.gz 400 0.0 GB 15440057&lt;00:02]\n# writing s2a-dim64-ttsr-valfix/train-000387.tar.gz 400 0.0 GB 15480057&lt;00:01]\n# writing s2a-dim64-ttsr-valfix/train-000388.tar.gz 400 0.0 GB 155200\n# writing s2a-dim64-ttsr-valfix/train-000389.tar.gz 400 0.0 GB 15560057&lt;00:01]\n# writing s2a-dim64-ttsr-valfix/train-000390.tar.gz 400 0.0 GB 15600058&lt;00:01]\n# writing s2a-dim64-ttsr-valfix/train-000391.tar.gz 400 0.0 GB 15640058&lt;00:01]\n# writing s2a-dim64-ttsr-valfix/train-000392.tar.gz 400 0.0 GB 156800\n# writing s2a-dim64-ttsr-valfix/train-000393.tar.gz 400 0.0 GB 15720058&lt;00:01]\n# writing s2a-dim64-ttsr-valfix/train-000394.tar.gz 400 0.0 GB 15760058&lt;00:00]\n# writing s2a-dim64-ttsr-valfix/train-000395.tar.gz 400 0.0 GB 15800058&lt;00:00]\n# writing s2a-dim64-ttsr-valfix/train-000396.tar.gz 400 0.0 GB 158400\n# writing s2a-dim64-ttsr-valfix/train-000397.tar.gz 400 0.0 GB 15880058&lt;00:00]\n# writing s2a-dim64-ttsr-valfix/train-000398.tar.gz 400 0.0 GB 15920059&lt;00:00]\n# writing s2a-dim64-ttsr-valfix/train-000399.tar.gz 400 0.0 GB 15960059&lt;00:00]\n |████████████████████████████████████████| 100.00% [160350/160350 00:59&lt;00:00]"
  },
  {
    "objectID": "B1. Training.html",
    "href": "B1. Training.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\nSimpleVisual\n\n SimpleVisual (model, masterbar, total_steps)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nvalidate\n\n validate (model, val, half=True, bs=16, drop_last=False, dl_workers=8,\n           device='cuda')\n\n\nsource\n\n\ntrain\n\n train (checkpoint_path, model, train, val, half=True, bs=16, lr=0.0001,\n        drop_last=False, weight_decay=0.1, warmup_steps=10000, epochs=10,\n        clip_gradient_norm=None, dl_workers=8, visual_class=&lt;class\n        '__main__.SimpleVisual'&gt;, profiler=None,\n        run_valid_every_iters=8000, table_row_every_iters=80000,\n        chkpt_every_iters=None, device='cuda', trainable_params=None,\n        callback=None, lr_schedule='wsd')"
  },
  {
    "objectID": "4b. multi-language semantic to acoustic token modeling.html",
    "href": "4b. multi-language semantic to acoustic token modeling.html",
    "title": "Semantic to acoustic token modeling",
    "section": "",
    "text": "from encodec.model import EncodecModel\nimport webdataset as wds\nfrom whisperspeech.train import *\n\nimport pylab as plt\nfrom IPython.display import Audio, HTML, display"
  },
  {
    "objectID": "4b. multi-language semantic to acoustic token modeling.html#model",
    "href": "4b. multi-language semantic to acoustic token modeling.html#model",
    "title": "Semantic to acoustic token modeling",
    "section": "Model",
    "text": "Model\n\nimport pylab as plt\nimport fastprogress\nimport IPython\nimport numpy as np\n\nclass CMLMVisual:\n    \"\"\"Visualize training progress\"\"\"\n    def __init__ (self, model, masterbar, total_steps):\n        self.model = model\n        self.masterbar = masterbar\n        self.total_steps = total_steps\n        self.epochs = total_steps // masterbar.main_bar.total\n        \n        gs = plt.GridSpec(3, 1, height_ratios=[2,2,1])\n        graph_fig = plt.figure(figsize=(10,6))\n        self.graph_fig = graph_fig\n        self.loss_p = graph_fig.add_subplot(gs[0])\n        self.acc_p = graph_fig.add_subplot(gs[1], sharex=self.loss_p)\n        self.acc_p.tick_params('x', labelbottom=False)\n        self.lr_p = graph_fig.add_subplot(gs[2], sharex=self.loss_p)\n        self.lr_p.tick_params('x', labelbottom=False)\n        self.graph_out = None\n        \n        self.its = []\n        self.train_losses = []\n        self.val_losses = []\n        self.lr_history = []\n        self.acc = np.nan\n        self.acc_history = []\n        self.pacc_history = []\n            \n    def show(self):\n        self.start_t = time.time()\n        self.masterbar.write([\"samples\", \"train\", \"val\", \"time\"], table=True)\n        self.graph_out = display(self.graph_fig, display_id=True)\n        self.acc_out = display(IPython.display.HTML(''), display_id=True)\n    \n    def hide(self):\n        if self.graph_out is not None:\n            self.graph_out.update(IPython.display.HTML(''))\n    \n    def plot(self):\n        loss_p, acc_p, lr_p = self.loss_p, self.acc_p, self.lr_p\n        loss_p.clear()\n        loss_p.plot(self.its, self.train_losses)\n        loss_p.plot(self.its, self.val_losses)\n        loss_p.set_xlim(0, self.total_steps)\n        loss_p.set_yscale('log')\n        acc_p.clear()\n        for k in self.acc_history[-1].keys():\n            acc_p.plot(self.its, [x[k] for x in self.acc_history], ':')\n        lr_p.clear()\n        lrs = np.array(self.lr_history)\n        lr_p.plot(self.its, lrs)\n        self.graph_out.update(self.graph_fig)\n    \n    def add_data(self, it, lr, train_loss, val_los):\n        self.its.append(it)\n        self.train_losses.append(train_loss)\n        self.val_losses.append(val_los)\n        self.lr_history.append(lr)\n        metrics = self.model.get_metrics()\n        self.acc_history.append(metrics)\n        html  = \"&lt;h5&gt;Accuracies:&lt;/h5&gt;&lt;table&gt;\"\n        html += \"&lt;thead&gt;\"+(''.join([f\"&lt;td&gt;{k}&lt;td&gt;\" for k,x in metrics.items()]))+\"&lt;/thead&gt;\"\n        html += \"&lt;tr&gt;\"+(''.join([f\"&lt;td&gt;{x*100:.1f}%&lt;td&gt;\" for k,x in metrics.items()]))+\"&lt;/tr&gt;\"\n        html += \"&lt;/table&gt;\"\n        self.acc_out.update(IPython.display.HTML(html))\n        self.plot()\n\n    def add_table_row(self, it, avg_train_loss, val_loss):\n        elapsed_t = time.time() - self.start_t\n        self.masterbar.write([it, f\"{avg_train_loss:.5f}\", f\"{val_loss:.5f}\", fastprogress.core.format_time(elapsed_t)], table=True)\n    \n    def on_iter(self, bar, it, avg_train_loss, val_loss):\n        epoch = math.ceil(it / self.total_steps * self.epochs)\n        bar.comment = f\"#{epoch}/{self.epochs} loss: {avg_train_loss:.3f} / {val_loss:.3f}\"\n\n\nsource\n\nDelSumEmbedding\n\n DelSumEmbedding (n_head=6, head_width=64, atoks_width=None, length=2250,\n                  codes=1024, quantizers=8, pos_embs=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nSADelARTransformer\n\n SADelARTransformer (depth=3, ctx_n=2250, stoks_len=750, stoks_codes=4097,\n                     stoks_width=None, spk_width=None, atoks_width=None,\n                     n_head=3, head_width=64, ffn_mult=4, quantizers=8,\n                     speaker_map={'1': 0}, tunables=Tunables(init_std=9,\n                     embeddings_std=0.2, embeddings_lr_scale=10,\n                     output_mult=5.6, query_mult=0.3,\n                     encoder_depth_ratio=0.25, linear_heads=False,\n                     rope=True, q0_loss_mult=1, causal_encoder=False,\n                     lr0=0.003, clip_gradient_norm=2, weight_decay=0.001,\n                     warmup_steps=2000, random=False,\n                     random_finetune=False, force_hidden_to_emb=False))\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTunables\n\n Tunables (init_std:float=9, embeddings_std:float=0.2,\n           embeddings_lr_scale:float=10, output_mult:float=5.6,\n           query_mult:float=0.3, encoder_depth_ratio:float=0.25,\n           linear_heads:bool=False, rope:bool=True, q0_loss_mult:float=1,\n           causal_encoder:bool=False, lr0:float=0.003,\n           clip_gradient_norm:float=2, weight_decay:float=0.001,\n           warmup_steps:float=2000, random:bool=False,\n           random_finetune:bool=False, force_hidden_to_emb:bool=False)\n\n\nsource\n\n\nrand\n\n rand (start, end)\n\n\nsource\n\n\nDelSumHead\n\n DelSumHead (quantizers=8, n_head=6, head_width=64)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool"
  },
  {
    "objectID": "4b. multi-language semantic to acoustic token modeling.html#training-test",
    "href": "4b. multi-language semantic to acoustic token modeling.html#training-test",
    "title": "Semantic to acoustic token modeling",
    "section": "Training test",
    "text": "Training test\n\ntrain_ds = load_dataset('../librilight/*atoks*.tar.gz', '../librilight-vq-en+pl/', 100000, vq_codes=513, exclude_files='../librilight/common-speakers-maxvad')\nval_ds = load_dataset('../librilight/common-speakers-maxvad.tar.gz', '../librilight-vq-en+pl/', 512, vq_codes=513, validation=True)\n\n\nmodel = make_model('micro', quantizers=4, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model',\n                   tunables=Tunables()).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=25000, run_valid_every_iters=5000, visual_class=CMLMVisual)\n\n\n\n\nAccuracies:acc_0acc_1acc_2acc_329.6%23.6%21.2%19.2%\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 09:39&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n25024\n3.95886\n4.17079\n02:34\n\n\n50016\n3.71909\n3.81947\n04:56\n\n\n75008\n3.53838\n3.62924\n07:18\n\n\n100000\n3.34118\n3.46100\n09:39\n\n\n\n\n\n    \n      \n      100.00% [3125/3125 09:39&lt;00:00 #100000/100000 loss: 3.341 / 3.461]\n    \n    \n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n\n\n\n\n\n\n\n\n\n\n# encoder loss barely helps, probably because the RoPE cross-attention bias is already helping a lot\nmodel = make_model('micro', quantizers=4, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model',\n                   tunables=Tunables(causal_encoder=True)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=25000, run_valid_every_iters=5000, visual_class=CMLMVisual)\n\n\n\n\nAccuracies:acc_0acc_1acc_2acc_329.6%23.8%21.2%19.2%\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 09:41&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n25024\n4.16333\n4.16063\n02:32\n\n\n50016\n3.98411\n3.79632\n04:55\n\n\n75008\n3.75278\n3.62357\n07:18\n\n\n100000\n3.54639\n3.45734\n09:41\n\n\n\n\n\n    \n      \n      100.00% [3125/3125 09:41&lt;00:00 #100000/100000 loss: 3.546 / 3.457]\n    \n    \n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n\n\n\n\n\n\n\n\n\n\n# we can prioritize the loss for the first quantizer\n# we'd have to compare generations to really know if it helps though\nmodel = make_model('micro', quantizers=4, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model',\n                   tunables=Tunables(q0_loss_mult=5)).cuda()\ntrain(f\"s2a-new\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=1, warmup_steps=model.tunables.warmup_steps,\n      table_row_every_iters=25000, run_valid_every_iters=5000, visual_class=CMLMVisual)\n\n\n\n\nAccuracies:acc_0acc_1acc_2acc_330.5%23.0%19.8%17.7%\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 09:39&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n25024\n3.59923\n4.24838\n02:32\n\n\n50016\n3.41711\n3.88030\n04:55\n\n\n75008\n3.19359\n3.70881\n07:17\n\n\n100000\n3.04986\n3.53762\n09:39\n\n\n\n\n\n    \n      \n      100.00% [3125/3125 09:39&lt;00:00 #100000/100000 loss: 3.050 / 3.538]\n    \n    \n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html",
    "href": "2A. Whisper quantization dataset preparation.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "Doing transcription means sampling from the Whisper auto-regresive decoder. This is too slow to do for each training batch. Fortunately the trainscriptions are small text snippets so we can precompute them once for the whole dataset.\nWe use segments from Voice Activity Detection to reduce any boundary issues, the we use webdataset to yields multiple chunks from a FLAC file we only load once. The VAD segments are merged into longer chunks to make Whisper processing more efficent (it always processes 30s at a time)\nUsage:\npython -m whisperspeech.wh_transcribe librilight-large-wo6454-flac-000002.tar\nYou can pass in either a URL or a local file name. Either way it will expect a vad file in the local directory. The result will go into a file in the current directory named after the source file but replacing flac with txt.\n\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nimport pylab as plt\nimport IPython\n\n\nflac_url = 'https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-small-flac-000000.tar'\n\n\nflac_url = './librilight-small-flac-000000.tar'"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#precompute-whisper-transcriptions-for-vq-bottleneck-distilation",
    "href": "2A. Whisper quantization dataset preparation.html#precompute-whisper-transcriptions-for-vq-bottleneck-distilation",
    "title": "WhisperSpeech",
    "section": "",
    "text": "Doing transcription means sampling from the Whisper auto-regresive decoder. This is too slow to do for each training batch. Fortunately the trainscriptions are small text snippets so we can precompute them once for the whole dataset.\nWe use segments from Voice Activity Detection to reduce any boundary issues, the we use webdataset to yields multiple chunks from a FLAC file we only load once. The VAD segments are merged into longer chunks to make Whisper processing more efficent (it always processes 30s at a time)\nUsage:\npython -m whisperspeech.wh_transcribe librilight-large-wo6454-flac-000002.tar\nYou can pass in either a URL or a local file name. Either way it will expect a vad file in the local directory. The result will go into a file in the current directory named after the source file but replacing flac with txt.\n\n\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nimport pylab as plt\nimport IPython\n\n\nflac_url = 'https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-small-flac-000000.tar'\n\n\nflac_url = './librilight-small-flac-000000.tar'"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#merge-vad-segments-into-longer-chunks",
    "href": "2A. Whisper quantization dataset preparation.html#merge-vad-segments-into-longer-chunks",
    "title": "WhisperSpeech",
    "section": "Merge VAD segments into longer chunks",
    "text": "Merge VAD segments into longer chunks\n\n# load some VAD ouputs\nds = wds.WebDataset(\n    vad.flac_to_vad_name(flac_url)\n).decode().to_tuple('vad.npy')\nchunks = [x[0] for x in progress_bar(ds, total='noinfer')]\n\n\n\n\n\n\n    \n      \n      100.00% [335/335 00:00&lt;00:00]\n    \n    \n\n\n\n# quick test\nlen(chunks[0]), len(chunk_merger(chunks[0]))\n\n(46, 28)\n\n\n\nplt.hist([te-ts for x in chunks for ts,te in x])\nplt.title('Segment length distribution straight out of the VAD algorithm');\n\n\n\n\n\n\n\n\n\nplt.hist([te-ts for x in chunks for ts,te in chunk_merger(x)]);\nplt.title('Chunk length distribution after greedy merging');\n\n\n\n\n\n\n\n\n\n(np.array([te-ts for x in chunks for ts,te in chunk_merger(x)]) &lt; 10).mean()\n\n0.03671825647504738\n\n\nIn the above distribution only 3,7% of the samples have &lt; 10 seconds. We noticed that this limits the ability of the T2S model to generate short sequences reliably.\nIt does not seem to matter for quantizing Whisper so we can keep this distribution (it uses less compute for training).\nFor T2S we can add some more shorter chunks at random:\n\nplt.hist([te-ts for x in chunks for ts,te in chunk_merger(x, random_cutter)])\nplt.title('Chunk length distribution after randomized merging');"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#merge-the-flac-and-vad-datasets",
    "href": "2A. Whisper quantization dataset preparation.html#merge-the-flac-and-vad-datasets",
    "title": "WhisperSpeech",
    "section": "Merge the FLAC and VAD datasets",
    "text": "Merge the FLAC and VAD datasets\nFirst we want to merge the VAD dataset with the FLAC audio data.\n\nds = wds_compose(vad.load_dataset(flac_url),\n    merge_in(wds.WebDataset(vad.flac_to_vad_name(flac_url)).decode())\n)\n\n\nfor s in ds: break\ns # notice the 'vad.npy' values that was missing from the FLAC dataset\n\n{'__key__': 'small/100/sea_fairies_0812_librivox_64kb_mp3/01_baum_sea_fairies_64kb',\n '__url__': 'librilight-small-vad-000000.tar.gz',\n 'flac': (tensor([[0., 0., 0.,  ..., 0., 0., 0.]]), 16000),\n 'json': {'speaker': '100',\n  'book_meta': {'id': '2315',\n   'title': 'Sea Fairies',\n   'description': \"&lt;p&gt;In 1910, Baum hoped to end the Oz series and follow with a new series about a little girl named Trot and her sailor companion, Cap'n Bill. The Sea Fairies (1911) was the first book in the projected series and took Trot and Cap'n Bill under the sea where they had adventures with mermaids and other fantastic creatures. It was followed by Sky Island (1912) and then Baum returned to the Oz titles. He brought Trot and Cap'n Bill to Oz in the Scarecrow of Oz (1915). (Summary by Judy Bieber)&lt;/p&gt;\",\n   'url_text_source': 'http://www.gutenberg.org/etext/4358',\n   'language': 'English',\n   'copyright_year': '1911',\n   'num_sections': '22',\n   'url_rss': 'https://librivox.org/rss/2315',\n   'url_zip_file': 'http://www.archive.org/download/sea_fairies_0812_librivox/sea_fairies_0812_librivox_64kb_mp3.zip',\n   'url_project': 'http://en.wikipedia.org/wiki/The_Sea_Fairies',\n   'url_librivox': 'https://librivox.org/the-sea-fairies-by-l-frank-baum/',\n   'url_other': None,\n   'totaltimesecs': 15311,\n   'authors': [{'id': '406',\n     'first_name': 'L. Frank',\n     'last_name': 'Baum',\n     'dob': '1856',\n     'dod': '1919'}],\n   'genre': ['Action & Adventure'],\n   'Dramatic Readings': False,\n   'meta_genre': 'Literature'},\n  'snr': 11.4471,\n  'voice_activity': [[1.52, 11.2],\n   [11.84, 14.08],\n   [15.12, 35.76],\n   [36.32, 55.6],\n   [56.24, 70.48],\n   [71.28, 79.52],\n   [80.08, 89.76],\n   [90.24, 97.52],\n   [98.0, 101.28],\n   [102.8, 124.88],\n   [125.36, 133.12],\n   [133.68, 154.16],\n   [154.64, 177.2],\n   [178.0, 196.96],\n   [197.68, 211.44],\n   [212.32, 216.32],\n   [216.96, 243.52],\n   [244.0, 250.72],\n   [251.52, 268.32],\n   [268.96, 308.56],\n   [309.04, 315.28],\n   [316.0, 317.36],\n   [317.92, 325.44],\n   [326.24, 343.6],\n   [344.08, 350.32],\n   [350.88, 356.64],\n   [357.2, 363.2],\n   [363.76, 365.2],\n   [365.2, 373.2],\n   [373.84, 392.0],\n   [392.56, 401.04],\n   [401.6, 456.96],\n   [457.68, 501.92],\n   [502.4, 531.04],\n   [531.6, 554.48],\n   [554.96, 568.32],\n   [568.96, 585.84],\n   [587.04, 588.48],\n   [597.12, 597.92]]},\n 'vad.npy': array([[  1.764,   6.49 ],\n        [  6.773,  11.18 ],\n        [ 11.98 ,  14.03 ],\n        [ 15.31 ,  36.3  ],\n        [ 36.3  ,  56.06 ],\n        [ 56.4  ,  70.6  ],\n        [ 71.4  , 101.2  ],\n        [102.75 , 103.56 ],\n        [103.7  , 121.75 ],\n        [122.06 , 125.   ],\n        [125.44 , 133.4  ],\n        [133.8  , 154.6  ],\n        [154.6  , 177.6  ],\n        [178.1  , 197.2  ],\n        [197.9  , 212.1  ],\n        [212.5  , 222.5  ],\n        [222.8  , 243.6  ],\n        [244.2  , 246.5  ],\n        [246.8  , 251.1  ],\n        [251.5  , 256.2  ],\n        [256.5  , 257.8  ],\n        [258.2  , 259.8  ],\n        [259.8  , 268.5  ],\n        [269.2  , 289.8  ],\n        [289.8  , 315.8  ],\n        [316.   , 317.2  ],\n        [318.   , 319.   ],\n        [319.8  , 344.   ],\n        [344.2  , 350.2  ],\n        [351.   , 352.5  ],\n        [353.   , 356.8  ],\n        [357.5  , 373.5  ],\n        [374.   , 388.   ],\n        [388.2  , 397.2  ],\n        [397.5  , 401.5  ],\n        [401.8  , 423.5  ],\n        [423.5  , 448.   ],\n        [448.   , 457.2  ],\n        [457.8  , 460.8  ],\n        [461.   , 477.8  ],\n        [478.5  , 502.2  ],\n        [502.2  , 527.5  ],\n        [527.5  , 550.5  ],\n        [550.5  , 576.5  ],\n        [577.   , 586.   ],\n        [587.5  , 588.5  ]], dtype=float16)}"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#split-the-audio-into-chunks",
    "href": "2A. Whisper quantization dataset preparation.html#split-the-audio-into-chunks",
    "title": "WhisperSpeech",
    "section": "Split the audio into chunks",
    "text": "Split the audio into chunks\nAfter we merge the datasets and chunk the segments we can split each audio file into individual samples and pad them to 30s.\n\nsplit_ds = wds_compose(ds,\n   wds.map_dict(**{\"vad.npy\":chunk_merger}),\n   split_to_chunks,\n   utils.resampler(16000, 'samples_16k')\n)\n\n\nfor s in split_ds: break\ns\n\n{'__key__': './dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_000',\n '__url__': '../wolnelektury-preproc-wds/wolnelektury-vad-000001.tar.gz',\n 'i': 0,\n 'imax': 115,\n 'tstart': 0.00844,\n 'tend': 10.06,\n 'total_seconds': 2776.057029478458,\n 'lpad': 0,\n 'rpad': 879616,\n 'lpad_s': 0.0,\n 'rpad_s': 19.9459410430839,\n 'samples': tensor([ 1.8147e-05, -4.9754e-06, -1.3190e-05,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00]),\n 'sample_rate': 44100,\n 'samples_16k': tensor([ 4.3992e-06,  9.4182e-07, -1.3307e-06,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00])}\n\n\n\nIPython.display.display(IPython.display.Audio(s['samples_16k'], rate=16000))\n\n\n                \n                    \n                    Your browser does not support the audio element."
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#transcribe",
    "href": "2A. Whisper quantization dataset preparation.html#transcribe",
    "title": "WhisperSpeech",
    "section": "Transcribe",
    "text": "Transcribe\n\nwhmodel = whisper.load_model('base.en')\ndecoding_options = whisper.DecodingOptions(language='en')\n\n\noutput = flac_url.rsplit(\"/\", 1)[1].replace('flac', 'txt') + \".gz\"\nwith wds.TarWriter(output) as sink:\n    for s in progress_bar(split_ds, total=256):\n        mel = whisper.log_mel_spectrogram(s['samples'].unsqueeze(0).cuda())\n        embs = whmodel.encoder(mel)\n        decs = whmodel.decode(embs, decoding_options)\n\n        sink.write({\n            \"__key__\": s['__key__'],\n            \"txt\": decs[0].text,\n        })\n\n\n\n\n\n\n    \n      \n      100.00% [256/256 00:59&lt;00:00]"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#transcribe-in-batches",
    "href": "2A. Whisper quantization dataset preparation.html#transcribe-in-batches",
    "title": "WhisperSpeech",
    "section": "Transcribe in batches",
    "text": "Transcribe in batches\nWe have one more thing to add – batch processing makes the transcription quite a bit faster (bs=16 brings a 4.5x speedup).\n\nbatched_ds = wds_compose(split_ds,\n    wds.to_tuple('__key__', 'samples'),\n    wds.batched(16),\n)"
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#verify-the-transcripts-and-the-chunks-work-together",
    "href": "2A. Whisper quantization dataset preparation.html#verify-the-transcripts-and-the-chunks-work-together",
    "title": "WhisperSpeech",
    "section": "Verify the transcripts and the chunks work together",
    "text": "Verify the transcripts and the chunks work together\n\ntxt_ds = wds_compose(split_ds,\n    merge_in(wds.WebDataset('../wolnelektury-preproc-wds/'+flac_url.rsplit(\"/\", 1)[1].replace('flac', 'txt') + \".gz\").decode())\n)\n\n\nfor x in txt_ds: break\nx\n\n{'__key__': './dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_000',\n '__url__': '../wolnelektury-preproc-wds/wolnelektury-raw-000001.tar.gz',\n 'i': 0,\n 'imax': 115,\n 'tstart': 0.00844,\n 'tend': 10.06,\n 'total_seconds': 2776.057029478458,\n 'lpad': 0,\n 'rpad': 879616,\n 'lpad_s': 0.0,\n 'rpad_s': 19.9459410430839,\n 'samples': tensor([ 1.8147e-05, -4.9754e-06, -1.3190e-05,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00]),\n 'sample_rate': 44100,\n 'samples_16k': tensor([ 4.3992e-06,  9.4182e-07, -1.3307e-06,  ...,  0.0000e+00,\n          0.0000e+00,  0.0000e+00]),\n 'txt': 'Rozdział 22. Stare mięśca, nowi ludzie. Stierfort i ja zabawiliśmy dwa tygodnie w tamtej okolicy.'}\n\n\n\nfor x in progress_bar(txt_ds, total=10):\n    IPython.display.display(IPython.display.Markdown(f\"#### {x['__key__']} chunk {x['i']} of {x['imax']}\"))\n    fname = f\"test-{x['i']}.ogg\"\n    torchaudio.save(fname, x['samples'][None,:int((x['tend']-x['tstart'])*s['sample_rate'])], s['sample_rate'])\n    IPython.display.display(IPython.display.Audio(url=fname, rate=x['sample_rate']))\n    IPython.display.display(IPython.display.Markdown(x['txt']))\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:02&lt;00:00]\n    \n    \n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_000 chunk 0 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nRozdział 22. Stare mięśca, nowi ludzie. Stierfort i ja zabawiliśmy dwa tygodnie w tamtej okolicy.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_001 chunk 1 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nByliśmy prawie ciągle razem. Czasem tylko rozstawaliśmy się na kilka godzin. Styrford, bowiem, był zawołanym żeglarzem. Ja zaś nie smakowałem zbytnia o w tego rodzaju rozrywkach. To też gdy przyjaciel mój puszczał się w towarzystwie pana PegoTi na morze, pozostawałem zwykle na lądzie. Korzystanie z pokoiku u PegoTi krępowało mnie nieco.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_002 chunk 2 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWiedząc, jak dalece przez dzień cały jest zajęta do oglądaniem chorego męża, wracałem wcześniej bieczorem, gdy Steelfort, będąc panem swego czasu, niczym się nie krempował. To wiedziałem się też, że gdy już spał w najlepsze, on podejmował rybaków w ulubionej winiarni pana PegoT pod dobrą chęcią, lub wyrybaczkim odzieniu spędzał całe księżycowe nocy na morzu.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_003 chunk 3 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWiedziałem, że jego żywa gorąca natura potrzebuje rozrywek i niebezpieczeństw i postępowanie jego wcale mnie nie dziwiło. Rostawaliśmy się i z tego jeszcze powodu, że Stirforta nie mogły pociągać tak jak mnie wycieczki do Blanderstone. To też czasem, że gnaliśmy się po wczesnym śniadaniu, a schodzi i dopiero późno na obiad. Nie miałem pojęcia co robił, czym się wówczas zajmował.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_004 chunk 4 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWiedziałem tylko, że znany był i lubiany przez wszystkich, a posiadał darż, szczególny wynajdwanie rozrywek i zajęcia nawet tam, gdzie inny na jego miejscu nic będzie nie znalazł. Co do mnie przebiegając drogę do Blanderstom, przeżywałem w pamięci każdy dzień z przeszłości i to mi wypełniało myśl i serce. Przypominało mi się każde niegdyś tu przeżyte wrażenie.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_005 chunk 5 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nnadmogiło pod drzewami, gdzie spoczywali moi rodzice, zamienioną przez pegoty w kwietnik, na którą gdybyła tylko jeszcze miejscem wiecznego z poczynku megajca, spoglądałem z takim żalem, i którą widział otwartą na przyjęcie z włog, mojej pięknej, kochanej matki jej dzieciątka, długie, spędzałem godziny. Lężała ona na uboczu, wrogu cmentarze.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_006 chunk 6 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nPrzechodząc z drogą, czytać mogłem wypisane na kamieniu nazwiska, a dzwonkościelny zdawał się być głosem pożegnania. W godzinach tych, myśląc o nich.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_007 chunk 7 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMyślałem zarazem o tym zawsze, jakie miejsce zajmę w życiu, jakich wielkich lub dobrych dokonam czynów. Ech okroków moich nie odbijało nud innych, te tylko, jak gdybym krocząc, ubokużyjącej jeszcze matki, nad powietrzne budował zamki. Stary dom nasz zmienił się niedopoznania.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_008 chunk 8 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nZnikły bez śladu, powiżone przez wrony dawno opuszczone gniazda, adrzewa, strzyżone i ścinane utraciły, dawny kształt, ogród zdziczał, połowa okien była zabita. Dom zamieszkany został przez jakiegoś chorego umysłowo-gentelmena ich, tych, co go doglądali.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_009 chunk 9 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nChory przesiadawał zwykle w okniem, i niech ktoś pokójku i spoglądał na cmentarz. Ciekawy byłem, czy też jego myśli, kreślą te same obrazy, co moje, gdy w czeznym rankiem, w nocnej koszulce wyglądałem tym okienkiem, witając pasące się o wschodzie słońca trzody. Dawni nasi sąsiedzi, Państwu Grraper,\n\n\n\nfor x in progress_bar(txt_ds, total=10):\n    IPython.display.display(IPython.display.Markdown(f\"#### {x['__key__']} chunk {x['i']} of {x['imax']}\"))\n    fname = f\"test-{x['i']}.ogg\"\n    torchaudio.save(fname, x['samples'][None,:int((x['tend']-x['tstart'])*s['sample_rate'])], s['sample_rate'])\n    IPython.display.display(IPython.display.Audio(url=fname, rate=x['sample_rate']))\n    IPython.display.display(IPython.display.Markdown(x['txt']))\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:02&lt;00:00]\n    \n    \n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_000 chunk 0 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nRozdział dwudziesty drugi Stare miejsca, nowi ludzie Styrford i ja zabawiliśmy dwa tygodnie w tamtej okolicy.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_001 chunk 1 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nByliśmy prawie ciągle razem. Czasem tylko rozstawaliśmy się na kilka godzin. Styrford bowiem był zawołanym żeglarzem, ja zaś nie smakowałem zbytnio w tego rodzaju rozrywkach. Toteż gdy przyjaciel mój puszczał się w towarzystwie pana Pegoty na morze, pozostawałem zwykle na lądzie. Korzystanie z pokoiku u Pegoty krępowało mnie nieco.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_002 chunk 2 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWiedząc, jak dalece przez dzień cały jest zajęta doglądaniem chorego męża, wracałem wcześniej wieczorem, gdy Stilford, będąc panem swego czasu, niczym się nie krępował. Dowiedziałem się też, że gdybym już spał w najlepsze, on podejmował rybaków w ulubionej winiarni pana Pegoty pod dobrą chęcią lub w rybackim odzieniu spędzał całe księżycowe noce na morzu.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_003 chunk 3 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWiedziałem, że jego żywa, gorąca natura potrzebuje rozrywek i niebezpieczeństw i postępowanie jego wcale mnie nie dziwiło. Rozstawaliśmy się i z tego jeszcze powodu, że z Tyrforda nie mogły pociągać, tak jak mnie, wycieczki do Blunderstown. To też czasem żegnaliśmy się po wczesnym śniadaniu, a schodzili dopiero późno na obiad. Nie miałem pojęcia, co robił, czym się wówczas zajmował.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_004 chunk 4 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nWiedziałem tylko, że znany był i lubiany przez wszystkich, a posiadał dar szczególny wynajdywanie rozrywek i zajęcia nawet tam, gdzie inny na jego miejscu nic by nie znalazł. Co do mnie, przebiegając drogę do Blunderstone, przeżywałem w pamięci każdy dzień z przeszłości i to mi wypełniało myśl i serce. Przypominało mi się każde niegdyś tu przeżyte wrażenie.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_005 chunk 5 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nNad mogiłą pod drzewami, gdzie spoczywali moi rodzice, zamienioną przez pegot i w kwietnik, na którą, gdy była tylko jeszcze miejscem wiecznego spoczynku mego ojca, spoglądałem z takim żalem i którą widział otwartą na przyjęcie zwłok mojej pięknej, kochanej matki i jej dzieciątka. Długie spędzałem godziny. Leżała ona na uboczu, w rogu cmentarza.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_006 chunk 6 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nPrzechodząc drogą, czytać mogłem wypisane na kamieniu nazwiska, a dzwoń kościelny zdawał się być głosem pożegnania. W godzinach tych, myśląc o nich…\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_007 chunk 7 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nMyślałem zarazem o tym zawsze, jakie miejsce zajmę w życiu, jakich wielkich lub dobrych dokonam czynów. Echo kroków moich nie odbijało nut innych, te tylko, jak gdybym krocząc u boku żyjącej jeszcze matki nadpowietrzne budował zamki. Stary dom nasz zmienił się nie do poznania.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_008 chunk 8 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nZnikły bez śladu powichrzone przez wrony dawno opuszczone gniazda, a drzewa strzyżone i ścinane utraciły dawny kształt. Ogród zdziczał. Połowa okien była zabita. Dom zamieszkany został przez jakiegoś chorego umysłowo dżentelmena i tych, co go doglądali.\n\n\n./dickens-dawid-copperfield-t1/dickens-dawid-copperfield-t1_022_tom-i-rozdzial-xxii_009 chunk 9 of 115\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nChory przesiadywał zwykle w oknie mego niegdyś pokoiku i spoglądał na cmentarz. Ciekawy byłem, czy też jego myśli kreślą te same obrazy co moje, gdy wczesnym rankiem w nocnej koszulce wyglądałem tym okienkiem, witając pasące się o wschodzie słońca trzody. Dawni nasi sąsiedzi, państwo Graper."
  },
  {
    "objectID": "2A. Whisper quantization dataset preparation.html#batch-processing",
    "href": "2A. Whisper quantization dataset preparation.html#batch-processing",
    "title": "WhisperSpeech",
    "section": "Batch processing",
    "text": "Batch processing\nLet’s put everything above together."
  },
  {
    "objectID": "A. Neural modules.html",
    "href": "A. Neural modules.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\ninit_transformer\n\n init_transformer (m)\n\n\nsource\n\n\nQueryHead\n\n QueryHead (in_features:int, out_features:int, bias:bool=True,\n            device=None, dtype=None)\n\nApplies a linear transformation to the incoming data: :math:y = xA^T + b.\nThis module supports :ref:TensorFloat32&lt;tf32_on_ampere&gt;.\nOn certain ROCm devices, when using float16 inputs this module will use :ref:different precision&lt;fp16_on_mi200&gt; for backward.\nArgs: in_features: size of each input sample out_features: size of each output sample bias: If set to False, the layer will not learn an additive bias. Default: True\nShape: - Input: :math:(*, H_{in}) where :math:* means any number of dimensions including none and :math:H_{in} = \\text{in\\_features}. - Output: :math:(*, H_{out}) where all but the last dimension are the same shape as the input and :math:H_{out} = \\text{out\\_features}.\nAttributes: weight: the learnable weights of the module of shape :math:(\\text{out\\_features}, \\text{in\\_features}). The values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}), where :math:k = \\frac{1}{\\text{in\\_features}} bias: the learnable bias of the module of shape :math:(\\text{out\\_features}). If :attr:bias is True, the values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where :math:k = \\frac{1}{\\text{in\\_features}}\nExamples::\n&gt;&gt;&gt; m = nn.Linear(20, 30)\n&gt;&gt;&gt; input = torch.randn(128, 20)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([128, 30])\n\nsource\n\n\nLinearHead\n\n LinearHead (in_features:int, out_features:int, bias:bool=True,\n             device=None, dtype=None)\n\nApplies a linear transformation to the incoming data: :math:y = xA^T + b.\nThis module supports :ref:TensorFloat32&lt;tf32_on_ampere&gt;.\nOn certain ROCm devices, when using float16 inputs this module will use :ref:different precision&lt;fp16_on_mi200&gt; for backward.\nArgs: in_features: size of each input sample out_features: size of each output sample bias: If set to False, the layer will not learn an additive bias. Default: True\nShape: - Input: :math:(*, H_{in}) where :math:* means any number of dimensions including none and :math:H_{in} = \\text{in\\_features}. - Output: :math:(*, H_{out}) where all but the last dimension are the same shape as the input and :math:H_{out} = \\text{out\\_features}.\nAttributes: weight: the learnable weights of the module of shape :math:(\\text{out\\_features}, \\text{in\\_features}). The values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}), where :math:k = \\frac{1}{\\text{in\\_features}} bias: the learnable bias of the module of shape :math:(\\text{out\\_features}). If :attr:bias is True, the values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where :math:k = \\frac{1}{\\text{in\\_features}}\nExamples::\n&gt;&gt;&gt; m = nn.Linear(20, 30)\n&gt;&gt;&gt; input = torch.randn(128, 20)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([128, 30])\n\nsource\n\n\nLayerNorm\n\n LayerNorm (normalized_shape:Union[int,List[int],torch.Size],\n            eps:float=1e-05, elementwise_affine:bool=True, bias:bool=True,\n            device=None, dtype=None)\n\nApplies Layer Normalization over a mini-batch of inputs.\nThis layer implements the operation as described in the paper Layer Normalization &lt;https://arxiv.org/abs/1607.06450&gt;__\n.. math:: y = * + \nThe mean and standard-deviation are calculated over the last D dimensions, where D is the dimension of :attr:normalized_shape. For example, if :attr:normalized_shape is (3, 5) (a 2-dimensional shape), the mean and standard-deviation are computed over the last 2 dimensions of the input (i.e. input.mean((-2, -1))). :math:\\gamma and :math:\\beta are learnable affine transform parameters of :attr:normalized_shape if :attr:elementwise_affine is True. The standard-deviation is calculated via the biased estimator, equivalent to torch.var(input, unbiased=False).\n.. note:: Unlike Batch Normalization and Instance Normalization, which applies scalar scale and bias for each entire channel/plane with the :attr:affine option, Layer Normalization applies per-element scale and bias with :attr:elementwise_affine.\nThis layer uses statistics computed from input data in both training and evaluation modes.\nArgs: normalized_shape (int or list or torch.Size): input shape from an expected input of size\n    .. math::\n        [* \\times \\text{normalized\\_shape}[0] \\times \\text{normalized\\_shape}[1]\n            \\times \\ldots \\times \\text{normalized\\_shape}[-1]]\n\n    If a single integer is used, it is treated as a singleton list, and this module will\n    normalize over the last dimension which is expected to be of that specific size.\neps: a value added to the denominator for numerical stability. Default: 1e-5\nelementwise_affine: a boolean value that when set to ``True``, this module\n    has learnable per-element affine parameters initialized to ones (for weights)\n    and zeros (for biases). Default: ``True``.\nbias: If set to ``False``, the layer will not learn an additive bias (only relevant if\n    :attr:`elementwise_affine` is ``True``). Default: ``True``.\nAttributes: weight: the learnable weights of the module of shape :math:\\text{normalized\\_shape} when :attr:elementwise_affine is set to True. The values are initialized to 1. bias: the learnable bias of the module of shape :math:\\text{normalized\\_shape} when :attr:elementwise_affine is set to True. The values are initialized to 0.\nShape: - Input: :math:(N, *) - Output: :math:(N, *) (same shape as input)\nExamples::\n&gt;&gt;&gt; # NLP Example\n&gt;&gt;&gt; batch, sentence_length, embedding_dim = 20, 5, 10\n&gt;&gt;&gt; embedding = torch.randn(batch, sentence_length, embedding_dim)\n&gt;&gt;&gt; layer_norm = nn.LayerNorm(embedding_dim)\n&gt;&gt;&gt; # Activate module\n&gt;&gt;&gt; layer_norm(embedding)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Image Example\n&gt;&gt;&gt; N, C, H, W = 20, 5, 10, 10\n&gt;&gt;&gt; input = torch.randn(N, C, H, W)\n&gt;&gt;&gt; # Normalize over the last three dimensions (i.e. the channel and spatial dimensions)\n&gt;&gt;&gt; # as shown in the image below\n&gt;&gt;&gt; layer_norm = nn.LayerNorm([C, H, W])\n&gt;&gt;&gt; output = layer_norm(input)\n.. image:: ../_static/img/nn/layer_norm.jpg :scale: 50 %\n\nsource\n\n\nsinusoids\n\n sinusoids (length, channels, max_timescale=10000)\n\nReturns sinusoids for positional embedding\n\nsource\n\n\nMultiHeadAttention\n\n MultiHeadAttention (n_state:int, n_head:int, qk_scale:float=1,\n                     rope:bool=False, cross=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nResidualAttentionBlock\n\n ResidualAttentionBlock (n_state:int, n_head:int,\n                         cross_attention:bool=False, rope:bool=False,\n                         qk_scale:float=1, ffn_mult:int=4)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nBaseDecoder\n\n BaseDecoder (depth=6, n_head=6, width=384, qk_scale=1, ffn_mult=4,\n              length=2250, rope=False)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nFlexEmbeddings\n\n FlexEmbeddings (codes, width, special_codes=None, frozen_width=None,\n                 special_embedding=None, unembed=True)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nEmbeddingProjector\n\n EmbeddingProjector (in_features:int, out_features:int, bias:bool=True,\n                     device=None, dtype=None)\n\nApplies a linear transformation to the incoming data: :math:y = xA^T + b.\nThis module supports :ref:TensorFloat32&lt;tf32_on_ampere&gt;.\nOn certain ROCm devices, when using float16 inputs this module will use :ref:different precision&lt;fp16_on_mi200&gt; for backward.\nArgs: in_features: size of each input sample out_features: size of each output sample bias: If set to False, the layer will not learn an additive bias. Default: True\nShape: - Input: :math:(*, H_{in}) where :math:* means any number of dimensions including none and :math:H_{in} = \\text{in\\_features}. - Output: :math:(*, H_{out}) where all but the last dimension are the same shape as the input and :math:H_{out} = \\text{out\\_features}.\nAttributes: weight: the learnable weights of the module of shape :math:(\\text{out\\_features}, \\text{in\\_features}). The values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}), where :math:k = \\frac{1}{\\text{in\\_features}} bias: the learnable bias of the module of shape :math:(\\text{out\\_features}). If :attr:bias is True, the values are initialized from :math:\\mathcal{U}(-\\sqrt{k}, \\sqrt{k}) where :math:k = \\frac{1}{\\text{in\\_features}}\nExamples::\n&gt;&gt;&gt; m = nn.Linear(20, 30)\n&gt;&gt;&gt; input = torch.randn(128, 20)\n&gt;&gt;&gt; output = m(input)\n&gt;&gt;&gt; print(output.size())\ntorch.Size([128, 30])\n\nfemb = FlexEmbeddings(2, 3, 1).half()\nwith torch.no_grad():\n    femb.main.weight[:] = 0\n    femb.main.weight[:,:2] = torch.eye(2)\n    femb.special.weight[:] = torch.tensor([0,0,1])\nfemb.main.weight, femb.special.weight\n\n(Parameter containing:\n tensor([[1., 0., 0.],\n         [0., 1., 0.]], dtype=torch.float16, requires_grad=True),\n Parameter containing:\n tensor([[0., 0., 1.]], dtype=torch.float16, requires_grad=True))\n\n\n\nembs = femb(torch.tensor([[0,2,1,0]]))\nembs\n\ntensor([[[1., 0., 0.],\n         [0., 0., 1.],\n         [0., 1., 0.],\n         [1., 0., 0.]]], dtype=torch.float16, grad_fn=&lt;IndexPutBackward0&gt;)\n\n\n\nembs += femb(torch.tensor([[0]]))\n\n\nfemb.unembed(embs.float())\n\ntensor([[[2., 0., 0.],\n         [1., 0., 1.],\n         [1., 1., 0.],\n         [2., 0., 0.]]], grad_fn=&lt;CatBackward0&gt;)"
  },
  {
    "objectID": "C. Word error rate metrics.html",
    "href": "C. Word error rate metrics.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\ndefault_transform([\"Footnote, Somber Tashan, May 12, 1856\", \"FOOTNOTE SUMNER TO SHANNON MAY TWELFTH EIGHTEEN FIFTY SIX\"])\n\n[['footnote', 'somber', 'tashan', 'may', '12', '1856'],\n ['footnote', 'sumner', 'to', 'shannon', 'may', '12th', '1856']]\n\n\n\nsource\n\nlibrispeech_data\n\n librispeech_data (datadir, sample_rate=16000)\n\n\nsource\n\n\nDfBuilder\n\n DfBuilder ()\n\nInitialize self. See help(type(self)) for accurate signature.\n\nsource\n\n\nWERStats\n\n WERStats (transform=&lt;jiwer.transforms.Compose object at 0x7fecf5e717c0&gt;)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "1b. voice activity detection.html",
    "href": "1b. voice activity detection.html",
    "title": "Perform Voice Activity Detection (VAD)",
    "section": "",
    "text": "from IPython.display import HTML\nimport pylab as plt\nWe use the voice activity detection model from WhisperX (but we don’t use their merging algorithm):\nTest just a few files:\nds = wds.WebDataset('/data2/libritts-r-raw-000000.tar').compose(wds.decode(wds.torch_audio))\nfor x in ds: break\nx\n\n{'__key__': './dev-clean/1272/128104/1272_128104_000001_000000',\n '__url__': '/data2/libritts-r-raw-000000.tar',\n 'normalized.txt': \"A 'JOLLY' ART CRITIC\",\n 'original.txt': \"A 'JOLLY' ART CRITIC\",\n 'wav': (tensor([[ 0.0000,  0.0000,  0.0000,  ..., -0.0036, -0.0038, -0.0050]]),\n  24000)}\n# test it locally\ninput:str  = 'https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-large-wo6454-flac-000002.tar'\noutput:str = input.rsplit(\"/\", 1)[1].replace('flac', 'vad') + \".gz\"\n\nds = load_dataset(input)\nvad_model = whisperx.vad.load_vad_model('cuda')\n\nwith wds.TarWriter(output) as sink:\n    for s in progress_bar(ds, total=10):\n        audio, sr = s['audio']\n        assert(sr == 16000)\n        sink.write({\n            \"__key__\": s['__key__'],\n            \"vad.npy\": np.array(segment_audio(vad_model, audio), dtype=np.float32)\n        })\n        \n!ls -lh {output}\n!tar tf {output}\n\nLightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/whisperx-vad-segmentation.bin`\n\n\nModel was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n-rw-r--r-- 1 root root 7.5K Sep 21 08:51 librilight-large-wo6454-vad-000002.tar.gz\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_03_molesworth_64kb.vad.npy\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_04_molesworth_64kb.vad.npy\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_08_molesworth_64kb.vad.npy\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_09_molesworth_64kb.vad.npy\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_10_molesworth_64kb.vad.npy\nlarge/10089/five_minutes_stories_1508_librivox_64kb_mp3/5minutesstories_11_molesworth_64kb.vad.npy\nlarge/10089/goodcheerstories_1511_librivox_64kb_mp3/goodcheerstories_13_dickinson_64kb.vad.npy\nlarge/10089/goodcheerstories_1511_librivox_64kb_mp3/goodcheerstories_30_dickinson_64kb.vad.npy\nlarge/10089/mothers_nursery_tales_1512_librivox_64kb_mp3/mothers_nursery_tales_16_pyle_64kb.vad.npy\nlarge/10089/mothers_nursery_tales_1512_librivox_64kb_mp3/mothers_nursery_tales_25_pyle_64kb.vad.npy\n\n\n\n\n\n\n\n    \n      \n      100.00% [10/10 00:10&lt;00:00]"
  },
  {
    "objectID": "1b. voice activity detection.html#batch-processing",
    "href": "1b. voice activity detection.html#batch-processing",
    "title": "Perform Voice Activity Detection (VAD)",
    "section": "Batch processing",
    "text": "Batch processing\nLet’s put everything above together.\n\n# for reference, this was the performance on a single 4090:\nprocess_shard('https://huggingface.co/datasets/collabora/librilight-webdataset/resolve/main/librilight-small-flac-000000.tar')\n\nLightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.0.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint --file ../../../.cache/torch/whisperx-vad-segmentation.bin`\n\n\nModel was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.0.1+cu118. Bad things might happen unless you revert torch to 1.x.\n\n\n\n\n\n\n\n    \n      \n      100.00% [335/335 03:30&lt;00:00]\n    \n    \n\n\n\nfor x in wds.WebDataset('/data2/libritts-r-vad-000000.tar').decode(): break\nx['__key__'].split('/')\n\n['.', 'dev-clean', '1272', '128104', '1272_128104_000001_000000']\n\n\n\nplt.hist([x['vad.npy'].shape[0] for x in wds.WebDataset('/data2/libritts-r-vad-000000.tar').decode()])\n\n(array([1.6967e+04, 0.0000e+00, 6.4500e+02, 0.0000e+00, 0.0000e+00,\n        1.0800e+02, 0.0000e+00, 2.5000e+01, 0.0000e+00, 7.0000e+00]),\n array([1. , 1.4, 1.8, 2.2, 2.6, 3. , 3.4, 3.8, 4.2, 4.6, 5. ]),\n &lt;BarContainer object of 10 artists&gt;)"
  },
  {
    "objectID": "D. Common dataset utilities.html",
    "href": "D. Common dataset utilities.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\nshard_glob\n\n shard_glob (input)\n\n\nshard_glob('../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-*.tar.gz')\n\n['../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000000.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000006.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000004.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000001.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000003.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000002.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000005.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000007.tar.gz']\n\n\n\n# \nshard_glob(Path('../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-*.tar.gz'))\n\n['../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000000.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000006.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000004.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000001.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000003.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000002.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000005.tar.gz',\n '../librilight/librilight-atoks-txts/librilight-small-atoks-3kbps-000007.tar.gz']\n\n\n\n# we can also specify the range and generate shard URLs\nshard_glob(Path('https://huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-{000000..000007}.tar.gz'))\n\n['https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000000.tar.gz',\n 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000001.tar.gz',\n 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000002.tar.gz',\n 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000003.tar.gz',\n 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000004.tar.gz',\n 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000005.tar.gz',\n 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000006.tar.gz',\n 'https:/huggingface.co/datasets/collabora/librilight-processed-webdataset/resolve/main/librilight-small-atoks-3kbps-000007.tar.gz']\n\n\n\nsource\n\n\njoin_datasets\n\n join_datasets (datasets)\n\nAn iterable Dataset.\nAll datasets that represent an iterable of data samples should subclass it. Such form of datasets is particularly useful when data come from a stream.\nAll subclasses should overwrite :meth:__iter__, which would return an iterator of samples in this dataset.\nWhen a subclass is used with :class:~torch.utils.data.DataLoader, each item in the dataset will be yielded from the :class:~torch.utils.data.DataLoader iterator. When :attr:num_workers &gt; 0, each worker process will have a different copy of the dataset object, so it is often desired to configure each copy independently to avoid having duplicate data returned from the workers. :func:~torch.utils.data.get_worker_info, when called in a worker process, returns information about the worker. It can be used in either the dataset’s :meth:__iter__ method or the :class:~torch.utils.data.DataLoader ’s :attr:worker_init_fn option to modify each copy’s behavior.\nExample 1: splitting workload across all workers in :meth:__iter__::\n&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_DATALOADER)\n&gt;&gt;&gt; # xdoctest: +SKIP(\"Fails on MacOS12\")\n&gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end &gt; start, \"this example code only works with end &gt;= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         worker_info = torch.utils.data.get_worker_info()\n...         if worker_info is None:  # single-process data loading, return the full iterator\n...             iter_start = self.start\n...             iter_end = self.end\n...         else:  # in a worker process\n...             # split workload\n...             per_worker = int(math.ceil((self.end - self.start) / float(worker_info.num_workers)))\n...             worker_id = worker_info.id\n...             iter_start = self.start + worker_id * per_worker\n...             iter_end = min(iter_start + per_worker, self.end)\n...         return iter(range(iter_start, iter_end))\n...\n&gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n&gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)\n\n&gt;&gt;&gt; # Single-process loading\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[tensor([3]), tensor([4]), tensor([5]), tensor([6])]\n\n&gt;&gt;&gt; # xdoctest: +REQUIRES(POSIX)\n&gt;&gt;&gt; # Mult-process loading with two worker processes\n&gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n&gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non deterministic\")\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[tensor([3]), tensor([5]), tensor([4]), tensor([6])]\n\n&gt;&gt;&gt; # With even more workers\n&gt;&gt;&gt; # xdoctest: +IGNORE_WANT(\"non deterministic\")\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=12)))\n[tensor([3]), tensor([5]), tensor([4]), tensor([6])]\nExample 2: splitting workload across all workers using :attr:worker_init_fn::\n&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_DATALOADER)\n&gt;&gt;&gt; class MyIterableDataset(torch.utils.data.IterableDataset):\n...     def __init__(self, start, end):\n...         super(MyIterableDataset).__init__()\n...         assert end &gt; start, \"this example code only works with end &gt;= start\"\n...         self.start = start\n...         self.end = end\n...\n...     def __iter__(self):\n...         return iter(range(self.start, self.end))\n...\n&gt;&gt;&gt; # should give same set of data as range(3, 7), i.e., [3, 4, 5, 6].\n&gt;&gt;&gt; ds = MyIterableDataset(start=3, end=7)\n\n&gt;&gt;&gt; # Single-process loading\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=0)))\n[3, 4, 5, 6]\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Directly doing multi-process loading yields duplicate data\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2)))\n[3, 3, 4, 4, 5, 5, 6, 6]\n\n&gt;&gt;&gt; # Define a `worker_init_fn` that configures each dataset copy differently\n&gt;&gt;&gt; def worker_init_fn(worker_id):\n...     worker_info = torch.utils.data.get_worker_info()\n...     dataset = worker_info.dataset  # the dataset copy in this worker process\n...     overall_start = dataset.start\n...     overall_end = dataset.end\n...     # configure the dataset to only process the split workload\n...     per_worker = int(math.ceil((overall_end - overall_start) / float(worker_info.num_workers)))\n...     worker_id = worker_info.id\n...     dataset.start = overall_start + worker_id * per_worker\n...     dataset.end = min(dataset.start + per_worker, overall_end)\n...\n\n&gt;&gt;&gt; # Mult-process loading with the custom `worker_init_fn`\n&gt;&gt;&gt; # Worker 0 fetched [3, 4].  Worker 1 fetched [5, 6].\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=2, worker_init_fn=worker_init_fn)))\n[3, 5, 4, 6]\n\n&gt;&gt;&gt; # With even more workers\n&gt;&gt;&gt; print(list(torch.utils.data.DataLoader(ds, num_workers=12, worker_init_fn=worker_init_fn)))\n[3, 4, 5, 6]\n\n# validate that we don't reset the datasets on each `iter`\n# this is important with webdatasets since sample shuffling is very bad initially, unless num_workers &lt;&lt; num_shards\nfrom itertools import islice\nds = join_datasets([\"ABCDEFG\"])\nfor x in islice(ds, 3):\n    print(x)\nfor x in islice(ds, 5):\n    print(x)\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n# will stop as soon as it exhausts one iterator\nfor x in join_datasets(['ABCDEFG', 'abcdefg', range(20)]):\n    print(x)\n\n0\na\n1\n2\n3\nA\n4\n5\nb\nB\nc\nC\nD\nE\n6\nd\ne\n7\nF\nf\ng\n8\nG\n9\n\n\n\nsource\n\n\nresampler\n\n resampler (newsr=24000, key='samples_24k')\n\n\nsource\n\n\nderived_name\n\n derived_name (input, kind, base='audio', suffix='.gz', dir=None)\n\n\nsource\n\n\nderived_dataset\n\n derived_dataset (kind, base='audio', suffix='.gz', decoders=[], dir=None)\n\n\nsource\n\n\nmerge_in\n\n merge_in (dataset_fun)\n\nMerge a dataset into the current one returning samples with the union of keys. Pass in a function that takes a URL of a sample and returns a dataset for it (called everytime the URL changes).\nIt requires (and validates) that both datasets have the same ordering of keys so you have to use it before any sample shuffling. Shard shuffling is ok.\n\nsource\n\n\nAtomicTarWriter\n\n AtomicTarWriter (name, throwaway=False)\n\n\nsource\n\n\nreadlines\n\n readlines (fname)"
  },
  {
    "objectID": "6. Quality-boosting vocoder.html",
    "href": "6. Quality-boosting vocoder.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\nVocoder\n\n Vocoder (repo_id='charactr/vocos-encodec-24khz', device=None)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "dataset preparation.html",
    "href": "dataset preparation.html",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "",
    "text": "WhisperSpeech is trained on heavily preprocessed speech data generated from several models:",
    "crumbs": [
      "I can has speech? What data WhisperSpeech needs?"
    ]
  },
  {
    "objectID": "dataset preparation.html#who-is-who-a-high-level-overview",
    "href": "dataset preparation.html#who-is-who-a-high-level-overview",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Who is who? A high-level overview",
    "text": "Who is who? A high-level overview\nTo get these 3 data representations we have to run the audio data through several models. The first two steps are always the same, the rest depend on the model we want to run.\n\nWe start by downloading the speech audio files into a sharded webdataset (e.g. A3. Download Project Guttenberg audiobooks).\nWe released webdatasetified versions of two important public domain speech datasets – LibriLight and Project Gutenberg Audiobooks.\nAll subsequent steps rely on voice activity detection (VAD) and diarization so we always generate segment lists and extract speaker embeddings for all audio files (see 1B. Voice activity detection and 2A. Speaker Embeddings for source code).\nThe results of this step were also released on Hugging Face – LibriLight and Project Gutenberg Audiobooks.\n\nThe next steps depend on which model we want to train or fine-tune.\n\nTo re-train the quantized Whisper model we need to transcribe the audio with base.en (2A. Whisper quantization dataset preparation). A model pretrained on 60k hours of LibriLight is available from Hugging Face whisper-vq-stoks-v2.model.\nTo train the text to semantic token model we need to transcribe the audio with Whisper small.en and extract the semantic tokens (5A. T2S dataset preparation).\nTo train the semantic to acoustic model we need to extract the semantic tokens and compress the audio with Encodec for the semantic to acoustic model (4A. S2A dataset preparation).\n\nThese three steps are all independent since they require different chunking of speech data. For quantizing Whisper and S2A training we greedily merge the VAD segments from the same speaker into (at most) 30 second chunks to improve training performance (more uniform chunks mean less computation time is spent on padding). For T2S we randomly truncate when merging the VAD segments so the model also learns how to work with shorter texts. The code to perform this is in 1C. VAD merging.",
    "crumbs": [
      "I can has speech? What data WhisperSpeech needs?"
    ]
  },
  {
    "objectID": "dataset preparation.html#tldr-example-give-me-the-codes",
    "href": "dataset preparation.html#tldr-example-give-me-the-codes",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "TL;DR example – give me the codes!",
    "text": "TL;DR example – give me the codes!\nIn this example we will convert a single split from the Multilingual Libri Speech dataset.\n\nPrepare the webdataset shards\nThe first, most time-consuming, step is to convert the data from it’s original form into the webdataset format. If you want to skip this section and still follow along, the results can be downloaded from Hugging Face at datasets/collabora/multilingual-librispeech-webdataset.\nFirst we need tarp which is a tool that helps create and manipulate the webdataset tar files more effectively. You can check out more about it in the official tarp README\ngo install -v github.com/collabora/tarp/tarp@latest\nAfterwards, we download and unpack the original dataset files:\naria2c -x10 https://dl.fbaipublicfiles.com/mls/mls_french_opus.tar.gz\ntar -xf mls_french_opus.tar.gz\nNext, we’ll need to convert each line in the transcripts.txt file:\n10065_10039_000000      ses vêtements devinrent tout brillants de lumière et blancs comme la neige en sorte qu'il n'y a point de foulon sur la terre qui puisse en faire d'aussi blancs\ninto a tarp script:\ntrain/10065_10039_000000.opus file:mls_french_opus/train/audio/10065/10039/10065_10039_000000.opus\ntrain/10065_10039_000000.txt text:ses vêtements devinrent tout brillants de lumière et blancs comme la neige en sorte qu'il n'y a point de foulon sur la terre qui puisse en faire d'aussi blancs\nWe can achieve this using a short Python script (saved as make-script.py):\nimport sys\n\nfname = sys.argv[1]\ndir, split, _ = fname.rsplit(\"/\", 2)\n\nfor ln in open(fname):\n    id, txt = ln.split(\"\\t\")\n    a,b,c = id.split(\"_\")\n    txt = txt.replace(\"\\n\", \"\")\n    print(f\"\"\"{split}/{id}.opus file:{dir}/{split}/audio/{a}/{b}/{id}.opus\n{split}/{id}.txt text:{txt}\"\"\")\nOnce we have this, we can run the conversion process. The python script outputs data sample descriptions which are fed to tarp create that archives them into a tar stream (a bit similar to tar -T -). The tarp split will then cut the incoming stream into 2GB shards and save them to separate files, making sure to split on sample boundaries.\nThe 2GB size was chosen as a good compromise between the shard count and shard transcription time for mp3/opus files with mutlilingual speech. For LibriLight (English compressed with FLAC) the magic number was 5GB because we FLAC compresses less and we can also use a smaller model for transcribing English speech.\npython3 make-script.py  mls_french_opus/train/transcripts.txt \\\n  | /root/go/bin/tarp create -o - - \\\n  | /root/go/bin/tarp split -s 2e9 -o 'mls_french_train-audio-%06d.tar' -\nWe’ll have to repeat the same command two times replacing train with test and dev and afterwards we can upload everything to Hugging Face:\nhuggingface-cli login\nhuggingface-cli upload --repo-type dataset collabora/multilingual-librispeech-webdataset .\n\n\nProcess the shards on a single GPU machine\nWe do the sharding mainly to be able to effectively process data on many GPUs but for the sake of simplicity we will use a single GPU here. The process stays the same, but different tools would be used to schedule the jobs. For reference, below the commands, we have specified their approximate runtimes on a RTX 4090 for the French subset of MLS.\nPerform voice activity detection:\nparallel --eta -j3 python -m whisperspeech.vad {} ::: ./*.tar\n# 50min\nExtract speaker embeddings for each fragment:\nparallel --eta -j2 python -m whisperspeech.extract_spk_emb --batch_size 16 {} ::: ./*.tar\n# 1h 10min\nWe perform VAD segment merging (we do it as a separate step here to remove all randomness and get reproducibility for later steps):\nparallel --eta -j16 python -m whisperspeech.vad_merge --eqvad {} ::: *.tar\nparallel --eta -j16 python -m whisperspeech.vad_merge {} ::: *.tar\nWith that covered we can start the heavy lifting with the transcripts:\nparallel --eta -j1 python -m whisperspeech.prepare_t2s_txts --transcription_model medium --language fr --batch_size 32 {} ::: *.tar\n# 6h 48min\nAfterwards comes Encodec compression:\nparallel --eta -j2 python -m whisperspeech.prepare_s2a_atoks --batch_size 4 {} ::: *.tar\n# 2h\nNow we can extract the semantic tokens for both the T2S (eqvad) and S2A (maxvad) training:\nparallel --eta -j1 python -m whisperspeech.extract_stoks --batch_size 16 --vq_model ../nbs/vqmodel-medium-en+pl-512c-dim64.model {} ::: *.tar\nparallel --eta -j1 python -m whisperspeech.extract_stoks --kind eqvad --batch_size 16 --vq_model ../nbs/vqmodel-medium-en+pl-512c-dim64.model {} ::: *.tar\n# 3h 45min\n\n\nSplitting out the validation set(s)\nAfter we have all the samples we may want to extract some validation sets. There are many ways to do it but here we’ll manually choose some speakers we’ll later skip completely during training.\nWe start by dumping all the sample ids:\nparallel tar tf {} ::: stoks/*-atoks-3kbps-*.tar.gz | sed -e 's/\\.atoks\\.npy//' &gt; all-samples-maxvad\nparallel tar tf {} ::: stoks/*-small.en-txt-*.tar.gz | sed -e 's/\\.txt//' &gt; all-samples-eqvad\nwc -l all-samples-maxvad\nBecause the sample ids (which are the original file paths) have speaker ids in them we can make a quick histogram:\n&lt; all-samples-maxvad awk -F_ '{ print $1; }'|sort|uniq -c|sort -n|less\nFrom the result we can copy and paste 10 speaker ids of around 50 samples each to get 512 validation samples. We’ll exclude them from the training set because we want to validate on unseen speakers. We have to repeat this process for both splits (maxvad and eqvad since they have’ll different sample counts and ids):\n&lt; all-samples-maxvad grep 'train/1579\\|train/2033\\|train/3182\\|train/12981\\|train/2284\\|train/2297\\|train/6348\\|train/7200\\|train/7679\\|train/1989' &gt;\nunseen-speakers-maxvad\n&lt; all-samples-eq grep 'train/1579\\|train/2033\\|train/3182\\|train/12981\\|train/2284\\|train/2297\\|train/6348\\|train/7200\\|train/7679\\|train/1989' &gt; unseen-speakers-eqvad\nOnce we have all the ids we can rescan the whole dataset once and split out the validation samples to separate webdataset shards to make validation fast:\npython -m whisperspeech.split_out_val_datasets *-atoks-* unseen-speakers-maxvad\npython -m whisperspeech.split_out_val_datasets '*-txt-*' unseen-speakers-eqvad\ncd stoks && python -m whisperspeech.split_out_val_datasets '*-maxvad-stoks-*' ../unseen-speakers-maxvad\ncd stoks && python -m whisperspeech.split_out_val_datasets '*-eqvad-stoks-*' ../unseen-speakers-eqvad\nWe can use wc -l all-samples-maxvad to find out how many samples we have.\n\n\nCreating the dataset configuration files for training\nFinally we create the configuration files for the training script:\ncat &gt; mls-fr-t2s-train.dataset &lt;&lt;EOF\nmultilingual-librispeech-webdataset/*-medium-txt-*.tar.gz multilingual-librispeech-webdataset/vq-en+pl/ 390203 --txt_kind='medium-txt' --language=fr --exclude_files multilingual-librispeech-webdataset/unseen-speakers-eqvad\nEOF\ncat &gt; mls-fr-s2a-train.dataset &lt;&lt;EOF\nmultilingual-librispeech-webdataset/*-atoks-*.tar.gz multilingual-librispeech-webdataset/vq-en+pl/ 338362  --language=fr --exclude_files multilingual-librispeech-webdataset/unseen-speakers-maxvad\nEOF\ncat &gt; mls-fr-s2a-val-unseen-speakers.dataset &lt;&lt;EOF\nmultilingual-librispeech-webdataset/unseen-speakers-maxvad.tar.gz multilingual-librispeech-webdataset/vq-en+pl/ 512 --language fr\nEOF\ncat &gt; mls-fr-t2s-val-unseen-speakers.dataset &lt;&lt;EOF\nmultilingual-librispeech-webdataset/unseen-speakers-eqvad.tar.gz multilingual-librispeech-webdataset/vq-en+pl/ 512 --txt_kind 'medium-txt' --language fr\nEOF",
    "crumbs": [
      "I can has speech? What data WhisperSpeech needs?"
    ]
  },
  {
    "objectID": "dataset preparation.html#why-webdataset",
    "href": "dataset preparation.html#why-webdataset",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Why WebDataset?",
    "text": "Why WebDataset?\nAll WhisperSpeech training and preproc code got reorganized around webdatasets. Webdatasets are just simple tar files that store all our data samples (files) but they are great for working with very large datasets. Inside these tar files we can store multiple files per sample in any format we want (e.g. the speech mp3/flac/wav files, the text transcripts, tokens in numpy arrays). For example from the data used to train the S2A model we have:\n$ tar tf whisperspeech-s2a-512c-dim64/librilight-small-000.tar.gz |head -6\nsmall/1874/shortlifelincoln_0809_librivox_64kb_mp3/shortlifeoflincoln_10_nicolay_64kb_021.atoks.npy\nsmall/1874/shortlifelincoln_0809_librivox_64kb_mp3/shortlifeoflincoln_10_nicolay_64kb_021.stoks.npy\nsmall/28/amateur_cracksman_librivox_64kb_mp3/amateur_cracksman_04_hornung_64kb_004.atoks.npy\nsmall/28/amateur_cracksman_librivox_64kb_mp3/amateur_cracksman_04_hornung_64kb_004.stoks.npy\nsmall/1874/shortlifelincoln_0809_librivox_64kb_mp3/shortlifeoflincoln_10_nicolay_64kb_052.atoks.npy\nsmall/1874/shortlifelincoln_0809_librivox_64kb_mp3/shortlifeoflincoln_10_nicolay_64kb_052.stoks.npy\nThe name of the file is the same as the file name of the original dataset sample and the extensions tell us what kind of value they hold and in which format.\nFurthermore we can split the whole dataset into fixed-size tar files called shards and load them on demand without unpacking. It turns out that this is exactly what we need for both AI training and data preprocessing:\n\nfor training we start a multiple CPU workers in parallel, open different shards in each, stream the data sequentially from disk (fast), decode it independently and them shuffle the samples we receive from each worker to create varied training batches\nfor preprocessing we independently send each shard to a worker and save all the results in a new webdataset shard\n\nReading samples sequentialy allows us to simply compress the whole file with gzip and offers best performance even on spinning or network disks.\n\n\n\n\n\n\nNote\n\n\n\nFor the Juwels cluster there is another crucial benefit. There is a pretty low limit on the total number of files on network disks (inodes to be precise) so there is a strong preference to keep data in a few large files. The network file system performance is also better if we don’t have to open too many files.\n\n\nKeeping each shard around 5GB seems to work great (the processed shards will likely be a lot smaller but it’s a lot easier to keep a 1-to-1 shard mapping). For the almost 4TB LibriLight dataset this translates to 625 files.\nWe found it quite useful to also keep all the data in some splits. This is data dependent but for LibriLight we followed the original split (small, medium, large) but also extracted the 6454 speaker from the large split because it is was the largest single speaker dataset and it allowed us to use it during development without downloading the full 4TB.\n\n\n\n\n\n\nCaution\n\n\n\nThe sample file names should not have dots in them, otherwise the WebDataset code gets confused which files go together into one sample. This can be worked around later but it’s easiest if we just do .replace('.', '_') when storing the initial raw dataset.",
    "crumbs": [
      "I can has speech? What data WhisperSpeech needs?"
    ]
  },
  {
    "objectID": "dataset preparation.html#joins-on-webdatasets",
    "href": "dataset preparation.html#joins-on-webdatasets",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Joins on WebDatasets",
    "text": "Joins on WebDatasets\nOne novel functionality we developed for this project is the capability to join multiple preprocessed webdatasets. This mechanism relies on keeping a constant ordering of samples in a shard and ensuring 1-to-1 correspondence between the input and output shards during preprocessing.\nExample usage:\nds = wds.WebDataset([str(x) for x in Path('librilight/').glob('*.tar')]).compose( # load all audio shards\n    wds.decode(wds.torch_audio), # decode the audio data\n    vq_stoks.merge_in( # merge another WebDataset\n        # for each audio (`raw`) shard, find the path and name of a corresponding `vad` shard\n        vq_stoks.derived_dataset('librilight-processed/', 'vad')\n    ),\n)\nderived_dataset creates for us a helper function that returns an opened derived dataset given the original shard file name:\ndef derived_dataset(path, kind):\n    def deriver(url):\n        url = str(Path(path)/(Path(url).name.replace(\"raw\", kind) + \".gz\"))\n        return wds.WebDataset(wds.SimpleShardList([url])).decode()\n    return deriver\nThis feature is experimental and the API may change as we develop more experience with this merging style.",
    "crumbs": [
      "I can has speech? What data WhisperSpeech needs?"
    ]
  },
  {
    "objectID": "dataset preparation.html#examples-of-preprocessing-runs",
    "href": "dataset preparation.html#examples-of-preprocessing-runs",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Examples of preprocessing runs",
    "text": "Examples of preprocessing runs\nAn example of running a preprocessing step locally on a single file:\nmkdir -p guttenberg-preproc && cd guttenberg-preproc\npython -m whisperspeech.vad ../guttenberg-audiobooks/guttenberg-audiobooks-raw-000010.tar\nThis will generate a file named guttenberg-audiobooks-vad-000000.tar.gz in the guttenberg-preproc directory.\nOn the cluster we can run multiple jobs in parallel (24 in this case), each processing one input shard. Since each job is pretty short (around 30 minutes) it’s easier for the scheduler to squeeze these between longer and higher-priority jobs.\nmkdir -p whisperspeech-s2a-512c-dim64 && cd whisperspeech-s2a-512c-dim64\nfind ../librilight/ -name 'librilight-small-*.tar'| ~/clapa1/run-batch 24 \\\n    'python -m whisperspeech.prepare_s2a_dataset $FILE ../librilight-preproc\n            --vq_model ~/clapa1/scratch/vqmodel-512c-dim64-4e-hyptuned-32gpu.model\n            --batch_size 8'\nThe prepare_s2a_dataset script is taking raw audio data from the input file, automatically finding corresponding shards with VAD results in ../librilight-preproc and writing the results to the whisperspeech-s2a-512c-dim64 directory.",
    "crumbs": [
      "I can has speech? What data WhisperSpeech needs?"
    ]
  },
  {
    "objectID": "dataset preparation.html#voice-activity-detection",
    "href": "dataset preparation.html#voice-activity-detection",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Voice activity detection",
    "text": "Voice activity detection\nCode: 1B. Voice activity detection\nRight now we are using the VAD model from WhisperX that is enough to avoid cutting audio in the middle of a word which would hurt automated transcriptions quite a lot. For more fancy datasets with multiple speakers we could use pyannote for it’s detection of multiple people speaking at once and diarization capability.\nWe later merge the VAD segments into longer chunks for more efficient training (less padding == higher efficiency). The code and histogram plots can be found in 2A. Whisper quantization dataset preparation",
    "crumbs": [
      "I can has speech? What data WhisperSpeech needs?"
    ]
  },
  {
    "objectID": "dataset preparation.html#transcription",
    "href": "dataset preparation.html#transcription",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Transcription",
    "text": "Transcription\nCode: 5A. T2S dataset preparation\nFor training the TTS model (T2S) we are using running batches of chunked speech segments though FasterWhisper. We use the small.en model since there seems to be little benefit from using the larger models on English speech. For multilingual TTS we would probably want to switch to large-v2.\n\n\n\n\n\n\nNote\n\n\n\nRight now we extract both semantic tokens and transcriptions in one go. Doing the transcriptions is very time consuming are the result is unlikely to change. OTOH we may want to regenerate the semantic tokens if we train different quantized Whisper models. Because of that we may want to split this into two separate steps and only merge the results just before we generate the training dataset.",
    "crumbs": [
      "I can has speech? What data WhisperSpeech needs?"
    ]
  },
  {
    "objectID": "dataset preparation.html#acoustic-token-extraction",
    "href": "dataset preparation.html#acoustic-token-extraction",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Acoustic token extraction",
    "text": "Acoustic token extraction\nCode: 4A. S2A dataset preparation\nThis is basically the same as T2S above but with Encodec instead of Whisper.",
    "crumbs": [
      "I can has speech? What data WhisperSpeech needs?"
    ]
  },
  {
    "objectID": "dataset preparation.html#trainvalidation-split",
    "href": "dataset preparation.html#trainvalidation-split",
    "title": "I can has speech? What data WhisperSpeech needs?",
    "section": "Train/validation split",
    "text": "Train/validation split\nWe create validation splits differently for each dataset. For example for LibriLight we use the speaker labels to create a common and unseen speakers splits. Once we have a list of samples we want to use we extract them from the full dataset into a new shard while keeping a list of IDs to skip during training. This way we avoid copying the training samples.\nThis has the downside of delaying all shuffling until training. This is especially problematic for smaller datasets with not enough shards since multiple workers may read the same shard and initially (before the shuffling buffer is filled) deliver the same samples multiple times. This causes overfitting. This is not a problem early in training (the model is too random to overfit) and we make sure we don’t reset the dataloaders between epochs but it is causing issues when resuming training from a checkpoint. The workaround is to preload the shuffling bufferwith a lot of samples (.shuffle(initial=20000)). Unfortunately it has the downside of putting a lot of load on the filesystem and adding a significant delay before training can start.",
    "crumbs": [
      "I can has speech? What data WhisperSpeech needs?"
    ]
  },
  {
    "objectID": "2c. whisper quantization (semantic token) evaluation.html",
    "href": "2c. whisper quantization (semantic token) evaluation.html",
    "title": "VQ semantic token extraction evaluation",
    "section": "",
    "text": "import io\nimport time\nimport torch\nimport torchaudio\nfrom pathlib import Path\nimport json\nfrom fastprogress import progress_bar, master_bar\nimport fastprogress\nimport numpy as np\nimport pylab as plt\nimport pandas as pd\nimport random\nimport IPython\n\nimport whisper\n\nfrom fastcore.script import *\nfrom whisperspeech.wer_metrics import *"
  },
  {
    "objectID": "2c. whisper quantization (semantic token) evaluation.html#how-whisper-works-with-speech-cut-at-different-lengths",
    "href": "2c. whisper quantization (semantic token) evaluation.html#how-whisper-works-with-speech-cut-at-different-lengths",
    "title": "VQ semantic token extraction evaluation",
    "section": "How Whisper works with speech cut at different lengths",
    "text": "How Whisper works with speech cut at different lengths\n\ndef test_incremental(model_name, Tmax=15):\n    whmodel = whisper.load_model(model_name)\n    for i in range(Tmax):\n        print(i, whmodel.transcribe(snd[0,:int(i*16000)])['text'])\n\n\ntest_incremental('tiny.en')\n\n0 \n1  Chapter\n2  Chapter 5 of the\n3  Chapter 5 of the things in our garden.\n4  Chapter 5 of the Things in Our Garden by Arthur Rachael.\n5  Chapter 5 of the things in our garden by Arthur Ransom.\n6  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox Recordings.\n7  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox recording is in the public.\n8  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox recording is in the public domain.\n9  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox recording is in the public domain. Chapter 5\n10  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox recording is in the public domain. Chapter 5, their own garden.\n11  Chapter 5 of the Things in Our Garden by Arthur Ransom. This LibraVox recording is in the public domain. Chapter 5, Their Own Gardens.\n12  Chapter 5 of the Things in Our Garden by Arthur Ransom. This Libra-Vox recording is in the public domain. Chapter 5, Their Own Gardens.\n13  Chapter 5 of the Things in Our Garden by Arthur Ransom. This Libra-Vox recording is in the public domain. Chapter 5, their own gardens, close by the wood at the\n14  Chapter 5 of the Things in Our Garden by Arthur Ransom. This Libra box recording is in the public domain. Chapter 5, Their Own Gardens, Close by the wood at the bottom of the garden.\n\n\n\ntest_incremental('base.en')\n\n0 \n1  Chapter 4\n2  Chapter 5 of the\n3  Chapter 5 of the Things in our Guard.\n4  Chapter 5 of The Things in Our Garden by Arthur Raffy\n5  Chapter 5 of The Things in Our Garden by Arthur Ransom.\n6  Chapter 5 of The Things in Our Garden by Arthur Ransom.\n7  CHAPTER V.\n8  CHAPTER V.\n9  CHAPTER V.\n10  CHAPTER V.\n11  CHAPTER V.\n12  CHAPTER V. Their Own Gardens.\n13  CHAPTER V. Their Own Gardens.\n14  CHAPTER V.\n\n\n\ntest_incremental('large-v2')\n\n0 \n1  Chapter 4.\n2  Chapter 5 of the\n3  Chapter 5 of The Things in Our Garden\n4  V. THE THINGS IN OUR GARDEN\n5  V. THE THINGS IN OUR GARDEN.\n6  CHAPTER V\n7  V. THE THINGS IN OUR GARDEN\n8  CHAPTER V\n9  CHAPTER V\n10  V. THEIR OWN GARDEN\n11  V. THEIR OWN GARDENS\n12  V. THEIR OWN GARDENS\n13  V. THEIR OWN GARDENS CLOSE BY THE WOOD\n14  V. THEIR OWN GARDENS CLOSE BY THE WOOD AT THE BOTTOM OF THE GARDEN"
  },
  {
    "objectID": "2c. whisper quantization (semantic token) evaluation.html#entropy-of-the-token-stream",
    "href": "2c. whisper quantization (semantic token) evaluation.html#entropy-of-the-token-stream",
    "title": "VQ semantic token extraction evaluation",
    "section": "Entropy of the token stream",
    "text": "Entropy of the token stream\n\nfrom whisperspeech.vq_stoks import RQBottleneckTransformer\n\n\nimport collections\ndef calc_model_entropy(ds, modelfile):\n    vqmodel = RQBottleneckTransformer.load_model(local_filename=modelfile).cuda()\n    cnts = collections.Counter()\n    for snd,txt in ds:\n        stoks = vqmodel.encode_audio(snd.cuda())\n        cnts.update(stoks[0].tolist())\n    pdf = torch.tensor([cnts[i] for i in range(max(cnts)+1)])\n    pdf = pdf / pdf.sum()\n    return -torch.nansum(pdf * np.log2(pdf))\n\n\n# the original semantic token model from early 2023\ncalc_model_entropy(make_test_ds(), None)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n6.097853445304322\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-ce9.2.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n6.357563112144668\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-256c.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n3.0997004132066834\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-256c-cosine.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n5.6921860685011225\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-2d-256c.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n2.899952018598168\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-2d-256c-cosine.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n5.769594466589709\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-2d-256c-cosine-padfix2.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n7.741530540488036\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-2d-512c-cosine-padfix-premlp-learnpos-5e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n8.164144580014993\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-2d-512c-cosine32-padfix-premlp-learnpos-5e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:04&lt;00:00]\n    \n    \n\n\n11.37221612373814\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:08&lt;00:00]\n    \n    \n\n\n11.240560444030649\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-1024c-cosine32-padfix-premlp-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:17&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_276/107266959.py:11: RuntimeWarning: divide by zero encountered in log2\n  return -torch.nansum(pdf * np.log2(pdf))\n\n\ntensor(9.6971)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:06&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_276/107266959.py:11: RuntimeWarning: divide by zero encountered in log2\n  return -torch.nansum(pdf * np.log2(pdf))\n\n\ntensor(11.4108)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:17&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_103351/107266959.py:11: RuntimeWarning: divide by zero encountered in log2\n  return -torch.nansum(pdf * np.log2(pdf))\n\n\ntensor(9.9410)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:17&lt;00:00]\n    \n    \n\n\n/tmp/ipykernel_9385/107266959.py:11: RuntimeWarning: divide by zero encountered in log2\n  return -torch.nansum(pdf * np.log2(pdf))\n\n\ntensor(11.2880)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-60k.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:20&lt;00:00]\n    \n    \n\n\ntensor(11.4831)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vq-base.en-2d-4096c-60k.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:20&lt;00:00]\n    \n    \n\n\ntensor(11.4831)\n\n\n\n# 4096 tokens, we later found out that tokens from this model do carry speaker information\ncalc_model_entropy(make_test_ds(), \"vqmodel-4e-hyptuned-32gpu.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:05&lt;00:00]\n    \n    \n\n\ntensor(11.6404)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vqmodel-256c-4e-hyptuned-32gpu.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:05&lt;00:00]\n    \n    \n\n\ntensor(8.7963)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vqmodel-256c-dim64-4e-hyptuned-32gpu.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:14&lt;00:00]\n    \n    \n\n\ntensor(8.7499)\n\n\n\ncalc_model_entropy(make_test_ds(), \"vqmodel-base-en+pl-512c-dim64.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:14&lt;00:00]\n    \n    \n\n\ntensor(8.3956)\n\n\n\n# the final model\ncalc_model_entropy(make_test_ds(), \"vqmodel-medium-en+pl-512c-dim64.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 00:45&lt;00:00]\n    \n    \n\n\ntensor(8.4314)"
  },
  {
    "objectID": "2c. whisper quantization (semantic token) evaluation.html#word-error-rate-measurements",
    "href": "2c. whisper quantization (semantic token) evaluation.html#word-error-rate-measurements",
    "title": "VQ semantic token extraction evaluation",
    "section": "Word Error Rate measurements",
    "text": "Word Error Rate measurements\n\nfrom whisperspeech.wer_metrics import *\n\n\nVanilla Whisper models\n\ndef test_wh_model(whmodel):\n    decoding_options=whisper.DecodingOptions(language='en')\n    stats = WERStats()\n    for snd, gt_text in progress_bar(librispeech_data('/data/LibriSpeech/test-clean'), total=1000):\n        text = whmodel.decode(whisper.log_mel_spectrogram(whisper.pad_or_trim(snd[0])).cuda(), decoding_options).text\n        diff = stats.push_sample(snd, gt_text, text)\n        last_diff = diff.alignments[0][-1]\n        stats.push(hallucination = last_diff.type == 'insert' and last_diff.hyp_end_idx - last_diff.hyp_start_idx &gt; 3)\n    stats = stats.df().sort_values('wer')\n    print(f\"WER: {stats.wer.mean()*100:.2f}%\")\n    print(f\"WER (w/o hallucinations): {stats[~stats['hallucination']].wer.mean()*100:.2f}%\")\n    return stats\n\n\ntest_wh_model(whisper.load_model('tiny.en'))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:05&lt;00:00]\n    \n    \n\n\nWER: 6.91%\nWER (w/o hallucinations): 6.91%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n355\n2.885\nNone\nI'M AFRAID I DON'T KNOW MUCH ABOUT THE LAND OF OZ\nI'm afraid I don't know much about the land of...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n353\n5.870\nNone\nTHE FIRST LOT WE TESTED ON OUR GLASS CAT WHICH...\nThe first lot we tested on our glass cat, whic...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n674\n2.295\nNone\nHE ONLY SHOOK HIS HEAD\nHe only shook his head.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n675\n11.545\nNone\nWELL BUT NOW SAID THE PRINCESS AND SHE FILLED ...\nWell, but now said the princess, and she fille...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n524\n3.195\nNone\nBROTHER MAC ARDLE BROTHER KEOGH\nBrother Maccardo, Brother Keoff.\n0.600000\n0.600000\n0.800000\n0.200000\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHans Stairz-Nied.\n0.666667\n0.666667\n0.888889\n0.111111\nFalse\n\n\n820\n2.155\nNone\nTHE FORMER BOOLOOROO GROANED\nThe former Billie Rook-Round\n0.750000\n0.600000\n0.800000\n0.200000\nFalse\n\n\n918\n3.000\nNone\nTHAT IS TRUE BADAUDERIE\nThat is true bad dealt gree.\n0.750000\n0.500000\n0.625000\n0.375000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCause A was my man's servant.\n1.250000\n0.714286\n0.857143\n0.142857\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_wh_model(whisper.load_model('base.en'))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:41&lt;00:00]\n    \n    \n\n\nWER: 5.08%\nWER (w/o hallucinations): 5.08%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_text\ntext\nwer\nhallucination\n\n\n\n\n0\n8.230\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said while on her lap ...\n0.000000\nFalse\n\n\n403\n5.370\nDEPARTING FROM FIVE HUNDRED THOUSAND THROATS T...\nDeparting from 500,000 throats, three cheers b...\n0.000000\nFalse\n\n\n404\n13.140\nTHOUSANDS OF HANDKERCHIEFS WERE WAVING ABOVE T...\nThousands of handkerchiefs were waving above t...\n0.000000\nFalse\n\n\n405\n2.695\nIT'S ALMOST BEYOND CONJECTURE\nIt's almost beyond conjecture.\n0.000000\nFalse\n\n\n406\n7.805\nTHIS REALITY BEGINS TO EXPLAIN THE DARK POWER ...\nThis reality begins to explain the dark power ...\n0.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n524\n3.195\nBROTHER MAC ARDLE BROTHER KEOGH\nBrother McCartill, Brother Kiaff.\n0.600000\nFalse\n\n\n592\n1.805\nHANS STIRS NOT\nHans Sturznide.\n0.666667\nFalse\n\n\n918\n3.000\nTHAT IS TRUE BADAUDERIE\nThat is true, bad girl degree.\n0.750000\nFalse\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCas￩ was my man's servant.\n1.000000\nFalse\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.500000\nFalse\n\n\n\n\n1000 rows ￗ 5 columns\n\n\n\n\n\ntest_wh_model(whisper.load_model('small.en'))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 02:53&lt;00:00]\n    \n    \n\n\nWER: 3.89%\nWER (w/o hallucinations): 3.84%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n789\n5.945\nNone\nAND THIS PLAN WAS ADOPTED TOO IN ORDER TO EXTR...\nand this plan was adopted too in order to extr...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n461\n10.980\nNone\nSHE MEANWHILE PASSED HER LIFE WITH HER PARENTS...\nShe, meanwhile, passed her life with her paren...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n464\n8.845\nNone\nONE DAY WHEN THE BOY WAS SENT BY HIS GRANDFATH...\nOne day when the boy was sent by his grandfath...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n465\n8.785\nNone\nTHE BED SHE TOO WELL REMEMBERED WAS THERE AND ...\nThe bed she too well remembered was there, and...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n524\n3.195\nNone\nBROTHER MAC ARDLE BROTHER KEOGH\nBrother McCardle. Brother Kiyof.\n0.600000\n0.600000\n0.800000\n0.200000\nFalse\n\n\n288\n1.905\nNone\nI DELIGHT IN YOUR KITCHEN\nby delighting your kitchen.\n0.600000\n0.600000\n0.800000\n0.200000\nFalse\n\n\n121\n15.270\nNone\nAT LAST THE LITTLE MICE STAYED AWAY ALSO AND T...\nAt last the little mice stayed away also, and ...\n0.636364\n0.636364\n0.636364\n0.363636\nFalse\n\n\n918\n3.000\nNone\nTHAT IS TRUE BADAUDERIE\nThat is true Bad Delt Grey.\n0.750000\n0.500000\n0.625000\n0.375000\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedtlos\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_wh_model(whisper.load_model('medium.en'))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 06:22&lt;00:00]\n    \n    \n\n\nWER: 4.19%\nWER (w/o hallucinations): 3.19%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_text\ntext\nwer\nhallucination\n\n\n\n\n386\n5.915\nYES WE ARE CERTAINLY I REPLIED EVASIVELY BUT A...\nYes, we are, certainly, I replied evasively, b...\n0.00\nFalse\n\n\n507\n6.480\nHIS CONDUCT AND PRESENCE OF MIND IN THIS EMERG...\nHis conduct and presence of mind in this emerg...\n0.00\nFalse\n\n\n865\n4.315\nTHEIR SUFFERINGS HAVE NEVER YET BEEN FITLY CHR...\nTheir sufferings have never yet been fitly chr...\n0.00\nFalse\n\n\n509\n13.610\nFROM THE SAME MEN NEW REGIMENTS AND NEW COMPAN...\nFrom the same men new regiments and new compan...\n0.00\nFalse\n\n\n511\n12.655\nTHOUGH THE DISCIPLINE OF THE FORMER PARLIAMENT...\nThough the discipline of the former parliament...\n0.00\nFalse\n\n\n...\n...\n...\n...\n...\n...\n\n\n782\n2.260\nTO DAY I SHOUTED\nToday, I shouted.\n0.50\nFalse\n\n\n524\n3.195\nBROTHER MAC ARDLE BROTHER KEOGH\nBrother McCardle, Brother Kiyof.\n0.60\nFalse\n\n\n918\n3.000\nTHAT IS TRUE BADAUDERIE\nThat is true bad health grief.\n0.75\nFalse\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefanos Daedalus\n1.00\nFalse\n\n\n226\n6.750\nHE CONTINUED HIS PRETENDED SEARCH AND TO GIVE ...\nHe continued his pretended search, and to give...\n9.80\nTrue\n\n\n\n\n1000 rows × 5 columns\n\n\n\n\n\ntest_wh_model(whisper.load_model('large-v2'))\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 07:39&lt;00:00]\n    \n    \n\n\nWER: 6.07%\nWER (w/o hallucinations): 3.19%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said while on her lap ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n606\n2.610\nNone\nWE SUFFER STIFLING PAINS\nWe suffer stifling pains.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n607\n7.040\nNone\nSATURDAY AUGUST FIFTEENTH THE SEA UNBROKEN ALL...\nSaturday, August 15th. The sea unbroken all ro...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n608\n3.070\nNone\nTHE HORIZON SEEMS EXTREMELY DISTANT\nThe horizon seems extremely distant.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n609\n9.985\nNone\nALL MY DANGER AND SUFFERINGS WERE NEEDED TO ST...\nAll my danger and sufferings were needed to st...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHans Sturznott\n0.666667\n0.666667\n0.833333\n0.166667\nFalse\n\n\n95\n8.800\nNone\nTHOUGHT THE FIR TREE AND BELIEVED IT ALL BECAU...\nthought the fir tree, and believed it all, bec...\n4.285714\n0.810811\n0.810811\n0.189189\nTrue\n\n\n902\n7.370\nNone\nI HAD A NAME I BELIEVE IN MY YOUNG DAYS BUT I ...\nI had a name, I believe, in my young days, but...\n7.476190\n0.882022\n0.882022\n0.117978\nTrue\n\n\n610\n7.370\nNone\nYOU SEEM ANXIOUS MY UNCLE I SAID SEEING HIM CO...\n\"'You seem anxious, my uncle,' I said, seeing ...\n7.823529\n0.886667\n0.886667\n0.113333\nTrue\n\n\n438\n6.665\nNone\nAS TO HIS AGE AND ALSO THE NAME OF HIS MASTER ...\nAs to his age, and also the name of his master...\n8.631579\n0.896175\n0.896175\n0.103825\nTrue\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\n\nQuantized Whisper models\n\ndef test_model(modelfile, N=1000):\n    vqmodel = RQBottleneckTransformer.load_model(local_filename=modelfile).cuda()\n    stats = WERStats()\n    for snd, gt_text in progress_bar(librispeech_data('/data/LibriSpeech/test-clean'), total=N):\n        stoks = vqmodel.encode_audio(snd.cuda())\n        text = vqmodel.decode_text(stoks[0])[0].text\n        diff = stats.push_sample(snd, gt_text, text)\n        last_diff = diff.alignments[0][-1]\n        stats.push(hallucination = last_diff.type == 'insert' and last_diff.hyp_end_idx - last_diff.hyp_start_idx &gt; 3)\n    stats = stats.df().sort_values('wer')\n    print(f\"WER: {stats.wer.mean()*100:.2f}%\")\n    print(f\"WER (w/o hallucinations): {stats[~stats['hallucination']].wer.mean()*100:.2f}%\")\n    return stats\n\n\ntest_model(None) # the old stoks model from early 2023\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:10&lt;00:00]\n    \n    \n\n\nWER: 16.06%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n207\n4.075\nSEVERAL HUNDRED FREE STATE MEN PROMPTLY RESPON...\nseveral hundred free state men promptly respon...\n0.000000\n\n\n209\n5.295\nTHE LEADERS OF THE CONSPIRACY BECAME DISTRUSTF...\nThe leaders of the conspiracy became distrustf...\n0.000000\n\n\n709\n2.440\nTHE THREE MODES OF MANAGEMENT\nThe three modes of management.\n0.000000\n\n\n708\n13.020\nTHE PAIN PRODUCED BY AN ACT OF HASTY AND ANGRY...\nThe pain produced by an act of hasty and angry...\n0.000000\n\n\n705\n5.250\nTHEY ARE CHIEFLY FORMED FROM COMBINATIONS OF T...\nThey are chiefly formed from combinations of t...\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCOSA was my man's servant.\n1.000000\n\n\n144\n4.680\nAND BESIDES SUPPOSE THEE DOES LEARN MEDICINE\nand be sides, supposed to be lost, Lord medicine.\n1.000000\n\n\n907\n4.195\nMADAME QUINSON BESIDES CAN ANSWER YOUR ENQUIRIES\nMadam Gwen-Saun, besides Ken Sir Ian Corrie's.\n1.142857\n\n\n187\n2.230\nNO ITS NOT TOO SOON\nKnow what's sought to assume.\n1.200000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephano's Nerdos.\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\ntest_model('vq-ce9.2.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:12&lt;00:00]\n    \n    \n\n\nWER: 8.80%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n0\n8.230\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n\n\n283\n1.420\nDIRECTION\ndirection.\n0.000000\n\n\n282\n2.385\nI DIDN'T PREACH WITHOUT DIRECTION\nI didn't preach without direction.\n0.000000\n\n\n624\n3.975\nI SHUDDER AS I RECALL THESE MONSTERS TO MY REM...\nI shudder as I recall these monsters to my rem...\n0.000000\n\n\n279\n10.490\nWE CAN ALL BE SERVANTS OF GOD WHEREVER OUR LOT...\nWe can all be servants of God, wherever our lo...\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n820\n2.155\nTHE FORMER BOOLOOROO GROANED\nthe former Boula Rook round.\n0.750000\n\n\n918\n3.000\nTHAT IS TRUE BADAUDERIE\nThat is true, bad, old-gree.\n0.750000\n\n\n105\n6.555\nIF IT ONLY WERE NOT SO DARK HERE AND SO TERRIB...\nIf... ... ... ... ... ... ... ... ... ... ... ...\n0.916667\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nJose was my man's servant.\n1.000000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefanos de los\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\ntest_model('vq-256c.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:13&lt;00:00]\n    \n    \n\n\nWER: 10.26%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n789\n5.945\nAND THIS PLAN WAS ADOPTED TOO IN ORDER TO EXTR...\nAnd this plan was adopted too, in order to ext...\n0.000\n\n\n365\n5.780\nI WILL SHOW YOU WHAT A GOOD JOB I DID AND SHE ...\nI will show you what a good job I did. And she...\n0.000\n\n\n722\n10.720\nAS I SPOKE I MADE HIM A GRACIOUS BOW AND I THI...\nAs I spoke, I made him a gracious bow, and I t...\n0.000\n\n\n723\n7.840\nI HAVE COME TO YOUR SHORES MISTER PRESIDENT WI...\nI have come to your shores, Mr. President, wit...\n0.000\n\n\n362\n5.335\nSOMETIMES IT IS CALLED A CRAZY QUILT BECAUSE T...\nSometimes it is called a crazy quilt because t...\n0.000\n\n\n...\n...\n...\n...\n...\n\n\n106\n2.020\nSQUEAK SQUEAK\nSquick. Squick.\n1.000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephanos de Arlos.\n1.000\n\n\n288\n1.905\nI DELIGHT IN YOUR KITCHEN\nI'd like to introduce you in your kitchen.\n1.000\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCall say, was my man servant?\n1.000\n\n\n381\n4.880\nCONSEIL I CALLED A THIRD TIME CONSEIL APPEARED\nCan't say, at call the third time. Can't say a...\n1.125\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\ntest_model('vq-256c-cosine.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:10&lt;00:00]\n    \n    \n\n\nWER: 10.24%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n710\n11.490\nTO SUPPOSE THAT THE OBJECT OF THIS WORK IS TO ...\nTo suppose that the object of this work is to ...\n0.000000\n\n\n629\n3.235\nTWO HOURS AFTERWARDS A TERRIBLE SHOCK AWOKE ME\nTwo hours afterwards, a terrible shock awoke me.\n0.000000\n\n\n640\n1.740\nPOOR ALICE\nPoor Alice.\n0.000000\n\n\n262\n2.435\nTHAT'S WHAT YOU'D LIKE TO BE DOING IS IT\nThat's what you'd like to be doing, is it?\n0.000000\n\n\n644\n3.105\nAND YESTERDAY THINGS WENT ON JUST AS USUAL\nAnd yesterday, things went on just as usual.\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n187\n2.230\nNO ITS NOT TOO SOON\nNo, it's not just here.\n0.800000\n\n\n115\n4.470\nWHO IS HUMPY DUMPY ASKED THE MICE\nWho is a MP? Don't be. Ask the mice.\n0.857143\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCross say, was my man servant.\n1.000000\n\n\n106\n2.020\nSQUEAK SQUEAK\nquick, quick.\n1.000000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephenos der los.\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\ntest_model('vq-2d-256c.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:11&lt;00:00]\n    \n    \n\n\nWER: 21.75%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n709\n2.440\nTHE THREE MODES OF MANAGEMENT\nThe Three Modes of Management\n0.000000\n\n\n419\n2.415\nFATHOM SIX FEET\nFathom six feet.\n0.000000\n\n\n703\n4.775\nNATURE OF THE EFFECT PRODUCED BY EARLY IMPRESS...\nnature of the effect produced by early impress...\n0.000000\n\n\n693\n2.110\nI AM VERY GLAD\nI am very glad.\n0.000000\n\n\n686\n2.740\nNO MY LITTLE SON SHE SAID\nNo, my little son, she said.\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n627\n3.060\nTUESDAY AUGUST EIGHTEENTH\n2. Day August 8th\n1.000000\n\n\n820\n2.155\nTHE FORMER BOOLOOROO GROANED\nThe former Bill of Rook around.\n1.000000\n\n\n28\n5.530\nKESWICK MARCH TWENTY SECOND EIGHTEEN THIRTY SE...\nYes, we wish between second 1837. Did you reme...\n1.333333\n\n\n106\n2.020\nSQUEAK SQUEAK\nQuick, quick, quick.\n1.500000\n\n\n792\n1.810\nVENICE\nThen Next\n2.000000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\ntest_model('vq-2d-256c-cosine.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:11&lt;00:00]\n    \n    \n\n\nWER: 11.61%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n686\n2.740\nNO MY LITTLE SON SHE SAID\nNo, my little son, she said.\n0.000000\n\n\n902\n7.370\nI HAD A NAME I BELIEVE IN MY YOUNG DAYS BUT I ...\nI had a name I believe in my young days, but I...\n0.000000\n\n\n904\n3.300\nYOU DO ME A GREAT HONOUR\nYou do me a great honor.\n0.000000\n\n\n228\n6.775\nAS HE HAD PROMISED TO PROTECT THE HOTEL THE RE...\nAs he had promised to protect the hotel, the r...\n0.000000\n\n\n521\n3.440\nSOON THE WHOLE BRIDGE WAS TREMBLING AND RESOUN...\nSoon the whole bridge was trembling and resoun...\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n918\n3.000\nTHAT IS TRUE BADAUDERIE\nThat is true, bad, old-gree.\n0.750000\n\n\n381\n4.880\nCONSEIL I CALLED A THIRD TIME CONSEIL APPEARED\nConse, at call to third town. Conse, appeared.\n0.750000\n\n\n115\n4.470\nWHO IS HUMPY DUMPY ASKED THE MICE\nWho eats umpi, don't pee? Ask the mice.\n0.857143\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephenau Stairlauce.\n1.000000\n\n\n106\n2.020\nSQUEAK SQUEAK\nSpeak. Speak. Speak.\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\n# full crop\ntest_model('vq-2d-256c-cosine-padfix2.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:10&lt;00:00]\n    \n    \n\n\nWER: 16.13%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n652\n3.475\nI AM SO VERY TIRED OF BEING ALL ALONE HERE\nI'm so very tired of being all alone here.\n0.000000\n\n\n906\n2.610\nAT YOUR SERVICE SIR\nAt your service, sir.\n0.000000\n\n\n904\n3.300\nYOU DO ME A GREAT HONOUR\nYou do me a great honor.\n0.000000\n\n\n902\n7.370\nI HAD A NAME I BELIEVE IN MY YOUNG DAYS BUT I ...\nI had a name I believe in my young days, but I...\n0.000000\n\n\n901\n2.755\nI NEVER HAD ANY FAMILY\nI never had any family.\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n448\n2.215\nWHO TOUCHES ME AM I IN BED\nLook at us, me, our young dad.\n1.000000\n\n\n934\n4.205\nI RESIDE IN THE MARAIS RUE DE DOUZE PORTES\nIrae's eye in the Ma'rae's crew did to support.\n1.111111\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStep 4, Zetelos.\n1.500000\n\n\n16\n1.695\nFAREWELL MADAM\nFair will, damn.\n1.500000\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCos they were my man's servant.\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\n# no cropping\ntest_model('vq-2d-256c-cosine-padfix2.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:10&lt;00:00]\n    \n    \n\n\nWER: 11.17%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n839\n2.275\nTHE CAPTAIN SHOOK HIS HEAD\nThe captain shook his head.\n0.000000\n\n\n408\n9.935\nNEMO BUILDS A FABULOUS FUTURISTIC SUBMARINE TH...\nNemo builds a fabulous futuristic submarine, t...\n0.000000\n\n\n405\n2.695\nIT'S ALMOST BEYOND CONJECTURE\nIt's almost beyond conjecture.\n0.000000\n\n\n404\n13.140\nTHOUSANDS OF HANDKERCHIEFS WERE WAVING ABOVE T...\nThousands of handkerchiefs were waving above t...\n0.000000\n\n\n790\n14.900\nBRIGHTER THAN EARLY DAWN'S MOST BRILLIANT DYE ...\nBrighter than early dawn's most brilliant dye ...\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephenos dellos\n1.000000\n\n\n592\n1.805\nHANS STIRS NOT\nHonsters nod.\n1.000000\n\n\n907\n4.195\nMADAME QUINSON BESIDES CAN ANSWER YOUR ENQUIRIES\nMadam Quinsong, besides Cinanza, you're in que...\n1.000000\n\n\n115\n4.470\nWHO IS HUMPY DUMPY ASKED THE MICE\nPhew, he's on P, don't pee. Ask the mice.\n1.142857\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCos they were my man's servant.\n1.500000\n\n\n\n\n1000 rows ￗ 4 columns\n\n\n\n\n\n# crop to 200 toks minimum\ntest_model('vq-2d-256c-cosine-padfix2.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:10&lt;00:00]\n    \n    \n\n\nWER: 12.56%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n871\n2.920\nWHO BEGAN THE QUARREL WAS IT THE MORMONS\nWho began the quarrel? Was it the Mormons?\n0.000000\n\n\n938\n12.435\nHOW STRANGE IT SEEMED TO THE SAD WOMAN AS SHE ...\nHow strange it seemed to the sad woman, as she...\n0.000000\n\n\n937\n12.605\nHIS HOUSEKEEPER HAD THE MANAGEMENT OF EVERYTHI...\nHis housekeeper had the management of everythi...\n0.000000\n\n\n558\n15.720\nIT WAS STRANGE TOO THAT HE FOUND AN ARID PLEAS...\nIt was strange too, that he found an arid plea...\n0.000000\n\n\n305\n3.835\nTHE HEAD OF THE PATCHWORK GIRL WAS THE MOST CU...\nThe head of the patchwork girl was the most cu...\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephenos dellos\n1.000000\n\n\n907\n4.195\nMADAME QUINSON BESIDES CAN ANSWER YOUR ENQUIRIES\nMadam Quinsong, besides Cenanza, you're in que...\n1.000000\n\n\n106\n2.020\nSQUEAK SQUEAK\nQuick, quick.\n1.000000\n\n\n115\n4.470\nWHO IS HUMPY DUMPY ASKED THE MICE\nP-E-S-A-P, don't be... asked the mice.\n1.142857\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCos-A was my man's servant.\n1.250000\n\n\n\n\n1000 rows ￗ 4 columns\n\n\n\n\n\n# crop to audio length\ntest_model('vq-2d-512c-cosine-padfix-premlp-learnpos.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:09&lt;00:00]\n    \n    \n\n\nWER: 9.89%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n570\n2.715\nBEWARE OF MAKING THAT MISTAKE\nBeware of making that mistake.\n0.000000\n\n\n260\n3.155\nWHO TAUGHT YOU TO SCRUB A FLOOR I SHOULD LIKE ...\nWho taught you to scrub a floor? I should like...\n0.000000\n\n\n800\n5.770\nOLD DANCES ARE SIMPLIFIED OF THEIR YEARNING BL...\nOld dances are simplified of their yearning, b...\n0.000000\n\n\n258\n2.260\nSPINNING INDEED\nSpinning Indeed.\n0.000000\n\n\n653\n3.815\nAND I DECLARE IT'S TOO BAD THAT IT IS\nAnd I declare it's too bad that it is.\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n934\n4.205\nI RESIDE IN THE MARAIS RUE DE DOUZE PORTES\nIries I'd in the Marfra Grudetus port.\n0.777778\n\n\n115\n4.470\nWHO IS HUMPY DUMPY ASKED THE MICE\nWho is a P-Don't Be? Ask the mice.\n0.857143\n\n\n448\n2.215\nWHO TOUCHES ME AM I IN BED\nPotatys me, and my embed.\n0.857143\n\n\n592\n1.805\nHANS STIRS NOT\nHan Stersnide\n1.000000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefanos de los\n1.500000\n\n\n\n\n1000 rows ￗ 4 columns\n\n\n\n\n\n# crop to audio length\ntest_model('vq-2d-512c-cosine-padfix-premlp-learnpos-5e.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:09&lt;00:00]\n    \n    \n\n\nWER: 9.51%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_texts\ntexts\nwers\n\n\n\n\n0\n8.230\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000\n\n\n607\n7.040\nSATURDAY AUGUST FIFTEENTH THE SEA UNBROKEN ALL...\nSaturday, August 15th. The sea unbroken all ro...\n0.000\n\n\n608\n3.070\nTHE HORIZON SEEMS EXTREMELY DISTANT\nThe horizon seems extremely distant.\n0.000\n\n\n615\n3.735\nTHEREFORE DON'T TALK TO ME ABOUT VIEWS AND PRO...\nTherefore, don't talk to me about views and pr...\n0.000\n\n\n616\n5.795\nI TAKE THIS AS MY ANSWER AND I LEAVE THE PROFE...\nI take this as my answer and I leave the profe...\n0.000\n\n\n...\n...\n...\n...\n...\n\n\n157\n3.830\nAND THEE WON'T GO WHY SHOULD I\nAnd, see you all next time!\n0.875\n\n\n381\n4.880\nCONSEIL I CALLED A THIRD TIME CONSEIL APPEARED\nCan say, at call the third time, can say appea...\n0.875\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCaus￩ was my man's servant.\n1.000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefanos dellos.\n1.000\n\n\n106\n2.020\nSQUEAK SQUEAK\nSweet, sweet.\n1.000\n\n\n\n\n1000 rows ￗ 4 columns\n\n\n\n\n\n# crop to audio length\ntest_model('vq-2d-512c-cosine32-padfix-premlp-learnpos-5e.model')\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:08&lt;00:00]\n    \n    \n\n\nWER: 9.84%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_text\ntext\nwer\n\n\n\n\n310\n4.040\nSHE POURED INTO THE DISH A QUANTITY FROM EACH ...\nShe poured into the dish a quantity from each ...\n0.0\n\n\n387\n2.735\nA ROUTE SLIGHTLY LESS DIRECT THAT'S ALL\na route slightly less direct, that's all.\n0.0\n\n\n385\n4.530\nANYHOW WE'LL LEAVE INSTRUCTIONS TO SHIP THE WH...\nAnyhow, we'll leave instructions to ship the w...\n0.0\n\n\n742\n4.730\nWE SAT WITH THE OFFICERS SOME LITTLE TIME AFTE...\nWe sat with the officers some little time afte...\n0.0\n\n\n383\n9.300\nPACK AS MUCH INTO MY TRUNK AS YOU CAN MY TRAVE...\nPack as much into my trunk as you can. My trav...\n0.0\n\n\n...\n...\n...\n...\n...\n\n\n559\n13.895\nTHE SENTENCE OF SAINT JAMES WHICH SAYS THAT HE...\nThank you.\n1.0\n\n\n775\n5.545\nTHE PECULIAR CIRCUMSTANCES OF THE COLONY ARE W...\nThank you.\n1.0\n\n\n106\n2.020\nSQUEAK SQUEAK\nQuick, quick.\n1.0\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStephanos de los\n1.0\n\n\n491\n4.805\nTHE PARLIAMENT AND THE SCOTS LAID THEIR PROPOS...\nThank you.\n1.0\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:41&lt;00:00]\n    \n    \n\n\nWER: 7.51%\n\n\n\n\n\n\n\n\n\n\nsecs\ngt_text\ntext\nwer\n\n\n\n\n862\n6.720\nTO THE FERVENT LATTER DAY SAINT A TEMPLE IS NO...\nTo the fervent Latter-day Saint, a temple is n...\n0.000000\n\n\n436\n6.380\nSHE WAS A LARGE HOMELY WOMAN THEY WERE COMMON ...\nShe was a large, homely woman. They were commo...\n0.000000\n\n\n437\n5.425\nSUBSTANTIALLY THIS WAS JACOB'S UNVARNISHED DES...\nSubstantially, this was Jacob's unvarnished de...\n0.000000\n\n\n438\n6.665\nAS TO HIS AGE AND ALSO THE NAME OF HIS MASTER ...\nAs to his age and also the name of his master,...\n0.000000\n\n\n439\n3.020\nOF STARTING I DIDN'T KNOW THE WAY TO COME\nof starting. I didn't know the way to come.\n0.000000\n\n\n...\n...\n...\n...\n...\n\n\n480\n12.510\nTHIS WAS DONE FOR THE EVENT TOOK PLACE AT A TI...\nThis was done for the event took place.\n0.783784\n\n\n713\n17.945\nTHE MOTHER AS SOON AS THE CHAISE IS SO FAR TUR...\nThe Mother. As soon as the chase\n0.869565\n\n\n454\n7.720\nAMONG OTHER THINGS ON WHICH SHE CAST HER EYES ...\nAmong other things...\n0.869565\n\n\n371\n2.440\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n\n\n538\n2.215\nSTEPHANOS DEDALOS\nStefano Staedt-Los\n1.500000\n\n\n\n\n1000 rows × 4 columns\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:33&lt;00:00]\n    \n    \n\n\nWER: 7.49%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n714\n8.010\nNone\nSO YOU WILL BE A GOOD GIRL I KNOW AND NOT MAKE...\nSo you will be a good girl, I know, and not ma...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n365\n5.780\nNone\nI WILL SHOW YOU WHAT A GOOD JOB I DID AND SHE ...\nI will show you what a good job I did, and she...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n608\n3.070\nNone\nTHE HORIZON SEEMS EXTREMELY DISTANT\nThe horizon seems extremely distant.\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n362\n5.335\nNone\nSOMETIMES IT IS CALLED A CRAZY QUILT BECAUSE T...\nSometimes it is called a crazy quilt because t...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n361\n6.045\nNone\nA BED QUILT MADE OF PATCHES OF DIFFERENT KINDS...\nA bed quilt made of patches of different kinds...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n480\n12.510\nNone\nTHIS WAS DONE FOR THE EVENT TOOK PLACE AT A TI...\nThis was done for the event took place.\n0.783784\n0.783784\n0.783784\n0.216216\n\n\n454\n7.720\nNone\nAMONG OTHER THINGS ON WHICH SHE CAST HER EYES ...\nAmong other things...\n0.869565\n0.869565\n0.869565\n0.130435\n\n\n713\n17.945\nNone\nTHE MOTHER AS SOON AS THE CHAISE IS SO FAR TUR...\nThe Mother. As soon as the chase\n0.869565\n0.869565\n0.888199\n0.111801\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Los\n1.500000\n1.000000\n1.000000\n0.000000\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n\ntest_model(\"vq-base.en-2d-1024c-cosine32-padfix-premlp-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:31&lt;00:00]\n    \n    \n\n\nWER: 10.44%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n669\n8.540\nNone\nAT THE FARTHER END OF THE LARGEST HALL A TABLE...\nAt the farther end of the largest hall, a tabl...\n0.0\n0.0\n0.0\n1.0\n\n\n349\n2.130\nNone\nTHE WOMAN SEEMED THOUGHTFUL\nThe woman seemed thoughtful.\n0.0\n0.0\n0.0\n1.0\n\n\n572\n4.090\nNone\nHE IS CALLED AS YOU KNOW THE APOSTLE OF THE IN...\nHe is called, as you know, the apostle of the ...\n0.0\n0.0\n0.0\n1.0\n\n\n347\n3.665\nNone\nOJO HAD NEVER EATEN SUCH A FINE MEAL IN ALL HI...\nOjo had never eaten such a fine meal in all hi...\n0.0\n0.0\n0.0\n1.0\n\n\n346\n3.705\nNone\nAND YOU MUST BE OJO THE UNLUCKY SHE ADDED\nAnd you must be Ojo the unlucky,\" she added.\n0.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n896\n18.540\nNone\nSILVIA WAS THE ADORATION OF FRANCE AND HER TAL...\nSylvia\n1.0\n1.0\n1.0\n0.0\n\n\n689\n5.995\nNone\nDELLA HAD A YOUNG SISTER NAMED MARIA AND A COU...\nDela.\n1.0\n1.0\n1.0\n0.0\n\n\n512\n27.525\nNone\nVALOR INDEED WAS VERY GENERALLY DIFFUSED OVER ...\nVala.\n1.0\n1.0\n1.0\n0.0\n\n\n897\n23.740\nNone\nSILVIA DID NOT THINK THAT HER GOOD CONDUCT WAS...\nSylvia.\n1.0\n1.0\n1.0\n0.0\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.5\n1.0\n1.0\n0.0\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:45&lt;00:00]\n    \n    \n\n\nWER: 6.64%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n377\n3.910\nNone\nHE WENT HERE THERE AND EVERYWHERE IN PERFECT C...\nHe went here, there, and everywhere in perfect...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n376\n8.340\nNone\nNEVER DID HE OBJECT TO BUCKLING UP HIS SUITCAS...\nNever did he object to buckling up his suitcas...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n687\n8.500\nNone\nIF YOU DRESSED IN SILK AND GOLD FROM TOP TO TO...\nIf you dressed in silk and gold from top to to...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n688\n11.125\nNone\nTO SUCH PERSONS THESE INDIRECT MODES OF TRAINI...\nTo such persons, these indirect modes of train...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n918\n3.000\nNone\nTHAT IS TRUE BADAUDERIE\nThat is true bad-dulch-gree.\n0.750000\n0.500000\n0.625000\n0.375000\n\n\n46\n25.640\nNone\nA GOOD NEIGHBOUR OF THE BRONTES A CLEVER INTEL...\nA good neighbor of the Bronte's, a clever, int...\n0.797101\n0.785714\n0.836957\n0.163043\n\n\n221\n15.060\nNone\nIN THE SHOOTING OF SHERIFF JONES IN LAWRENCE A...\nIn the shooting of Sheriff's\n0.857143\n0.857143\n0.880952\n0.119048\n\n\n879\n17.840\nNone\nTHEY KNEW NO NORTH NO SOUTH NO EAST NO WEST TH...\nThey knew no North.\n0.925926\n0.925926\n0.925926\n0.074074\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedalus.\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n\n_9.plot.scatter('secs', 'wer', alpha=.2)\n\n\n\n\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:36&lt;00:00]\n    \n    \n\n\nWER: 6.34%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n696\n18.415\nNone\nFOR INSTANCE ONE DAY THE CHILDREN HAD BEEN PLA...\nFor instance, one day the children had been pl...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n370\n2.340\nNone\nBUT NOW NOTHING COULD HOLD ME BACK\nBut now nothing could hold me back.\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n369\n9.340\nNone\nI WANTED NOTHING MORE THAN TO SEE MY COUNTRY A...\nI wanted nothing more than to see my country a...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n368\n6.190\nNone\nEVEN SO I HAD JUST RETURNED FROM AN ARDUOUS JO...\nEven so, I had just returned from an arduous j...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n820\n2.155\nNone\nTHE FORMER BOOLOOROO GROANED\nthe former Boula-Ri-Growned.\n0.750000\n0.600000\n0.800000\n0.200000\n\n\n843\n2.110\nNone\nFINE GLORIOUS\nFind. Chlorious.\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHon Stir's Night.\n1.333333\n1.000000\n1.000000\n0.000000\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.500000\n1.000000\n1.000000\n0.000000\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-repro.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:30&lt;00:00]\n    \n    \n\n\nWER: 10.00%\nWER (w/o hallucinations): 10.00%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n696\n18.415\nNone\nFOR INSTANCE ONE DAY THE CHILDREN HAD BEEN PLA...\nFor instance, one day the children had been pl...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n594\n4.865\nNone\nI REFER TO THE THERMOMETER IT INDICATES THE FI...\nI refer to the thermometer, it indicates the f...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n738\n8.105\nNone\nTHEN THERE WERE THREE OR FOUR LEADING MEN OF T...\nThen there were three or four leading men of t...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n355\n2.885\nNone\nI'M AFRAID I DON'T KNOW MUCH ABOUT THE LAND OF OZ\nI'm afraid I don't know much about the land of...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n354\n9.840\nNone\nI THINK THE NEXT GLASS CAT THE MAGICIAN MAKES ...\nI think the next glass cat the magician makes ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n886\n22.095\nNone\nTHIS MEANT THAT FOR AN ALLEGED MISDEMEANOR FOR...\nThis is the end of the video.\n0.949153\n0.949153\n0.978208\n0.021792\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n85\n2.610\nNone\nTHIS EVENING THEY ALL SAID\nThis is the end of the video.\n1.200000\n0.857143\n0.971429\n0.028571\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n418\n17.640\nNone\nFOR MANY THEN THIS BOOK HAS BEEN A SOURCE OF F...\nFor many then, this is the end of the video. F...\n2.923077\n0.942149\n0.989616\n0.010384\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-repro.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:28&lt;00:00]\n    \n    \n\n\nWER: 7.82%\nWER (w/o hallucinations): 7.82%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n709\n2.440\nNone\nTHE THREE MODES OF MANAGEMENT\nthe three modes of management.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n740\n2.715\nNone\nBUT I MEAN TO HAVE MY INNINGS BEFORE LONG\nBut I mean to have my innings before long.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n362\n5.335\nNone\nSOMETIMES IT IS CALLED A CRAZY QUILT BECAUSE T...\nSometimes it is called a crazy quilt because t...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n361\n6.045\nNone\nA BED QUILT MADE OF PATCHES OF DIFFERENT KINDS...\nA bed quilt made of patches of different kinds...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n605\n6.305\nNone\nA SUFFOCATING SMELL OF NITROGEN FILLS THE AIR ...\nA suffocating smell of nitrogen fills the air....\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n793\n14.580\nNone\nIN A SUNSET GLOWING OF CRIMSON AND GOLD SHE LI...\nIn a sunset\n0.906250\n0.906250\n0.906250\n0.093750\nFalse\n\n\n170\n8.740\nNone\nRUTH WAS GLAD TO HEAR THAT PHILIP HAD MADE A P...\nRuth was\n0.931034\n0.931034\n0.931034\n0.068966\nFalse\n\n\n818\n9.870\nNone\nI'LL GLADLY DO THAT PROMISED THE NEW BOOLOOROO...\nI'll\n0.933333\n0.933333\n0.933333\n0.066667\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCosse was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-repro-warm1000.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:36&lt;00:00]\n    \n    \n\n\nWER: 7.23%\nWER (w/o hallucinations): 7.23%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n760\n6.370\nNone\nTHERE CAME UPON ME A SUDDEN SHOCK WHEN I HEARD...\nThere came upon me a sudden shock when I heard...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n368\n6.190\nNone\nEVEN SO I HAD JUST RETURNED FROM AN ARDUOUS JO...\nEven so, I had just returned from an arduous j...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n586\n5.515\nNone\nTHERE'S A HEAVY STORM COMING ON I CRIED POINTI...\nThere's a heavy storm coming on, I cried, poin...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n366\n3.615\nNone\nCHAPTER THREE AS MASTER WISHES\nChapter 3 As Master Wishes\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n365\n5.780\nNone\nI WILL SHOW YOU WHAT A GOOD JOB I DID AND SHE ...\nI will show you what a good job I did, and she...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n694\n5.965\nNone\nI EXPECT YOU HAVE BEEN A VERY GOOD GIRL ANDELL...\nI\n0.933333\n0.933333\n0.933333\n0.066667\nFalse\n\n\n881\n13.950\nNone\nWE BELIEVE IN A LITERAL RESURRECTION AND AN AC...\nWe believe that we are the most important ones.\n0.944444\n0.944444\n0.987654\n0.012346\nFalse\n\n\n106\n2.020\nNone\nSQUEAK SQUEAK\nQuick, quick!\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano's dead loss.\n2.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-repro-warm1000-2.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:35&lt;00:00]\n    \n    \n\n\nWER: 6.47%\nWER (w/o hallucinations): 6.47%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n702\n14.175\nNone\nAND THIS METHOD OF TREATING THE CASE WAS MUCH ...\nAnd this method of treating the case was much ...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n703\n4.775\nNone\nNATURE OF THE EFFECT PRODUCED BY EARLY IMPRESS...\nNature of the Effect produced by Early Impress...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n377\n3.910\nNone\nHE WENT HERE THERE AND EVERYWHERE IN PERFECT C...\nHe went here, there, and everywhere in perfect...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n376\n8.340\nNone\nNEVER DID HE OBJECT TO BUCKLING UP HIS SUITCAS...\nNever did he object to buckling up his suitcas...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossé was my man's servant.\n1.0\n0.666667\n0.833333\n0.166667\nFalse\n\n\n322\n3.200\nNone\nI NOW USE THEM AS ORNAMENTAL STATUARY IN MY GA...\nand\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n652\n3.475\nNone\nI AM SO VERY TIRED OF BEING ALL ALONE HERE\nand\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n555\n5.815\nNone\nBUT THE DUSK DEEPENING IN THE SCHOOLROOM COVER...\nand\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStaphano's dead loss.\n2.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:33&lt;00:00]\n    \n    \n\n\nWER: 5.93%\nWER (w/o hallucinations): 5.93%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n781\n3.050\nNone\nWHEN DO YOU INTEND THAT THE JOHN BRIGHT SHALL ...\nWhen do you intend that the John Bright shall ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n388\n2.355\nNone\nWE'RE LEAVING ON THE ABRAHAM LINCOLN\nWe're leaving on the Abraham Lincoln.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n387\n2.735\nNone\nA ROUTE SLIGHTLY LESS DIRECT THAT'S ALL\na route slightly less direct. That's all.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n386\n5.915\nNone\nYES WE ARE CERTAINLY I REPLIED EVASIVELY BUT A...\nYes, we are. Certainly, I replied evasively, b...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n385\n4.530\nNone\nANYHOW WE'LL LEAVE INSTRUCTIONS TO SHIP THE WH...\nAnyhow, we'll leave instructions to ship the w...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n524\n3.195\nNone\nBROTHER MAC ARDLE BROTHER KEOGH\nBrother McCarle, Brother Kioff.\n0.600000\n0.600000\n0.800000\n0.200000\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHans-Stirrsnacht.\n0.666667\n0.666667\n0.833333\n0.166667\nFalse\n\n\n766\n2.540\nNone\nYOU PROPOSE TO KIDNAP ME I SAID\nYou proposed a kenatmi set.\n0.857143\n0.857143\n0.971429\n0.028571\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nSteffano Staedalus\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\nax = _14.plot.scatter('secs', 'wer', alpha=.2)\nax.set_ylim(0, 1.5)\n\n\n\n\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-60k.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:30&lt;00:00]\n    \n    \n\n\nWER: 9.34%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n646\n3.385\nNone\nI ALMOST THINK I CAN REMEMBER FEELING A LITTLE...\nI almost think I can remember feeling a little...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n862\n6.720\nNone\nTO THE FERVENT LATTER DAY SAINT A TEMPLE IS NO...\nTo the fervent Latter-day Saint, a temple is n...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n370\n2.340\nNone\nBUT NOW NOTHING COULD HOLD ME BACK\nBut now nothing could hold me back.\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n369\n9.340\nNone\nI WANTED NOTHING MORE THAN TO SEE MY COUNTRY A...\nI wanted nothing more than to see my country a...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n368\n6.190\nNone\nEVEN SO I HAD JUST RETURNED FROM AN ARDUOUS JO...\nEven so, I had just returned from an arduous j...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n61\n10.250\nNone\nIN WINTER WHEN THE SNOW LAY GLITTERING ON THE ...\nIn winter, when the snow lay glittering on the...\n1.791667\n0.651515\n0.666035\n0.333965\n\n\n468\n12.250\nNone\nI HAVE GREAT THINGS TO TELL YOU SENOR SAID DON...\nI have great things to tell you, Senor, sadona...\n1.861111\n0.676768\n0.712682\n0.287318\n\n\n558\n15.720\nNone\nIT WAS STRANGE TOO THAT HE FOUND AN ARID PLEAS...\nIt was strange, too, that he found an arid ple...\n2.317073\n0.698529\n0.698529\n0.301471\n\n\n770\n13.960\nNone\nWHAT WORLD WIDE INIQUITY SUCH A SPEECH AS THAT...\nWhat worldwide iniquity such a speech as that ...\n2.375000\n0.719697\n0.738740\n0.261260\n\n\n444\n12.475\nNone\nTHEY DREW THEIR SWORDS HID THEIR FACES IN THE ...\nThey drew their swords, hid their faces in the...\n4.200000\n0.807692\n0.807692\n0.192308\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n\ntest_model(\"vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-eqvad.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:38&lt;00:00]\n    \n    \n\n\nWER: 7.47%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n673\n12.130\nNone\nTHE PRINCESS CERTAINLY WAS BEAUTIFUL AND HE WO...\nThe princess certainly was beautiful, and he w...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n674\n2.295\nNone\nHE ONLY SHOOK HIS HEAD\nHe only shook his head.\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n355\n2.885\nNone\nI'M AFRAID I DON'T KNOW MUCH ABOUT THE LAND OF OZ\nI'm afraid I don't know much about the land of...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n353\n5.870\nNone\nTHE FIRST LOT WE TESTED ON OUR GLASS CAT WHICH...\nThe first lot we tested on our glass cat, whic...\n0.000000\n0.000000\n0.000000\n1.000000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Stettelos.\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHonsters, nod.\n1.000000\n1.000000\n1.000000\n0.000000\n\n\n146\n3.260\nNone\nWHERE THEE AND THY FAMILY ARE KNOWN\nWhere's D and I-F where's D and I-F are known?\n1.428571\n0.714286\n0.836735\n0.163265\n\n\n996\n19.915\nNone\nEDISON HAD INSTALLED HIS HISTORIC FIRST GREAT ...\nEdison had installed his historic first-grade ...\n3.208333\n0.766169\n0.771041\n0.228959\n\n\n\n\n1000 rows × 8 columns\n\n\n\n\n\nax = _8.plot.scatter('secs', 'wer', alpha=.2)\nax.set_ylim(0, 1.5)\n\n\n\n\n\n\n\n\n\nax = _15['secs'].hist()\nax.set_yscale('log')\n\n\n\n\n\n\n\n\n\nplt.plot(_15['secs'], 1/_15['gt_text'].str.split('\\w+').str.len(), '.')\n\n\n\n\n\n\n\n\n\n# the reproducibility got pretty low ;)\nfor i in range(4):\n    print(i)\n    test_model(f\"test-run-{i}.model\")\n    print()\n\n0\nWER: 6.37%\nWER (w/o hallucinations): 6.37%\n\n1\nWER: 10.69%\nWER (w/o hallucinations): 9.89%\n\n2\nWER: 12.34%\nWER (w/o hallucinations): 11.79%\n\n3\nWER: 15.83%\nWER (w/o hallucinations): 15.30%\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:33&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:28&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:34&lt;00:00]\n    \n    \n\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:31&lt;00:00]\n    \n    \n\n\n\ntest_model(\"test-run-warm1000.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:31&lt;00:00]\n    \n    \n\n\nWER: 8.81%\nWER (w/o hallucinations): 8.81%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n368\n6.190\nNone\nEVEN SO I HAD JUST RETURNED FROM AN ARDUOUS JO...\nEven so, I had just returned from an arduous j...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n691\n4.985\nNone\nTO GIVE AN IDEA OF THESE CONVERSATIONS I WILL ...\nTo give an idea of these conversations, I will...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n366\n3.615\nNone\nCHAPTER THREE AS MASTER WISHES\nChapter 3 As Master Wishes\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n365\n5.780\nNone\nI WILL SHOW YOU WHAT A GOOD JOB I DID AND SHE ...\nI will show you what a good job I did, and she...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n792\n1.810\nNone\nVENICE\nVINIS.\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n324\n2.700\nNone\nASKED THE VOICE IN SCORNFUL ACCENTS\nAsk the voice in the voice in the voice in the...\n1.500000\n0.750000\n0.875000\n0.125000\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano's dead loss.\n2.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n26\n16.735\nNone\nP S PRAY SIR EXCUSE ME FOR WRITING TO YOU A SE...\nP-S-P-S-P-S-P-S-P-S-P-S-P-S-P-S-P-S-P-S-P-S-P-...\n2.037037\n0.982143\n0.999339\n0.000661\nFalse\n\n\n106\n2.020\nNone\nSQUEAK SQUEAK\nIn the past, we have a question.\n3.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"test-run-1e.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:34&lt;00:00]\n    \n    \n\n\nWER: 8.41%\nWER (w/o hallucinations): 8.05%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n655\n4.895\nNone\nI SHALL BE PUNISHED FOR IT NOW I SUPPOSE BY BE...\nI shall be punished for it now, I suppose, by ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n657\n3.640\nNone\nI AM VERY TIRED OF SWIMMING ABOUT HERE O MOUSE\nI am very tired of swimming about here, oh mouse.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n318\n5.115\nNone\nMOST PEOPLE TALK TOO MUCH SO IT IS A RELIEF TO...\nMost people talk too much, so it is a relief t...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n317\n7.920\nNone\nHE SELECTED A SMALL GOLD BOTTLE WITH A PEPPER ...\nHe selected a small gold bottle with a pepper ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n549\n10.575\nNone\nAT MOST BY AN ALMS GIVEN TO A BEGGAR WHOSE BLE...\nAt most, by an alms given to a beggar whose bl...\n1.000000\n0.500000\n0.500000\n0.500000\nTrue\n\n\n399\n6.365\nNone\nI WAS WELL SATISFIED WITH MY CABIN WHICH WAS L...\nI was well satisfied with my cabin, which was ...\n1.052632\n0.540541\n0.588905\n0.411095\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nSteffinor's Daedalus.\n1.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n659\n4.995\nNone\nWE WON'T TALK ABOUT HER ANY MORE IF YOU'D RATH...\nWe won't talk about her anymore if he'd rather...\n1.866667\n0.700000\n0.760000\n0.240000\nTrue\n\n\n95\n8.800\nNone\nTHOUGHT THE FIR TREE AND BELIEVED IT ALL BECAU...\nthought the fur tree, and believed it all, bec...\n4.619048\n0.829060\n0.837200\n0.162800\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\n# but it got better after some hyperparam tuning\ntest_model(\"vqmodel-4e-6454-hyptuned.model\")\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:30&lt;00:00]\n    \n    \n\n\nWER: 7.71%\nWER (w/o hallucinations): 7.71%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n403\n5.370\nNone\nDEPARTING FROM FIVE HUNDRED THOUSAND THROATS T...\nDeparting from 500,000 throats, three cheers b...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n922\n4.400\nNone\nBUT HOW DID SHE MANAGE TO RENDER IT SO FASHION...\nBut how did she manage to render it so fashion...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n629\n3.235\nNone\nTWO HOURS AFTERWARDS A TERRIBLE SHOCK AWOKE ME\nTwo hours afterwards, a terrible shock, awoke me.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n355\n2.885\nNone\nI'M AFRAID I DON'T KNOW MUCH ABOUT THE LAND OF OZ\nI'm afraid I don't know much about the land of...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n353\n5.870\nNone\nTHE FIRST LOT WE TESTED ON OUR GLASS CAT WHICH...\nThe first lot we tested on our glass cat, whic...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n849\n3.560\nNone\nI HAD A NOTION IT WAS YOU MATE AS SAVED ME FRO...\nI'll have a note.\n0.928571\n0.866667\n0.942857\n0.057143\nFalse\n\n\n741\n16.360\nNone\nOF WHAT MISSUS NEVERBEND HAD GONE THROUGH IN P...\nOf what Mrs. N N N N N N N N N N N N N N N N N...\n0.936170\n0.936170\n0.992021\n0.007979\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCasa was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Stetelos.\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHon Stur's Night.\n1.333333\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vqmodel-4e-6454-hyptuned-small.en.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:41&lt;00:00]\n    \n    \n\n\nWER: 7.38%\nWER (w/o hallucinations): 7.38%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.00000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n350\n10.680\nNone\nAT ONE END STOOD A GREAT FIREPLACE IN WHICH A ...\nAt one end stood a great fireplace, in which a...\n0.00000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n349\n2.130\nNone\nTHE WOMAN SEEMED THOUGHTFUL\nThe woman seemed thoughtful.\n0.00000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n680\n6.450\nNone\nHE DARTED LIKE AN ARROW THROUGH ALL THE HALLS ...\nHe darted like an arrow through all the halls,...\n0.00000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n347\n3.665\nNone\nOJO HAD NEVER EATEN SUCH A FINE MEAL IN ALL HI...\nOjo had never eaten such a fine meal in all hi...\n0.00000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHonsters, Nod.\n1.00000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n792\n1.810\nNone\nVENICE\nVINUS.\n1.00000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.00000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStephenos dead loss.\n1.50000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n440\n15.770\nNone\nELEVEN O'CLOCK HAD STRUCK IT WAS A FINE CLEAR ...\nAt the time of the day, the morning of the day...\n4.12766\n0.960396\n0.993259\n0.006741\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vqmodel-4e-hyptuned-16gpu.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:32&lt;00:00]\n    \n    \n\n\nWER: 6.01%\nWER (w/o hallucinations): 6.01%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n390\n1.975\nNone\nWE DON'T KNOW WHERE IT WILL TAKE US\nWe don't know where it will take us.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n708\n13.020\nNone\nTHE PAIN PRODUCED BY AN ACT OF HASTY AND ANGRY...\nThe pain produced by an act of hasty and angry...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n388\n2.355\nNone\nWE'RE LEAVING ON THE ABRAHAM LINCOLN\nWe're leaving on the Abraham Lincoln.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n387\n2.735\nNone\nA ROUTE SLIGHTLY LESS DIRECT THAT'S ALL\nA route slightly less direct, that's all.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n918\n3.000\nNone\nTHAT IS TRUE BADAUDERIE\nThat is true, bad old gree.\n0.750000\n0.500000\n0.625000\n0.375000\nFalse\n\n\n809\n8.875\nNone\nWHEN THE BLUESKINS SAW GHIP GHISIZZLE THEY RAI...\nThanks for watching!\n0.961538\n0.961538\n0.987179\n0.012821\nFalse\n\n\n643\n12.020\nNone\nALICE TOOK UP THE FAN AND GLOVES AND AS THE HA...\nThank you.\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCosse was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefanos de los\n1.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vqmodel-4e-hyptuned-32gpu.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:32&lt;00:00]\n    \n    \n\n\nWER: 5.94%\nWER (w/o hallucinations): 5.94%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n757\n10.030\nNone\nTHEREFORE I FEEL MYSELF QUITE ABLE AS PRESIDEN...\nTherefore, I feel myself quite able, as Presid...\n0.00\n0.000000\n0.000000\n1.000000\nFalse\n\n\n628\n2.550\nNone\nDURING HIS WATCH I SLEPT\nDuring his watch, I slept.\n0.00\n0.000000\n0.000000\n1.000000\nFalse\n\n\n756\n4.735\nNone\nYOU HAVE COME TO US THREATENING US WITH ABSOLU...\nYou have come to us threatening us with absolu...\n0.00\n0.000000\n0.000000\n1.000000\nFalse\n\n\n377\n3.910\nNone\nHE WENT HERE THERE AND EVERYWHERE IN PERFECT C...\nHe went here, there, and everywhere in perfect...\n0.00\n0.000000\n0.000000\n1.000000\nFalse\n\n\n376\n8.340\nNone\nNEVER DID HE OBJECT TO BUCKLING UP HIS SUITCAS...\nNever did he object to buckling up his suitcas...\n0.00\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n918\n3.000\nNone\nTHAT IS TRUE BADAUDERIE\nThat is true bad-delt gree.\n0.75\n0.500000\n0.625000\n0.375000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.00\n0.666667\n0.833333\n0.166667\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHonsters Nied.\n1.00\n1.000000\n1.000000\n0.000000\nFalse\n\n\n819\n5.775\nNone\nSCUSE ME SAID TROT I NEGLECTED TO TELL YOU THA...\nThanks for watching.\n1.00\n1.000000\n1.000000\n0.000000\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.50\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vqmodel-512c-4e-hyptuned-32gpu.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 08:52&lt;00:00]\n    \n    \n\n\nWER: 7.37%\nWER (w/o hallucinations): 7.37%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n715\n11.340\nNone\nTHE MOTHER IN MANAGING THE CASE IN THIS WAY RE...\nThe mother, in managing the case in this way, ...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n347\n3.665\nNone\nOJO HAD NEVER EATEN SUCH A FINE MEAL IN ALL HI...\nOjo had never eaten such a fine meal in all hi...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n860\n10.555\nNone\nIT IS NOTABLE THAT THE INDIAN TRIBES HAVE GENE...\nIt is notable that the Indian tribes have gene...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n608\n3.070\nNone\nTHE HORIZON SEEMS EXTREMELY DISTANT\nThe horizon seems extremely distant.\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n344\n4.275\nNone\nI AM MY DEAR AND ALL STRANGERS ARE WELCOME TO ...\nI am, my dear, and all strangers are welcome t...\n0.0\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCosay was my man's servant.\n1.0\n0.666667\n0.833333\n0.166667\nFalse\n\n\n260\n3.155\nNone\nWHO TAUGHT YOU TO SCRUB A FLOOR I SHOULD LIKE ...\n.\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHonster's Night.\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n792\n1.810\nNone\nVENICE\nVenus.\n1.0\n1.000000\n1.000000\n0.000000\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefanos de los.\n1.5\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vqmodel-512c-dim64-4e-hyptuned-32gpu.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:29&lt;00:00]\n    \n    \n\n\nWER: 7.13%\nWER (w/o hallucinations): 7.13%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n680\n6.450\nNone\nHE DARTED LIKE AN ARROW THROUGH ALL THE HALLS ...\nHe darted like an arrow through all the halls,...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n682\n5.145\nNone\nAND ALL HIS BROTHERS AND SISTERS STOOD ROUND A...\nand all his brothers and sisters stood round a...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n684\n2.165\nNone\nANDERS FACE GREW RED\nAnders face grew red.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n685\n2.775\nNone\nBUT HIS MOTHER HUGGED HIM CLOSE\nBut his mother hugged him close.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n106\n2.020\nNone\nSQUEAK SQUEAK\nSpeak, speak.\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n371\n2.440\nNone\nCONSEIL WAS MY MANSERVANT\nCossay was my man's servant.\n1.000000\n0.666667\n0.833333\n0.166667\nFalse\n\n\n592\n1.805\nNone\nHANS STIRS NOT\nHonsters, Nied.\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n336\n4.835\nNone\nFOR A LONG TIME HE HAD WISHED TO EXPLORE THE B...\nFor a long time, you can see that the video is...\n1.333333\n0.800000\n0.933333\n0.066667\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Staedt-Loss\n1.500000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vqmodel-base-en+pl-512c-dim64.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 01:34&lt;00:00]\n    \n    \n\n\nWER: 8.45%\nWER (w/o hallucinations): 8.45%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, while on her lap...\n0.0\n0.0\n0.0\n1.0\nFalse\n\n\n740\n2.715\nNone\nBUT I MEAN TO HAVE MY INNINGS BEFORE LONG\nBut I mean to have my innings before long.\n0.0\n0.0\n0.0\n1.0\nFalse\n\n\n387\n2.735\nNone\nA ROUTE SLIGHTLY LESS DIRECT THAT'S ALL\nA route slightly less direct, that's all.\n0.0\n0.0\n0.0\n1.0\nFalse\n\n\n386\n5.915\nNone\nYES WE ARE CERTAINLY I REPLIED EVASIVELY BUT A...\nYes, we are, certainly, I replied evasively, b...\n0.0\n0.0\n0.0\n1.0\nFalse\n\n\n744\n3.630\nNone\nWHAT COULD I DO NOW BUT JUST LAY MYSELF DOWN A...\nWhat could I do now, but just lay myself down ...\n0.0\n0.0\n0.0\n1.0\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n114\n6.560\nNone\nYES IN REALITY THOSE WERE HAPPY TIMES\nThank you.\n1.0\n1.0\n1.0\n0.0\nFalse\n\n\n191\n5.580\nNone\nWHY IT'S IN MISSOURI SOMEWHERE ON THE FRONTIER...\nThank you.\n1.0\n1.0\n1.0\n0.0\nFalse\n\n\n538\n2.215\nNone\nSTEPHANOS DEDALOS\nStefano Stedilos\n1.0\n1.0\n1.0\n0.0\nFalse\n\n\n16\n1.695\nNone\nFAREWELL MADAM\nFair Well, Madame.\n1.5\n1.0\n1.0\n0.0\nFalse\n\n\n106\n2.020\nNone\nSQUEAK SQUEAK\nS'quik, s'quik !\n2.0\n1.0\n1.0\n0.0\nFalse\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ntest_model(\"vqmodel-medium-en+pl-512c-dim64.model\", N=1000)\n\n\n\n\n\n\n    \n      \n      100.00% [1000/1000 06:09&lt;00:00]\n    \n    \n\n\nWER: 7.34%\nWER (w/o hallucinations): 6.62%\n\n\n\n\n\n\n\n\n\n\nsecs\nidx\ngt_text\ntext\nwer\nmer\nwil\nwip\nhallucination\n\n\n\n\n0\n8.230\nNone\nAND OFTEN HAS MY MOTHER SAID WHILE ON HER LAP ...\nAnd often has my mother said, While on her lap...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n571\n6.615\nNone\nSTEPHEN'S HEART BEGAN SLOWLY TO FOLD AND FADE ...\nStephen's heart began slowly to fold and fade ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n572\n4.090\nNone\nHE IS CALLED AS YOU KNOW THE APOSTLE OF THE IN...\nHe is called, as you know, the Apostle of the ...\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n573\n3.330\nNone\nA GREAT SAINT SAINT FRANCIS XAVIER\nA great saint, St. Francis Xavier.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n575\n3.445\nNone\nHE HAD THE FAITH IN HIM THAT MOVES MOUNTAINS\nHe had the faith in him that moves mountains.\n0.000000\n0.000000\n0.000000\n1.000000\nFalse\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n76\n4.110\nNone\nREJOICE IN THY OWN FRESH YOUTH\nRead more at www.BritishMedia.com\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n63\n13.950\nNone\nTO GROW AND GROW TO GET OLDER AND BE TALL THOU...\n. . . . . . . . . . . . . . . . . . . . . . . ...\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n819\n5.775\nNone\nSCUSE ME SAID TROT I NEGLECTED TO TELL YOU THA...\nThanks for watching!\n1.000000\n1.000000\n1.000000\n0.000000\nFalse\n\n\n95\n8.800\nNone\nTHOUGHT THE FIR TREE AND BELIEVED IT ALL BECAU...\nthought the fur tree, and believed it all, bec...\n2.047619\n0.682540\n0.697657\n0.302343\nFalse\n\n\n654\n6.200\nNone\nI WISH I HADN'T CRIED SO MUCH SAID ALICE AS SH...\n\"'I wish I hadn't cried so much,' said Alice, ...\n6.900000\n0.873418\n0.873418\n0.126582\nTrue\n\n\n\n\n1000 rows × 9 columns\n\n\n\n\n\ndef show_stat(stats, i):\n    row = stats.loc[i]\n    print('WER: ', row['wer'])\n    print('GT:  ', row['gt_text'])\n    print('GEN: ', row['text'])\n\n\nshow_stat(_18, 654)\n\nWER:  6.9\nGT:   I WISH I HADN'T CRIED SO MUCH SAID ALICE AS SHE SWAM ABOUT TRYING TO FIND HER WAY OUT\nGEN:  \"'I wish I hadn't cried so much,' said Alice, as she swam about, trying to find her way out. \"'I wish I hadn't cried so much,' said Alice, as she swam about, trying to find her way out. \"'I wish I hadn't cried so much,' said Alice, as she swam about, trying to find her way out. \"'I wish I hadn't cried so much,' said Alice, as she swam about, trying to find her way out. \"'I wish I hadn't cried so much,' said Alice, as she swam about, trying to find her way out. \"'I wish I hadn't cried so much,' said Alice, as she swam about, trying to find her way out. \"'I wish I hadn't cried so much,' said Alice, as she swam about, trying to find her way out. \"'I wish I hadn't cried so much,' said Alice, as she swam about, trying to find her\n\n\n\nshow_stat(_18, 819)\n\nWER:  1.0\nGT:   SCUSE ME SAID TROT I NEGLECTED TO TELL YOU THAT YOU'RE NOT THE BOOLOOROO ANY MORE\nGEN:  Thanks for watching!\n\n\n\nshow_stat(_13, 114)\n\nWER:  1.0\nGT:   YES IN REALITY THOSE WERE HAPPY TIMES\nGEN:  Thank you."
  },
  {
    "objectID": "C2. Testing.html",
    "href": "C2. Testing.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\ntest_model\n\n test_model (model, ds, bs=1)"
  },
  {
    "objectID": "b. languages.html",
    "href": "b. languages.html",
    "title": "Language codes",
    "section": "",
    "text": "This language list is comming straigh from openai-whisper. The upstream file is here: https://github.com/openai/whisper/blob/main/whisper/tokenizer.py but we are freezing this to the openai-whisper==20230918 version right now.\n\nsource\n\nto_id\n\n to_id (lang)"
  },
  {
    "objectID": "3a. t2s transcripts preparation.html",
    "href": "3a. t2s transcripts preparation.html",
    "title": "T2S dataset preparation",
    "section": "",
    "text": "prepare_txt('../wolnelektury-wds2/wolnelektury-audio-000000.tar', n_samples=1024, batch_size=16)\n\nLightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/torch/whisperx-vad-segmentation.bin`\n\n\nModel was trained with pyannote.audio 0.0.1, yours is 2.1.1. Bad things might happen unless you revert pyannote.audio to 0.x.\nModel was trained with torch 1.10.0+cu102, yours is 2.1.0+cu121. Bad things might happen unless you revert torch to 1.x.\nBenchmarking run of 1024 samples (64 batches)\n\n\n\n\n\n\n\n    \n      \n      100.00% [64/64 00:40&lt;00:00]\n    \n    \n\n\n\nprepare_txt('../wolnelektury-wds2/wolnelektury-audio-000000.tar', n_samples=1024, batch_size=16)\n\nBenchmarking run of 1024 samples (64 batches)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\n\n\n\n\n    \n      \n      100.00% [64/64 01:33&lt;00:00]\n    \n    \n\n\n\nprepare_txt('../wolnelektury-wds2/wolnelektury-audio-000000.tar', transcription_model='medium', n_samples=1024, batch_size=16)\n\nBenchmarking run of 1024 samples (64 batches)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\n\n\n\n\n    \n      \n      100.00% [64/64 02:06&lt;00:00]\n    \n    \n\n\n\nprepare_txt('../wolnelektury-wds2/wolnelektury-audio-000000.tar', transcription_model='medium', n_samples=1024, batch_size=1)\n\nBenchmarking run of 1024 samples (1024 batches)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n    - Avoid using `tokenizers` before the fork if possible\n    - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\n\n\n\n\n\n\n    \n      \n      100.00% [1024/1024 10:01&lt;00:00]"
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html",
    "href": "2b. whisper quantization (semantic token) model.html",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "",
    "text": "from whisperspeech import wh_transcribe\nimport IPython\n\n/opt/hostedtoolcache/Python/3.9.18/x64/lib/python3.9/site-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n  torchaudio.set_audio_backend(\"soundfile\")\ntorchvision is not available - cannot save figures"
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#prepare-the-dataset",
    "href": "2b. whisper quantization (semantic token) model.html#prepare-the-dataset",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "Prepare the dataset",
    "text": "Prepare the dataset\n\nshards = [str(x) for x in Path('/data/whisperspeech-wds/').glob('librilight-*.tar')]\n\n\nds = wds.WebDataset(shards, shardshuffle=True)\n\n\nds2 = ds.compose(\n    wds.decode(wds.torch_audio),\n    utils.find_audio,\n    merge_in(derived_dataset('/data/whisperspeech-processed-wds/', 'vad')),\n    wds.map_dict(**{\"vad.npy\":wh_transcribe.chunk_merger}),\n    wh_transcribe.split_to_chunks,\n    merge_in(derived_dataset('/data/whisperspeech-processed-wds/', 'base.en-txt')),\n    wds.shuffle(),\n    wds.select(lambda x: x['i'] != 0 and x['i'] != x['imax']),\n)\n\n\nvad_shards = [str(x) for x in Path('/data/whisperspeech-processed-wds/').glob('*-large-6454-vad-*.tar.gz')]\n\n\nds = wds.WebDataset(vad_shards).decode().map_dict(**{'vad.npy':wh_transcribe.chunk_merger})\n\n\nchunks = [len(x['vad.npy'][1:-1]) for x in progress_bar(ds, total='noinfer')]\n\n\n\n\n\n\n    \n      \n      100.00% [3411/3411 00:01&lt;00:00]\n    \n    \n\n\n\nsum(chunks)\n\n203078\n\n\n\nfor x in progress_bar(ds2, total=5):\n    IPython.display.display(IPython.display.Markdown(f\"## {x['__key__']} from {x['__url__']}\\n{x['txt']}\"))\n    IPython.display.display(IPython.display.Audio(x['samples'], rate=16000))\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 00:01&lt;00:00]\n    \n    \n\n\nlarge/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_006 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nPhysically I was incapable of complying with the command, and mentally I had not the slightest intention of departing. In an outhouse devoted to storing melees, sheepskins, and harness, an old man was sitting on the doorstep, compounding a mixture which I recognized as a sheep remedy.\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nlarge/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_009 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nThe following day I was the most surprised man in South Africa when I learned that my preparation was working a marvelous cure. I was invited to remain with the bore the balance of the season as an honoured guest. Day after day I tramped the hills, returning at night as wise and as rich as when I set out. There were unmistakable indications that gold should be found in the vicinity, but the stubborn fact remained that I could not find it.\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nlarge/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_001 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nI was one of the first prospectors in the Transvaal to search for gold and a precious dance it led me. At that time, but few Englishmen had ventured into the Boer country, and such was the jealousy with which they were regarded that it was impossible to secure any information which would assist in the search. Footsoir and weary, I tramped from farm to farm, content\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nlarge/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_032 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nDead, more than twenty years. In fact, before I was married and came to live here, for he was my husband’s father. Did you know him? Yes, but I was only a little girl at the time. Why have the clothes been kept?\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nlarge/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_004 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nFortunately, I had acquired some knowledge of sheep in Australia else I believe that I should have starved. When all else failed, I became a sheep doctor and then did a compound whose virtues would have done credit to the most widely advertised path and medicine nostrum.\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\n\nds3 = ds2.compose(\n    add_masks,\n    tokenize_text,\n    wds.to_tuple('samples', 'mask', 'in_ttoks', 'out_ttoks')\n)\n\n\nfor x in ds3: break\nx\n\n(tensor([0.0043, 0.0102, 0.0163,  ..., 0.0000, 0.0000, 0.0000]),\n tensor([ True,  True,  True,  ..., False, False, False]),\n tensor([50257,  3152,   257, 44823,  3154,  1589,    11,   484,   673,  1144,\n           572,   503,   286,  2837,   290,   706,  2063,   281,  1711,   338,\n          1057,    11,   262, 39535, 21067,   373,   625,   262,  2318,   290,\n           287,  5897, 10150,    13,  1119,  2582, 40424,   510,   262, 27913,\n          4608,   284, 47251,   290,  1043,   257,  1588,  1426,   325,   286,\n          4684, 13384,  3492,   284, 17655,   511, 15892,    13, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n         50256]),\n tensor([ 3152,   257, 44823,  3154,  1589,    11,   484,   673,  1144,   572,\n           503,   286,  2837,   290,   706,  2063,   281,  1711,   338,  1057,\n            11,   262, 39535, 21067,   373,   625,   262,  2318,   290,   287,\n          5897, 10150,    13,  1119,  2582, 40424,   510,   262, 27913,  4608,\n           284, 47251,   290,  1043,   257,  1588,  1426,   325,   286,  4684,\n         13384,  3492,   284, 17655,   511, 15892,    13, 50256,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100]))\n\n\n\nds3 = ds2.compose(\n    add_masks,\n    lambda x: tokenize_text(x, model='medium', language='en'),\n    wds.to_tuple('samples', 'mask', 'in_ttoks', 'out_ttoks')\n)\n\n\nfor x in ds3: break\nx\n\n(tensor([0.0013, 0.0010, 0.0011,  ..., 0.0000, 0.0000, 0.0000]),\n tensor([ True,  True,  True,  ..., False, False, False]),\n tensor([50258, 50259, 50359,    32,  1326,  1270,  3931,   382,   613,    11,\n         11672,   293, 37632, 13809,    11,   576,  1319,   264,  1851,   295,\n           264,  1002,    11,   293,  1939,   576,   572,   544,  1643,   281,\n         18071,   264,  1164,   295,  3687,    11,   420,  1497,   554,  1952,\n          6018,    11,   813,   264,  1974,  5010,   295,   721,    11,   689,\n           264,  7700,   366,  4054,   293,  7006,   293, 14154,   292,    13,\n          2188,  1359, 17431,  2212,   281,  3511,   328,  3780,   311,  3567,\n           294,   702,  1536,  6717,  1062,   362, 16424,   796,   666,   257,\n          5403, 14763,    13, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n         50257, 50257, 50257]),\n tensor([50259, 50359,    32,  1326,  1270,  3931,   382,   613,    11, 11672,\n           293, 37632, 13809,    11,   576,  1319,   264,  1851,   295,   264,\n          1002,    11,   293,  1939,   576,   572,   544,  1643,   281, 18071,\n           264,  1164,   295,  3687,    11,   420,  1497,   554,  1952,  6018,\n            11,   813,   264,  1974,  5010,   295,   721,    11,   689,   264,\n          7700,   366,  4054,   293,  7006,   293, 14154,   292,    13,  2188,\n          1359, 17431,  2212,   281,  3511,   328,  3780,   311,  3567,   294,\n           702,  1536,  6717,  1062,   362, 16424,   796,   666,   257,  5403,\n         14763,    13, 50257,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n          -100,  -100,  -100]))\n\n\n\ntrain_ds = load_dataset('librilight-wds/librilight-small-flac-000000-s0*.tar', 'librilight-preproc-wds/', samples=2500 * 32)\n\n\nval_ds = load_dataset('librilight-wds/librilight-small-flac-000000-s11.tar', 'librilight-preproc-wds/', samples=500)\n\n\nfor x in progress_bar(wds.WebLoader(train_ds, num_workers=16, batch_size=None), total='noinfer'): pass\n\n\n\n\n\n\n    \n      \n      [245/? 00:09&lt;?]\n    \n    \n\n\n╭─────────────────────────────── Traceback (most recent call last) ────────────────────────────────╮\n│ in &lt;module&gt;:1                                                                                    │\n│                                                                                                  │\n│ ❱ 1 for x in progress_bar(wds.WebLoader(train_ds, num_workers=16, batch_size=None), total='n     │\n│   2                                                                                              │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/fastprogress/fastprogress.py:41 in __iter__              │\n│                                                                                                  │\n│    38 │   def __iter__(self):                                                                    │\n│    39 │   │   if self.total != 0: self.update(0)                                                 │\n│    40 │   │   try:                                                                               │\n│ ❱  41 │   │   │   for i,o in enumerate(self.gen):                                                │\n│    42 │   │   │   │   if self.total and i &gt;= self.total: break                                   │\n│    43 │   │   │   │   yield o                                                                    │\n│    44 │   │   │   │   self.update(i+1)                                                           │\n│                                                                                                  │\n│ /root/workspace/webdataset/webdataset/pipeline.py:64 in iterator                                 │\n│                                                                                                  │\n│    61 │   def iterator(self):                                                                    │\n│    62 │   │   \"\"\"Create an iterator through the entire dataset, using the given number of repe   │\n│    63 │   │   for i in range(self.repetitions):                                                  │\n│ ❱  64 │   │   │   for sample in self.iterator1():                                                │\n│    65 │   │   │   │   yield sample                                                               │\n│    66 │                                                                                          │\n│    67 │   def __iter__(self):                                                                    │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:633 in __next__           │\n│                                                                                                  │\n│    630 │   │   │   if self._sampler_iter is None:                                                │\n│    631 │   │   │   │   # TODO(https://github.com/pytorch/pytorch/issues/76750)                   │\n│    632 │   │   │   │   self._reset()  # type: ignore[call-arg]                                   │\n│ ❱  633 │   │   │   data = self._next_data()                                                      │\n│    634 │   │   │   self._num_yielded += 1                                                        │\n│    635 │   │   │   if self._dataset_kind == _DatasetKind.Iterable and \\                          │\n│    636 │   │   │   │   │   self._IterableDataset_len_called is not None and \\                    │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1328 in _next_data        │\n│                                                                                                  │\n│   1325 │   │   │   │   return self._process_data(data)                                           │\n│   1326 │   │   │                                                                                 │\n│   1327 │   │   │   assert not self._shutdown and self._tasks_outstanding &gt; 0                     │\n│ ❱ 1328 │   │   │   idx, data = self._get_data()                                                  │\n│   1329 │   │   │   self._tasks_outstanding -= 1                                                  │\n│   1330 │   │   │   if self._dataset_kind == _DatasetKind.Iterable:                               │\n│   1331 │   │   │   │   # Check for _IterableDatasetStopIteration                                 │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1294 in _get_data         │\n│                                                                                                  │\n│   1291 │   │   │   # need to call `.task_done()` because we don't use `.join()`.                 │\n│   1292 │   │   else:                                                                             │\n│   1293 │   │   │   while True:                                                                   │\n│ ❱ 1294 │   │   │   │   success, data = self._try_get_data()                                      │\n│   1295 │   │   │   │   if success:                                                               │\n│   1296 │   │   │   │   │   return data                                                           │\n│   1297                                                                                           │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1132 in _try_get_data     │\n│                                                                                                  │\n│   1129 │   │   # Returns a 2-tuple:                                                              │\n│   1130 │   │   #   (bool: whether successfully get data, any: data if successful else None)      │\n│   1131 │   │   try:                                                                              │\n│ ❱ 1132 │   │   │   data = self._data_queue.get(timeout=timeout)                                  │\n│   1133 │   │   │   return (True, data)                                                           │\n│   1134 │   │   except Exception as e:                                                            │\n│   1135 │   │   │   # At timeout and error, we manually check whether any worker has              │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/multiprocessing/queues.py:113 in get                                   │\n│                                                                                                  │\n│   110 │   │   │   try:                                                                           │\n│   111 │   │   │   │   if block:                                                                  │\n│   112 │   │   │   │   │   timeout = deadline - time.monotonic()                                  │\n│ ❱ 113 │   │   │   │   │   if not self._poll(timeout):                                            │\n│   114 │   │   │   │   │   │   raise Empty                                                        │\n│   115 │   │   │   │   elif not self._poll():                                                     │\n│   116 │   │   │   │   │   raise Empty                                                            │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/multiprocessing/connection.py:257 in poll                              │\n│                                                                                                  │\n│   254 │   │   \"\"\"Whether there is any input available to be read\"\"\"                              │\n│   255 │   │   self._check_closed()                                                               │\n│   256 │   │   self._check_readable()                                                             │\n│ ❱ 257 │   │   return self._poll(timeout)                                                         │\n│   258 │                                                                                          │\n│   259 │   def __enter__(self):                                                                   │\n│   260 │   │   return self                                                                        │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/multiprocessing/connection.py:424 in _poll                             │\n│                                                                                                  │\n│   421 │   │   return self._recv(size)                                                            │\n│   422 │                                                                                          │\n│   423 │   def _poll(self, timeout):                                                              │\n│ ❱ 424 │   │   r = wait([self], timeout)                                                          │\n│   425 │   │   return bool(r)                                                                     │\n│   426                                                                                            │\n│   427                                                                                            │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/multiprocessing/connection.py:931 in wait                              │\n│                                                                                                  │\n│   928 │   │   │   │   deadline = time.monotonic() + timeout                                      │\n│   929 │   │   │                                                                                  │\n│   930 │   │   │   while True:                                                                    │\n│ ❱ 931 │   │   │   │   ready = selector.select(timeout)                                           │\n│   932 │   │   │   │   if ready:                                                                  │\n│   933 │   │   │   │   │   return [key.fileobj for (key, events) in ready]                        │\n│   934 │   │   │   │   else:                                                                      │\n│                                                                                                  │\n│ /opt/conda/lib/python3.10/selectors.py:416 in select                                             │\n│                                                                                                  │\n│   413 │   │   │   timeout = math.ceil(timeout * 1e3)                                             │\n│   414 │   │   ready = []                                                                         │\n│   415 │   │   try:                                                                               │\n│ ❱ 416 │   │   │   fd_event_list = self._selector.poll(timeout)                                   │\n│   417 │   │   except InterruptedError:                                                           │\n│   418 │   │   │   return ready                                                                   │\n│   419 │   │   for fd, event in fd_event_list:                                                    │\n╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\nKeyboardInterrupt\n\n\n\n\nfor x in train_ds:\n    print(x[3])\n    break\n\ntensor([[  464,  7664,   286,  ...,  -100,  -100,  -100],\n        [ 2953,   717,   612,  ...,  -100,  -100,  -100],\n        [25383,   339,   587,  ...,  -100,  -100,  -100],\n        ...,\n        [  392,   340,   880,  ...,  -100,  -100,  -100],\n        [  464, 31526, 11416,  ...,  -100,  -100,  -100],\n        [ 2202,   262, 16720,  ...,  -100,  -100,  -100]])"
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_006-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "href": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_006-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_006 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar",
    "text": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_006 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nPhysically I was incapable of complying with the command, and mentally I had not the slightest intention of departing. In an outhouse devoted to storing melees, sheepskins, and harness, an old man was sitting on the doorstep, compounding a mixture which I recognized as a sheep remedy."
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_009-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "href": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_009-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_009 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar",
    "text": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_009 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nThe following day I was the most surprised man in South Africa when I learned that my preparation was working a marvelous cure. I was invited to remain with the bore the balance of the season as an honoured guest. Day after day I tramped the hills, returning at night as wise and as rich as when I set out. There were unmistakable indications that gold should be found in the vicinity, but the stubborn fact remained that I could not find it."
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_001-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "href": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_001-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_001 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar",
    "text": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_001 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nI was one of the first prospectors in the Transvaal to search for gold and a precious dance it led me. At that time, but few Englishmen had ventured into the Boer country, and such was the jealousy with which they were regarded that it was impossible to secure any information which would assist in the search. Footsoir and weary, I tramped from farm to farm, content"
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_032-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "href": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_032-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_032 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar",
    "text": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_032 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nDead, more than twenty years. In fact, before I was married and came to live here, for he was my husband’s father. Did you know him? Yes, but I was only a little girl at the time. Why have the clothes been kept?"
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_004-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "href": "2b. whisper quantization (semantic token) model.html#large6454kaffirkangarooklondiketales_1611_librivox_64kb_mp3kaffirkangaroo_03_leavitt_64kb_004-from-datawhisperspeech-wdslibrilight-large-6454-flac-000007.tar",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_004 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar",
    "text": "large/6454/kaffirkangarooklondiketales_1611_librivox_64kb_mp3/kaffirkangaroo_03_leavitt_64kb_004 from /data/whisperspeech-wds/librilight-large-6454-flac-000007.tar\nFortunately, I had acquired some knowledge of sheep in Australia else I believe that I should have starved. When all else failed, I became a sheep doctor and then did a compound whose virtues would have done credit to the most widely advertised path and medicine nostrum."
  },
  {
    "objectID": "2b. whisper quantization (semantic token) model.html#architectural-experiments",
    "href": "2b. whisper quantization (semantic token) model.html#architectural-experiments",
    "title": "Distill Whisper with a VQ bottleneck",
    "section": "Architectural experiments",
    "text": "Architectural experiments\n\n# with learned positional embeddings, no out_blocks\nvqmodel = RQBottleneckTransformer(codebook_dim=16, vq_codes=512, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=1, lr=3e-3, warmup_steps=1000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6290 1\n\n\n\n\n\n'Entropy: 8.71'\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n107.56952\n157.32113\n8.71\n05:24\n\n\n100000\n85.44750\n101.79171\n8.70\n10:37\n\n\n126688\n81.44776\n104.25017\n8.71\n13:27\n\n\n\n\n\n    \n      \n      62.94% [3959/6290 13:26&lt;07:54 #126688/201280 loss: 81.448 / 104.250]\n    \n    \n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 11 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 12 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n\n\n\n\n# with learned positional embeddings, out_blocks before positional\nvqmodel = RQBottleneckTransformer(codebook_dim=16, vq_codes=512, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=1, lr=3e-3, warmup_steps=1000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6290 1\n\n\n\n\n\n'Entropy: 8.70'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 22:57&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n23.45991\n42.24113\n8.80\n05:48\n\n\n100000\n16.19686\n23.67809\n8.78\n11:27\n\n\n150016\n11.99028\n17.22306\n8.74\n17:07\n\n\n200000\n11.68037\n16.67605\n8.70\n22:46\n\n\n201280\n11.92631\n16.65236\n8.70\n22:57\n\n\n\n\n\n    \n      \n      100.00% [6290/6290 22:57&lt;00:00 #201280/201280 loss: 11.926 / 16.652]\n    \n    \n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6290 batches x 32 samples, 1307.9 hours) was reported to be 6290 (when accessing len(dataloader)), but 6291 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n\n\n\n\n# with learned positional embeddings, out_blocks before positional, mlp before vq\nvqmodel = RQBottleneckTransformer(codebook_dim=16, vq_codes=512, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=1, lr=3e-3, warmup_steps=1000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6290 1\n\n\n\n\n\n'Entropy: 8.57'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 23:09&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n24.63220\n44.67238\n8.74\n05:53\n\n\n100000\n14.69983\n19.67298\n8.67\n11:35\n\n\n150016\n11.50774\n17.75203\n8.58\n17:16\n\n\n200000\n11.33895\n15.66892\n8.55\n22:58\n\n\n201280\n10.87422\n15.81362\n8.57\n23:09\n\n\n\n\n\n    \n      \n      100.00% [6290/6290 23:08&lt;00:00 #201280/201280 loss: 10.874 / 15.814]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n# with learned positional embeddings, out_blocks after positional, mlp before vq\nvqmodel = RQBottleneckTransformer(codebook_dim=16, vq_codes=512, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=1, lr=3e-3, warmup_steps=1000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6290 1\n\n\n\n\n\n'Entropy: 8.54'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 23:11&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.37899\n27.54997\n8.65\n05:53\n\n\n100000\n13.13329\n17.32240\n8.60\n11:35\n\n\n150016\n10.83435\n13.55371\n8.56\n17:18\n\n\n200000\n9.69492\n12.35855\n8.51\n23:00\n\n\n201280\n10.54271\n12.43994\n8.54\n23:11\n\n\n\n\n\n    \n      \n      100.00% [6290/6290 23:11&lt;00:00 #201280/201280 loss: 10.543 / 12.440]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n# with learned positional embeddings, out_blocks after positional, mlp before vq\nvqmodel = RQBottleneckTransformer(codebook_dim=16, vq_codes=512, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-2d-512c-cosine-padfix-premlp-learnpos-5e.model')\n\nOneCycle: 6290 5\n\n\n\n\n\n'Entropy: 8.40'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:55:58&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n24.24790\n47.61960\n8.62\n05:53\n\n\n100000\n14.35983\n18.50102\n8.55\n11:35\n\n\n150016\n12.35634\n16.84217\n8.56\n17:18\n\n\n200000\n11.74603\n16.10603\n8.52\n23:00\n\n\n250016\n10.85323\n14.83014\n8.49\n28:56\n\n\n300000\n10.78046\n14.04290\n8.47\n34:38\n\n\n350016\n10.05354\n12.98133\n8.40\n40:21\n\n\n400000\n9.59631\n13.78049\n8.50\n46:03\n\n\n450016\n9.22316\n12.76403\n8.40\n51:57\n\n\n500000\n9.38958\n11.96084\n8.46\n57:40\n\n\n550016\n8.36034\n12.59843\n8.35\n1:03:22\n\n\n600000\n9.39242\n11.55411\n8.43\n1:09:05\n\n\n650016\n8.30749\n10.80241\n8.42\n1:15:02\n\n\n700000\n8.20436\n10.39852\n8.48\n1:20:45\n\n\n750016\n8.21392\n10.36367\n8.41\n1:26:27\n\n\n800000\n7.73189\n11.21438\n8.48\n1:32:10\n\n\n850016\n7.64852\n10.93893\n8.47\n1:38:06\n\n\n900000\n7.72010\n10.49391\n8.39\n1:43:48\n\n\n950016\n7.58901\n9.85925\n8.42\n1:49:31\n\n\n1000000\n7.14871\n10.67987\n8.40\n1:55:14\n\n\n1006400\n6.73056\n10.67323\n8.40\n1:55:58\n\n\n\n\n\n    \n      \n      100.00% [6290/6290 23:12&lt;00:00 #201280/201280 loss: 6.731 / 10.673]\n    \n    \n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 11 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 12 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6290 batches x 32 samples, 1307.9 hours) was reported to be 6290 (when accessing len(dataloader)), but 6291 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n\n\n\n\n# with learned positional embeddings, out_blocks after positional, mlp before vq\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=6, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True).cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=32, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model')\n\nOneCycle: 6290 5\n\n\n\n\n\n'Entropy: 11.07'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 1:57:58&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n15.49718\n26.42581\n11.23\n06:00\n\n\n100000\n11.36006\n14.78076\n11.25\n11:48\n\n\n150016\n10.29752\n13.68974\n11.19\n17:36\n\n\n200000\n9.22019\n12.14817\n11.26\n23:24\n\n\n250016\n9.09067\n13.16928\n11.17\n29:26\n\n\n300000\n8.56113\n12.38342\n11.13\n35:14\n\n\n350016\n8.30965\n12.02589\n11.15\n41:02\n\n\n400000\n7.76135\n10.97900\n11.14\n46:50\n\n\n450016\n7.34585\n10.10667\n11.11\n52:53\n\n\n500000\n7.65255\n11.02440\n11.10\n58:41\n\n\n550016\n7.47726\n10.73619\n11.10\n1:04:29\n\n\n600000\n6.96974\n9.63206\n11.14\n1:10:17\n\n\n650016\n6.93395\n9.97940\n11.08\n1:16:19\n\n\n700000\n6.64507\n8.91945\n11.13\n1:22:07\n\n\n750016\n6.53036\n9.27800\n11.01\n1:27:55\n\n\n800000\n6.50427\n8.30845\n11.07\n1:33:44\n\n\n850016\n6.51113\n9.09502\n11.12\n1:39:48\n\n\n900000\n6.05660\n8.44461\n10.99\n1:45:36\n\n\n950016\n6.20974\n8.88156\n11.06\n1:51:25\n\n\n1000000\n5.95045\n8.69922\n11.08\n1:57:13\n\n\n1006400\n6.18939\n8.88604\n11.07\n1:57:58\n\n\n\n\n\n    \n      \n      100.00% [6290/6290 23:37&lt;00:00 #201280/201280 loss: 6.189 / 8.886]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n# base.en Whisper with learned positional embeddings, out_blocks after positional, mlp before vq\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e.model')\n\nOneCycle: 6280 5\n\n\n\n\n\n'Entropy: 10.86'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:05:51&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.17899\n27.83681\n11.11\n09:23\n\n\n100000\n13.50658\n17.32206\n11.06\n18:34\n\n\n150016\n12.10491\n15.49411\n11.08\n27:47\n\n\n200000\n11.84169\n15.30570\n10.95\n36:58\n\n\n250016\n11.19514\n14.05272\n10.99\n46:23\n\n\n300000\n10.98578\n13.69234\n10.86\n55:34\n\n\n350016\n10.58517\n13.25610\n10.99\n1:04:46\n\n\n400000\n9.87159\n12.88844\n10.91\n1:13:57\n\n\n450016\n9.76353\n12.50161\n10.92\n1:23:22\n\n\n500000\n10.08099\n12.71940\n10.94\n1:32:33\n\n\n550016\n9.85388\n12.70232\n10.89\n1:41:45\n\n\n600000\n10.50843\n11.94505\n10.93\n1:50:57\n\n\n650016\n9.29321\n12.16166\n10.96\n2:00:20\n\n\n700000\n9.24717\n11.35387\n10.93\n2:09:32\n\n\n750016\n8.80798\n11.78821\n10.95\n2:18:43\n\n\n800000\n9.14499\n10.97496\n10.93\n2:27:55\n\n\n850016\n8.75328\n11.08632\n10.96\n2:37:21\n\n\n900000\n8.40084\n10.79851\n10.88\n2:46:33\n\n\n950016\n8.73481\n11.27116\n10.96\n2:55:45\n\n\n1000000\n8.55846\n11.28967\n10.86\n3:04:57\n\n\n1004800\n8.09170\n11.12924\n10.86\n3:05:51\n\n\n\n\n\n    \n      \n      100.00% [6280/6280 37:12&lt;00:00 #200960/200960 loss: 8.092 / 11.129]\n    \n    \n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 11 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 10 batches x 32 samples, 1.9 hours) was reported to be 10 (when accessing len(dataloader)), but 12 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6280 batches x 32 samples, 1306.1 hours) was reported to be 6280 (when accessing len(dataloader)), but 6281 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n\n\n\n\n# base.en whisper with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset (removed 1st and last segments)\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 10.79'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:09:42&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n19.44056\n22.67257\n11.13\n09:46\n\n\n100000\n13.55178\n14.58443\n11.26\n19:23\n\n\n150016\n11.96837\n13.18968\n11.09\n29:00\n\n\n200000\n11.43871\n12.44640\n11.05\n38:49\n\n\n250016\n11.28360\n11.70081\n11.10\n48:26\n\n\n300000\n10.83751\n11.31110\n11.09\n58:03\n\n\n350016\n10.69315\n11.17086\n11.12\n1:07:40\n\n\n400000\n9.98770\n10.92539\n11.05\n1:17:30\n\n\n450016\n9.83174\n10.69181\n11.05\n1:27:07\n\n\n500000\n9.77236\n10.48352\n11.14\n1:36:44\n\n\n550016\n9.66632\n10.36597\n11.09\n1:46:21\n\n\n600000\n9.40930\n10.08656\n11.02\n1:56:09\n\n\n650016\n9.44357\n9.92484\n11.04\n2:05:46\n\n\n700000\n8.96556\n9.79054\n11.06\n2:15:23\n\n\n750016\n8.83601\n9.65099\n11.01\n2:25:00\n\n\n800000\n8.66107\n9.39148\n11.12\n2:34:48\n\n\n850016\n8.44581\n9.40969\n11.00\n2:44:26\n\n\n900000\n8.56439\n9.22455\n11.05\n2:54:03\n\n\n950016\n8.52489\n9.30351\n11.03\n3:03:40\n\n\n981120\n8.84632\n9.33108\n10.79\n3:09:42\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 37:57&lt;00:00 #196224/196224 loss: 8.846 / 9.331]\n    \n    \n\n\n\n/tmp/ipykernel_90303/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_90303/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6132 batches x 32 samples, 1277.7 hours) was reported to be 6132 (when accessing len(dataloader)), but 6133 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n\n\n\n\n# base.en whisper with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=1024, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=12, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-1024c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 9.36'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:08:14&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n21.66206\n27.27091\n9.59\n09:41\n\n\n100000\n15.25066\n16.20915\n9.53\n19:13\n\n\n150016\n13.21848\n14.25581\n9.54\n28:45\n\n\n200000\n11.82871\n13.98582\n9.49\n38:30\n\n\n250016\n11.85884\n13.12596\n9.42\n48:02\n\n\n300000\n11.54107\n12.60187\n9.43\n57:34\n\n\n350016\n11.45310\n12.29700\n9.46\n1:07:07\n\n\n400000\n11.08207\n11.98462\n9.38\n1:16:51\n\n\n450016\n10.65160\n11.61482\n9.44\n1:26:24\n\n\n500000\n10.69448\n11.57619\n9.34\n1:35:56\n\n\n550016\n10.25768\n11.15084\n9.38\n1:45:29\n\n\n600000\n9.86860\n10.86430\n9.48\n1:55:14\n\n\n650016\n9.90988\n10.71315\n9.44\n2:04:47\n\n\n700000\n9.53233\n10.52028\n9.42\n2:14:19\n\n\n750016\n9.89578\n10.26827\n9.36\n2:23:52\n\n\n800000\n9.15078\n10.15152\n9.42\n2:33:36\n\n\n850016\n9.16481\n9.96554\n9.34\n2:43:09\n\n\n900000\n9.14512\n9.90501\n9.40\n2:52:42\n\n\n950016\n9.18524\n9.92719\n9.36\n3:02:15\n\n\n981120\n8.97033\n9.95517\n9.36\n3:08:14\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 37:41&lt;00:00 #196224/196224 loss: 8.970 / 9.955]\n    \n    \n\n\n\n/tmp/ipykernel_90303/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_90303/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n\n\n\n\n# base.en whisper with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=64, q_depth=1, n_head=8, depth=1,\n                                  downsample=1, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-64c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 5.64'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:09:51&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n76.17780\n192.67165\n5.82\n09:48\n\n\n100000\n27.85803\n31.11143\n5.71\n19:25\n\n\n150016\n19.38920\n22.02595\n5.75\n29:02\n\n\n200000\n16.75521\n18.75611\n5.68\n38:51\n\n\n250016\n16.22832\n17.68415\n5.60\n48:29\n\n\n300000\n15.28871\n16.20028\n5.68\n58:06\n\n\n350016\n14.91663\n16.24565\n5.63\n1:07:43\n\n\n400000\n14.08824\n15.30097\n5.64\n1:17:32\n\n\n450016\n13.53690\n15.08575\n5.61\n1:27:10\n\n\n500000\n13.62558\n14.45319\n5.65\n1:36:47\n\n\n550016\n12.45450\n13.74045\n5.66\n1:46:25\n\n\n600000\n12.25172\n14.05763\n5.68\n1:56:14\n\n\n650016\n12.76195\n13.71730\n5.69\n2:05:51\n\n\n700000\n12.19483\n13.02070\n5.61\n2:15:28\n\n\n750016\n11.83110\n12.79714\n5.62\n2:25:06\n\n\n800000\n12.23673\n12.70706\n5.73\n2:34:56\n\n\n850016\n11.69901\n12.50606\n5.64\n2:44:34\n\n\n900000\n12.03180\n12.29434\n5.71\n2:54:11\n\n\n950016\n12.06521\n12.22985\n5.67\n3:03:49\n\n\n981120\n13.17802\n12.70389\n5.64\n3:09:51\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 38:00&lt;00:00 #196224/196224 loss: 13.178 / 12.704]\n    \n    \n\n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6132 batches x 32 samples, 1277.7 hours) was reported to be 6132 (when accessing len(dataloader)), but 6133 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=512, q_depth=1, n_head=8, depth=1,\n                                  downsample=1, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=12, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-512c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 8.44'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:10:13&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n21.94018\n27.54010\n8.70\n09:48\n\n\n100000\n15.30265\n16.38729\n8.72\n19:26\n\n\n150016\n13.55491\n14.22489\n8.67\n29:04\n\n\n200000\n12.27958\n13.59388\n8.53\n38:54\n\n\n250016\n11.48394\n12.79483\n8.59\n48:33\n\n\n300000\n11.45791\n12.34518\n8.52\n58:11\n\n\n350016\n11.51288\n11.73254\n8.54\n1:07:49\n\n\n400000\n11.04880\n11.61340\n8.44\n1:17:41\n\n\n450016\n10.74074\n11.15114\n8.51\n1:27:20\n\n\n500000\n10.22759\n11.11760\n8.52\n1:36:59\n\n\n550016\n10.23485\n10.82111\n8.45\n1:46:38\n\n\n600000\n9.62602\n10.52901\n8.48\n1:56:30\n\n\n650016\n9.54247\n10.39591\n8.40\n2:06:08\n\n\n700000\n9.27610\n10.17579\n8.41\n2:15:47\n\n\n750016\n9.39848\n10.03072\n8.46\n2:25:25\n\n\n800000\n8.95939\n9.87603\n8.49\n2:35:15\n\n\n850016\n9.08446\n9.74571\n8.47\n2:44:54\n\n\n900000\n8.76172\n9.79162\n8.43\n2:54:32\n\n\n950016\n9.12931\n9.58630\n8.47\n3:04:10\n\n\n981120\n9.33700\n9.72177\n8.44\n3:10:13\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 38:02&lt;00:00 #196224/196224 loss: 9.337 / 9.722]\n    \n    \n\n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=64, vq_codes=512, q_depth=1, n_head=8, depth=1,\n                                  downsample=1, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 8.55'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 38:00&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n24.54137\n31.36435\n8.57\n09:47\n\n\n100000\n15.90889\n17.09020\n8.58\n19:26\n\n\n150016\n13.30405\n13.95759\n8.51\n29:05\n\n\n196224\n14.19891\n12.88708\n8.55\n38:00\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 38:00&lt;00:00 #196224/196224 loss: 14.199 / 12.887]\n    \n    \n\n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 11.28'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 37:54&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n17.26417\n22.29299\n11.24\n09:45\n\n\n100000\n12.41381\n14.22859\n11.25\n19:22\n\n\n150016\n11.16801\n11.97096\n11.21\n29:00\n\n\n196224\n10.49819\n10.57301\n11.28\n37:54\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 37:54&lt;00:00 #196224/196224 loss: 10.498 / 10.573]\n    \n    \n\n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-preconv-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 10.75'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:11:21&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.85334\n22.89696\n10.80\n09:51\n\n\n100000\n13.86454\n16.37101\n10.73\n19:33\n\n\n150016\n12.85605\n13.55042\n10.70\n29:15\n\n\n200000\n11.59676\n12.87997\n10.70\n39:09\n\n\n250016\n11.12804\n12.39809\n10.76\n48:52\n\n\n300000\n11.10460\n11.67927\n10.78\n58:33\n\n\n350016\n11.11719\n11.55583\n10.77\n1:08:16\n\n\n400000\n10.57183\n11.07552\n10.69\n1:18:09\n\n\n450016\n10.49243\n10.82820\n10.79\n1:27:51\n\n\n500000\n10.20853\n10.77793\n10.81\n1:37:33\n\n\n550016\n10.11812\n10.54805\n10.73\n1:47:15\n\n\n600000\n9.56493\n10.22062\n10.77\n1:57:10\n\n\n650016\n9.40594\n10.19217\n10.68\n2:06:52\n\n\n700000\n9.17259\n9.85726\n10.74\n2:16:34\n\n\n750016\n9.18224\n9.74915\n10.68\n2:26:17\n\n\n800000\n8.92105\n9.47104\n10.70\n2:36:09\n\n\n850016\n8.61280\n9.39290\n10.71\n2:45:51\n\n\n900000\n8.43418\n9.33166\n10.72\n2:55:33\n\n\n950016\n8.57911\n9.33823\n10.71\n3:05:16\n\n\n981120\n8.63924\n9.37749\n10.75\n3:11:21\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 38:16&lt;00:00 #196224/196224 loss: 8.639 / 9.377]\n    \n    \n\n\n\n/tmp/ipykernel_100642/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_100642/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\nIOPub message rate exceeded.\nThe notebook server will temporarily stop sending output\nto the client in order to avoid crashing it.\nTo change this limit, set the config variable\n`--NotebookApp.iopub_msg_rate_limit`.\n\nCurrent values:\nNotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\nNotebookApp.rate_limit_window=3.0 (secs)\n\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset, mean downsampling\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 5\n\n\n\n\n\n'Entropy: 10.87'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 3:09:50&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n17.48580\n22.87051\n10.93\n09:49\n\n\n100000\n13.30088\n14.67394\n11.07\n19:26\n\n\n150016\n12.26683\n12.99752\n10.98\n29:04\n\n\n200000\n11.53840\n12.33599\n10.96\n38:53\n\n\n250016\n10.86994\n12.00824\n11.01\n48:30\n\n\n300000\n10.59976\n11.63654\n11.01\n58:08\n\n\n350016\n10.76181\n11.29659\n10.93\n1:07:45\n\n\n400000\n9.99428\n10.90412\n10.98\n1:17:35\n\n\n450016\n9.78972\n10.65274\n10.92\n1:27:13\n\n\n500000\n9.70262\n10.54080\n10.93\n1:36:50\n\n\n550016\n9.86663\n10.32896\n10.96\n1:46:28\n\n\n600000\n9.41082\n10.16734\n10.97\n1:56:16\n\n\n650016\n9.54473\n9.94173\n10.96\n2:05:53\n\n\n700000\n9.06406\n9.71947\n10.93\n2:15:30\n\n\n750016\n9.10101\n9.46919\n10.93\n2:25:08\n\n\n800000\n8.60536\n9.40041\n10.94\n2:34:56\n\n\n850016\n8.50216\n9.23997\n10.89\n2:44:34\n\n\n900000\n8.29970\n9.23626\n10.90\n2:54:11\n\n\n950016\n8.52151\n9.20892\n10.93\n3:03:48\n\n\n981120\n8.69804\n9.14721\n10.87\n3:09:50\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 37:58&lt;00:00 #196224/196224 loss: 8.698 / 9.147]\n    \n    \n\n\n\n/tmp/ipykernel_129075/774804256.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_129075/774804256.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6132 batches x 32 samples, 1277.7 hours) was reported to be 6132 (when accessing len(dataloader)), but 6133 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset, mean downsampling\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\nvqmodel.ensure_whisper()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=16, visual_class=RQVisual)\n\n\n\n\n'Entropy: 10.91'\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/5 00:00&lt;?]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50008\n15.93577\n18.26651\n10.88\n31:51\n\n\n71736\n14.07252\n15.22314\n10.91\n57:51\n\n\n\n\n\n    \n      \n      35.23% [5124/14546 57:50&lt;1:46:20 #14348/203648 loss: 14.073 / 15.223]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset, mean downsampling\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\nvqmodel.ensure_whisper()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=8, visual_class=RQVisual)\n#vqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned.model')\n\n\n\n\n'Entropy: 10.75'\n\n\n\n\n\n\n\n\n    \n      \n      20.00% [1/5 30:53&lt;2:03:32]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50008\n17.99252\n21.13446\n10.86\n07:13\n\n\n100002\n14.73851\n15.26074\n10.74\n14:30\n\n\n150010\n12.67679\n13.50757\n10.61\n22:25\n\n\n200004\n11.98636\n12.63929\n10.72\n30:13\n\n\n248374\n12.14378\n12.26164\n10.75\n37:45\n\n\n\n\n\n    \n      \n      22.25% [3236/14546 06:51&lt;23:57 #49675/203648 loss: 12.144 / 12.262]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset, mean downsampling, eqvad\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=5, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\nvqmodel.save_model('vq-base.en-2d-4096c-cosine32-padfix-premlp-premean-learnpos-5e-cleaned-eqvad.model')\n\nOneCycle: 9933 5\n\n\n\n\n\n'Entropy: 9.83'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [5/5 5:07:42&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.06458\n19.45549\n10.27\n09:48\n\n\n100000\n13.27705\n13.06077\n10.36\n19:27\n\n\n150016\n11.91958\n12.15395\n10.17\n29:05\n\n\n200000\n11.59404\n11.67862\n10.28\n38:44\n\n\n250016\n11.44242\n11.32514\n10.16\n48:22\n\n\n300000\n10.80200\n11.16721\n10.17\n58:01\n\n\n350016\n10.78535\n10.94168\n10.32\n1:07:53\n\n\n400000\n10.66275\n10.93297\n10.21\n1:17:32\n\n\n450016\n11.32866\n10.82697\n10.23\n1:27:11\n\n\n500000\n10.40007\n10.87806\n10.05\n1:36:50\n\n\n550016\n10.74838\n10.63030\n10.02\n1:46:30\n\n\n600000\n10.57567\n10.58560\n9.97\n1:56:08\n\n\n650016\n10.26159\n10.44148\n10.19\n2:06:01\n\n\n700000\n10.08803\n10.51371\n10.12\n2:15:40\n\n\n750016\n10.02600\n10.39278\n9.97\n2:25:19\n\n\n800000\n10.27624\n10.39350\n10.06\n2:34:58\n\n\n850016\n10.19159\n10.25763\n9.81\n2:44:37\n\n\n900000\n10.08171\n10.23527\n10.00\n2:54:16\n\n\n950016\n9.88339\n10.25396\n9.92\n3:03:55\n\n\n1000000\n9.62146\n10.11803\n10.06\n3:13:46\n\n\n1050016\n9.46334\n10.04561\n9.84\n3:23:25\n\n\n1100000\n9.51465\n10.11484\n9.79\n3:33:04\n\n\n1150016\n9.50131\n9.95828\n9.79\n3:42:43\n\n\n1200000\n9.53149\n9.94314\n9.89\n3:52:22\n\n\n1250016\n9.33688\n9.85693\n9.80\n4:02:01\n\n\n1300000\n9.26627\n9.81014\n9.75\n4:11:53\n\n\n1350016\n9.37144\n9.76661\n9.77\n4:21:32\n\n\n1400000\n9.06240\n9.80434\n9.76\n4:31:11\n\n\n1450016\n9.10573\n9.80284\n9.77\n4:40:50\n\n\n1500000\n9.01136\n9.71748\n9.74\n4:50:29\n\n\n1550016\n9.15775\n9.71512\n9.85\n5:00:08\n\n\n1589280\n9.26362\n9.71802\n9.83\n5:07:42\n\n\n\n\n\n    \n      \n      100.00% [9933/9933 1:01:29&lt;00:00 #317856/317856 loss: 9.264 / 9.718]\n    \n    \n\n\n\n/tmp/ipykernel_133489/774804256.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_133489/774804256.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 9933 batches x 32 samples, 1275.0 hours) was reported to be 9933 (when accessing len(dataloader)), but 9934 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\n# downsample conv\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n#vqmodel.save_model('vq-base.en-512c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 10.70'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 38:13&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.56527\n21.86226\n10.70\n09:50\n\n\n100000\n14.16297\n14.83381\n10.66\n19:32\n\n\n150016\n11.57994\n12.28649\n10.68\n29:14\n\n\n196224\n10.27239\n10.96855\n10.70\n38:13\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 38:13&lt;00:00 #196224/196224 loss: 10.272 / 10.969]\n    \n    \n\n\n\n/tmp/ipykernel_100642/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_100642/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:645: UserWarning: Length of IterableDataset Dataset: 6132 batches x 32 samples, 1277.7 hours) was reported to be 6132 (when accessing len(dataloader)), but 6133 samples have been fetched. For multiprocessing data-loading, this could be caused by not properly configuring the IterableDataset replica at each worker. Please see https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\n  warnings.warn(warn_msg)\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=64, vq_codes=4096, q_depth=1, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n#vqmodel.save_model('vq-base.en-512c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 10.14'\n\n\n\n\n\n\n\n\n    \n      \n      0.00% [0/1 00:00&lt;?]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n19.88679\n26.18120\n10.21\n09:49\n\n\n100000\n14.04911\n15.88962\n10.19\n19:26\n\n\n107520\n13.98125\n15.41472\n10.14\n20:55\n\n\n\n\n\n    \n      \n      54.79% [3360/6132 20:54&lt;17:14 #107520/196224 loss: 13.981 / 15.415]\n    \n    \n\n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=4096, q_depth=1, n_head=8, depth=2,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n#vqmodel.save_model('vq-base.en-512c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 11.10'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 40:03&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n18.68695\n25.23358\n11.06\n10:18\n\n\n100000\n13.17344\n14.20349\n11.11\n20:28\n\n\n150016\n10.66736\n11.51643\n11.02\n30:39\n\n\n196224\n9.68099\n10.36363\n11.10\n40:03\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 40:03&lt;00:00 #196224/196224 loss: 9.681 / 10.364]\n    \n    \n\n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)\n\n\n\n\n\n\n\n\n\n\n# base.en! with learned positional embeddings, out_blocks after positional, mlp before vq\n# cleaned dataset\nvqmodel = RQBottleneckTransformer(codebook_dim=32, vq_codes=64, q_depth=2, n_head=8, depth=1,\n                                  downsample=2, threshold_ema_dead_code=0, use_cosine_sim=True, whisper_model_name=\"base.en\").cuda()\ntrain(\"svq\", vqmodel, train_ds, val_ds, bs=14, epochs=1, lr=3e-3, warmup_steps=2000,\n      run_valid_every_iters=10000, table_row_every_iters=50000, dl_workers=4, visual_class=RQVisual)\n#vqmodel.save_model('vq-base.en-512c-cosine32-padfix-premlp-learnpos-5e-cleaned.model')\n\nOneCycle: 6132 1\n\n\n\n\n\n'Entropy: 5.65'\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [1/1 37:35&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ncodebook entropy\ntime\n\n\n\n\n50016\n82.99027\n173.42301\n5.91\n09:42\n\n\n100000\n31.85972\n36.78515\n5.81\n19:14\n\n\n150016\n23.16688\n25.48340\n5.76\n28:46\n\n\n196224\n20.68511\n23.00216\n5.65\n37:36\n\n\n\n\n\n    \n      \n      100.00% [6132/6132 37:35&lt;00:00 #196224/196224 loss: 20.685 / 23.002]\n    \n    \n\n\n\n/tmp/ipykernel_94907/1747892456.py:43: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.clear()\n/tmp/ipykernel_94907/1747892456.py:46: UserWarning: Attempt to set non-positive xlim on a log-scaled axis will be ignored.\n  loss_p.set_xlim(10000, self.total_steps)"
  },
  {
    "objectID": "2A. Speaker Embeddings.html",
    "href": "2A. Speaker Embeddings.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "Doing transcription means sampling from the Whisper auto-regresive decoder. This is too slow to do for each training batch. Fortunately the trainscriptions are small text snippets so we can precompute them once for the whole dataset.\nWe use segments from Voice Activity Detection to reduce any boundary issues, the we use webdataset to yields multiple chunks from a FLAC file we only load once. The VAD segments are merged into longer chunks to make Whisper processing more efficent (it always processes 30s at a time)\nUsage:\npython -m whisperspeech.extract_spk_emb librilight-large-wo6454-flac-000002.tar\nYou can pass in either a URL or a local file name. Either way it will expect a vad file in the local directory. The result will go into a file in the current directory named after the source file but replacing flac with txt."
  },
  {
    "objectID": "2A. Speaker Embeddings.html#precompute-whisper-transcriptions-for-vq-bottleneck-distilation",
    "href": "2A. Speaker Embeddings.html#precompute-whisper-transcriptions-for-vq-bottleneck-distilation",
    "title": "WhisperSpeech",
    "section": "",
    "text": "Doing transcription means sampling from the Whisper auto-regresive decoder. This is too slow to do for each training batch. Fortunately the trainscriptions are small text snippets so we can precompute them once for the whole dataset.\nWe use segments from Voice Activity Detection to reduce any boundary issues, the we use webdataset to yields multiple chunks from a FLAC file we only load once. The VAD segments are merged into longer chunks to make Whisper processing more efficent (it always processes 30s at a time)\nUsage:\npython -m whisperspeech.extract_spk_emb librilight-large-wo6454-flac-000002.tar\nYou can pass in either a URL or a local file name. Either way it will expect a vad file in the local directory. The result will go into a file in the current directory named after the source file but replacing flac with txt."
  },
  {
    "objectID": "2A. Speaker Embeddings.html#batch-processing",
    "href": "2A. Speaker Embeddings.html#batch-processing",
    "title": "WhisperSpeech",
    "section": "Batch processing",
    "text": "Batch processing\nLet’s put everything above together.\n\ndl = chunked_dataset('../cc-small/cc-mix-000000.tar', 'mix')\nfor keys, samples, seconds in dl: break\nkeys, samples, seconds\n\n(['cc/7 Sec Riddles/[-hIfETsPxPg] New TYPE Of Riddles： Can You Ace Our New Game？ ？_023',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_037',\n  'cc/7 Sec Riddles/[-hIfETsPxPg] New TYPE Of Riddles： Can You Ace Our New Game？ ？_009',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_010',\n  'cc/7 Sec Riddles/[-hIfETsPxPg] New TYPE Of Riddles： Can You Ace Our New Game？ ？_004',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_049',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_000',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_024',\n  'cc/7 Sec Riddles/[-hIfETsPxPg] New TYPE Of Riddles： Can You Ace Our New Game？ ？_033',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_034',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_012',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_052',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_007',\n  'cc/7 Sec Riddles/[-hIfETsPxPg] New TYPE Of Riddles： Can You Ace Our New Game？ ？_030',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_055',\n  'cc/Bon Appetit/[jmntzm5yBYY] Melissa Makes Chicken Afritada ｜ From the Home Kitchen ｜ Bon Appétit_006'],\n tensor([[-0.0154, -0.0289, -0.0376,  ...,  0.0000,  0.0000,  0.0000],\n         [-0.0035, -0.0058, -0.0082,  ...,  0.0000,  0.0000,  0.0000],\n         [-0.0082, -0.0150, -0.0179,  ...,  0.0000,  0.0000,  0.0000],\n         ...,\n         [-0.0018,  0.0017,  0.0005,  ...,  0.0000,  0.0000,  0.0000],\n         [ 0.0014,  0.0021,  0.0019,  ...,  0.0000,  0.0000,  0.0000],\n         [ 0.0055,  0.0106,  0.0086,  ...,  0.0000,  0.0000,  0.0000]]),\n tensor([ 5.1536,  8.5666,  2.2867, 22.5939,  1.7406, 22.4744,  2.2355,  2.3549,\n          2.0307, 18.0717,  6.4505,  2.1843,  1.6382,  5.5461,  2.6450, 29.1297]))\n\n\n\nclassifier = EncoderClassifier.from_hparams(\"speechbrain/spkrec-ecapa-voxceleb\",\n                                            savedir=os.path.expanduser(\"~/.cache/speechbrain/\"),\n                                            run_opts={\"device\": \"cuda\"})\n\n\nembs = F.normalize(classifier.encode_batch(samples, wav_lens=seconds/30).squeeze(1), dim=-1)\n\n\nembs  @ embs.T\n\ntensor([[ 1.0000e+00, -1.5016e-01, -1.0663e-01,  7.4717e-01,  6.6663e-01,\n          6.7088e-01,  7.3192e-01,  8.0751e-01, -1.4667e-01, -1.5538e-01,\n          7.8594e-01, -1.7131e-01,  4.7389e-02,  3.8923e-01,  8.0528e-02,\n          6.8884e-02],\n        [-1.5016e-01,  1.0000e+00,  7.4450e-01, -7.8480e-02, -9.2287e-02,\n         -4.8926e-02, -1.8269e-01, -1.2868e-01,  6.2437e-01,  7.6687e-01,\n         -1.2109e-01,  5.7231e-01,  5.4483e-02, -1.3711e-02,  5.4225e-02,\n          1.0608e-01],\n        [-1.0663e-01,  7.4450e-01,  1.0000e+00, -9.6316e-02, -8.8784e-02,\n         -3.4282e-02, -1.6323e-01, -1.2561e-01,  5.7611e-01,  7.6471e-01,\n         -1.1900e-01,  5.1093e-01,  3.6564e-02,  2.1784e-03,  4.5240e-02,\n          8.6847e-02],\n        [ 7.4717e-01, -7.8480e-02, -9.6316e-02,  1.0000e+00,  6.9144e-01,\n          7.3513e-01,  7.2880e-01,  7.7707e-01, -8.8781e-02, -8.2090e-02,\n          7.7152e-01, -5.2820e-02,  7.3040e-02,  3.4047e-01,  9.3617e-02,\n          1.1111e-01],\n        [ 6.6663e-01, -9.2287e-02, -8.8784e-02,  6.9144e-01,  1.0000e+00,\n          7.1832e-01,  6.3586e-01,  7.3829e-01, -7.3225e-02, -1.2791e-01,\n          7.3249e-01, -4.8655e-04, -2.3932e-02,  3.5492e-01,  5.7829e-02,\n          1.2558e-01],\n        [ 6.7088e-01, -4.8926e-02, -3.4282e-02,  7.3513e-01,  7.1832e-01,\n          1.0000e+00,  6.7989e-01,  7.1707e-01, -1.1102e-01, -3.2756e-02,\n          7.0298e-01,  9.7910e-04,  3.2516e-02,  3.2002e-01,  9.5534e-02,\n          1.2125e-01],\n        [ 7.3192e-01, -1.8269e-01, -1.6323e-01,  7.2880e-01,  6.3586e-01,\n          6.7989e-01,  1.0000e+00,  7.4862e-01, -1.4716e-01, -1.8850e-01,\n          7.7709e-01, -1.4848e-01,  3.5645e-02,  3.9155e-01,  8.5304e-02,\n          7.6598e-02],\n        [ 8.0751e-01, -1.2868e-01, -1.2561e-01,  7.7707e-01,  7.3829e-01,\n          7.1707e-01,  7.4862e-01,  1.0000e+00, -1.3192e-01, -9.4631e-02,\n          8.1980e-01, -1.0383e-01, -1.2569e-02,  4.0366e-01,  1.3611e-02,\n          7.2108e-02],\n        [-1.4667e-01,  6.2437e-01,  5.7611e-01, -8.8781e-02, -7.3225e-02,\n         -1.1102e-01, -1.4716e-01, -1.3192e-01,  1.0000e+00,  6.1238e-01,\n         -7.4339e-02,  4.6340e-01, -3.2115e-02,  1.9445e-02, -2.3383e-03,\n         -5.2721e-04],\n        [-1.5538e-01,  7.6687e-01,  7.6471e-01, -8.2090e-02, -1.2791e-01,\n         -3.2756e-02, -1.8850e-01, -9.4631e-02,  6.1238e-01,  1.0000e+00,\n         -1.2142e-01,  5.6736e-01,  3.0472e-02, -2.1869e-02,  3.7176e-02,\n          1.1145e-01],\n        [ 7.8594e-01, -1.2109e-01, -1.1900e-01,  7.7152e-01,  7.3249e-01,\n          7.0298e-01,  7.7709e-01,  8.1980e-01, -7.4339e-02, -1.2142e-01,\n          1.0000e+00, -4.7116e-02,  3.0283e-02,  3.6122e-01,  3.7660e-02,\n          1.3460e-01],\n        [-1.7131e-01,  5.7231e-01,  5.1093e-01, -5.2820e-02, -4.8655e-04,\n          9.7910e-04, -1.4848e-01, -1.0383e-01,  4.6340e-01,  5.6736e-01,\n         -4.7116e-02,  1.0000e+00,  1.2047e-01,  1.8673e-02,  1.4013e-01,\n          1.9592e-01],\n        [ 4.7389e-02,  5.4483e-02,  3.6564e-02,  7.3040e-02, -2.3932e-02,\n          3.2516e-02,  3.5645e-02, -1.2569e-02, -3.2115e-02,  3.0472e-02,\n          3.0283e-02,  1.2047e-01,  1.0000e+00, -2.5141e-02,  8.7659e-01,\n          6.1994e-01],\n        [ 3.8923e-01, -1.3711e-02,  2.1784e-03,  3.4047e-01,  3.5492e-01,\n          3.2002e-01,  3.9155e-01,  4.0366e-01,  1.9445e-02, -2.1869e-02,\n          3.6122e-01,  1.8673e-02, -2.5141e-02,  1.0000e+00,  2.9265e-04,\n          2.0769e-02],\n        [ 8.0528e-02,  5.4225e-02,  4.5240e-02,  9.3617e-02,  5.7829e-02,\n          9.5534e-02,  8.5304e-02,  1.3611e-02, -2.3383e-03,  3.7176e-02,\n          3.7660e-02,  1.4013e-01,  8.7659e-01,  2.9265e-04,  1.0000e+00,\n          6.3008e-01],\n        [ 6.8884e-02,  1.0608e-01,  8.6847e-02,  1.1111e-01,  1.2558e-01,\n          1.2125e-01,  7.6598e-02,  7.2108e-02, -5.2721e-04,  1.1145e-01,\n          1.3460e-01,  1.9592e-01,  6.1994e-01,  2.0769e-02,  6.3008e-01,\n          1.0000e+00]], device='cuda:0')\n\n\n\nseconds\n\ntensor([ 4.9147, 14.5051,  8.8225,  9.8293,  4.2150,  3.1399,  5.1536,  5.5290,\n         4.9317, 12.8499,  7.5085,  2.3379, 17.1672,  1.2287, 29.0785,  3.2935])\n\n\n\n((embs.unsqueeze(1) - embs.unsqueeze(0))**2).sum(-1)\n\ntensor([[0.0000, 2.3003, 2.2133, 0.5057, 0.6667, 0.6582, 0.5362, 0.3850, 2.2933,\n         2.3108, 0.4281, 2.3426, 1.9052, 1.2215, 1.8389, 1.8622],\n        [2.3003, 0.0000, 0.5110, 2.1570, 2.1846, 2.0979, 2.3654, 2.2574, 0.7513,\n         0.4663, 2.2422, 0.8554, 1.8910, 2.0274, 1.8916, 1.7878],\n        [2.2133, 0.5110, 0.0000, 2.1926, 2.1776, 2.0686, 2.3265, 2.2512, 0.8478,\n         0.4706, 2.2380, 0.9781, 1.9269, 1.9956, 1.9095, 1.8263],\n        [0.5057, 2.1570, 2.1926, 0.0000, 0.6171, 0.5297, 0.5424, 0.4459, 2.1776,\n         2.1642, 0.4570, 2.1056, 1.8539, 1.3191, 1.8128, 1.7778],\n        [0.6667, 2.1846, 2.1776, 0.6171, 0.0000, 0.5634, 0.7283, 0.5234, 2.1465,\n         2.2558, 0.5350, 2.0010, 2.0479, 1.2902, 1.8843, 1.7488],\n        [0.6582, 2.0979, 2.0686, 0.5297, 0.5634, 0.0000, 0.6402, 0.5659, 2.2220,\n         2.0655, 0.5940, 1.9980, 1.9350, 1.3600, 1.8089, 1.7575],\n        [0.5362, 2.3654, 2.3265, 0.5424, 0.7283, 0.6402, 0.0000, 0.5028, 2.2943,\n         2.3770, 0.4458, 2.2970, 1.9287, 1.2169, 1.8294, 1.8468],\n        [0.3850, 2.2574, 2.2512, 0.4459, 0.5234, 0.5659, 0.5028, 0.0000, 2.2638,\n         2.1893, 0.3604, 2.2077, 2.0251, 1.1927, 1.9728, 1.8558],\n        [2.2933, 0.7513, 0.8478, 2.1776, 2.1465, 2.2220, 2.2943, 2.2638, 0.0000,\n         0.7752, 2.1487, 1.0732, 2.0642, 1.9611, 2.0047, 2.0011],\n        [2.3108, 0.4663, 0.4706, 2.1642, 2.2558, 2.0655, 2.3770, 2.1893, 0.7752,\n         0.0000, 2.2428, 0.8653, 1.9391, 2.0437, 1.9256, 1.7771],\n        [0.4281, 2.2422, 2.2380, 0.4570, 0.5350, 0.5940, 0.4458, 0.3604, 2.1487,\n         2.2428, 0.0000, 2.0942, 1.9394, 1.2776, 1.9247, 1.7308],\n        [2.3426, 0.8554, 0.9781, 2.1056, 2.0010, 1.9980, 2.2970, 2.2077, 1.0732,\n         0.8653, 2.0942, 0.0000, 1.7591, 1.9627, 1.7197, 1.6082],\n        [1.9052, 1.8910, 1.9269, 1.8539, 2.0479, 1.9350, 1.9287, 2.0251, 2.0642,\n         1.9391, 1.9394, 1.7591, 0.0000, 2.0503, 0.2468, 0.7601],\n        [1.2215, 2.0274, 1.9956, 1.3191, 1.2902, 1.3600, 1.2169, 1.1927, 1.9611,\n         2.0437, 1.2776, 1.9627, 2.0503, 0.0000, 1.9994, 1.9585],\n        [1.8389, 1.8916, 1.9095, 1.8128, 1.8843, 1.8089, 1.8294, 1.9728, 2.0047,\n         1.9256, 1.9247, 1.7197, 0.2468, 1.9994, 0.0000, 0.7398],\n        [1.8622, 1.7878, 1.8263, 1.7778, 1.7488, 1.7575, 1.8468, 1.8558, 2.0011,\n         1.7771, 1.7308, 1.6082, 0.7601, 1.9585, 0.7398, 0.0000]],\n       device='cuda:0')\n\n\n\nplt.imshow(((embs.unsqueeze(1) - embs.unsqueeze(0))**2).sum(-1).cpu())"
  },
  {
    "objectID": "1c. vad merging.html",
    "href": "1c. vad merging.html",
    "title": "VAD merging",
    "section": "",
    "text": "source\n\nderived_name\n\n derived_name (input, kind, base='audio')\n\n\nds = wds.WebDataset(['../wolnelektury-wds2/wolnelektury-audio-000000.tar']).compose(\n    wds.decode(wds.torch_audio),\n    utils.merge_in(utils.derived_dataset('vad')),\n    utils.find_audio,\n    utils.split_to_chunks,\n    utils.merge_in(utils.derived_dataset('spk_emb')),\n)\n\n\nimport IPython\nimport time\n\n\nprev = None\nfor s in progress_bar(ds, total=20):\n    sim = F.cosine_similarity(torch.tensor(s['spk_emb.npy']), torch.tensor((prev if prev is not None else s)['spk_emb.npy']), dim=0)\n    secs = s['tend'] - s['tstart']\n    same = sim &gt; 0.6 if secs &gt; 2 else sim &gt; 0.1\n    if not same: print(\"new\")\n    print(s['__key__'], sim, secs)\n    display(IPython.display.Audio(s['samples'], rate=s['sample_rate']))\n    if secs &gt; 2:\n        prev = s\n    time.sleep(.5)\ns\n\n\nds = wds.WebDataset([utils.derived_name('../wolnelektury-wds2/wolnelektury-audio-000000.tar', 'vad')]).compose(\n    wds.decode(),\n    split,\n    utils.merge_in(utils.derived_dataset('spk_emb', base='vad', suffix='')),\n    merge_by_src_key,\n)\n\n\nfor s in ds: break\ns\n\n\nds = wds.WebDataset([utils.derived_name('../wolnelektury-wds2/wolnelektury-audio-000000.tar', 'vad')]).compose(\n    wds.decode(),\n    split,\n    utils.merge_in(utils.derived_dataset('spk_emb', base='vad', suffix='')),\n    merge_by_src_key,\n    chunk_merger,\n)\n\n\nfor s in ds: break\ns\n\n\nds = wds.WebDataset(['../wolnelektury-wds2/wolnelektury-audio-000000.tar']).compose(\n    wds.decode(wds.torch_audio),\n    utils.merge_in(utils.derived_dataset('vad')),\n    utils.find_audio,\n    utils.split_to_chunks,\n    utils.merge_in(utils.derived_dataset('spk_emb')),\n    merge_by_src_key,\n    chunk_merger,\n    utils.merge_in(utils.derived_dataset('audio', suffix='', decoders=[wds.torch_audio])),\n    utils.find_audio,\n    lambda x: utils.split_to_chunks(x, metakeys=['spk_emb.npy']),\n)\n\n\nfor s in ds: break\ns\n\n\nprev = None\nfor s in progress_bar(ds, total=20):\n    sim = F.cosine_similarity(torch.tensor(s['spk_emb.npy']), torch.tensor((prev if prev is not None else s)['spk_emb.npy']), dim=0)\n    secs = s['tend'] - s['tstart']\n    same = sim &gt; 0.6 if secs &gt; 2 else sim &gt; 0.1\n    if not same: print(\"new\")\n    print(s['__key__'], sim, secs, sum([e-s for s,e in s['orig_s']['subvads.pyd'][s['i']]]))\n    display(IPython.display.Audio(s['samples'], rate=s['sample_rate']))\n    if secs &gt; 2:\n        prev = s\n    time.sleep(.5)\n\n\nprepare_mvad('../wolnelektury-wds2/wolnelektury-audio-000000.tar')\n\n\n\n\n\n\n    \n      \n      100.00% [235/235 00:04&lt;00:00]\n    \n    \n\n\n\n!tar tf ../wolnelektury-wds2/wolnelektury-maxvad-000000.tar.gz\n\n./kornhauser-wiatr/kornhauser-wiatr_001.spk_emb.npy\n./kornhauser-wiatr/kornhauser-wiatr_001.subvads.pyd\n./kornhauser-wiatr/kornhauser-wiatr_001.vad.npy\n./fraszki-ksiegi-pierwsze-epitafium-wysockiemu/jan-kochanowski-fraszki-ksiegi-pierwsze-epitafium-wysockiemu.spk_emb.npy\n./fraszki-ksiegi-pierwsze-epitafium-wysockiemu/jan-kochanowski-fraszki-ksiegi-pierwsze-epitafium-wysockiemu.subvads.pyd\n./fraszki-ksiegi-pierwsze-epitafium-wysockiemu/jan-kochanowski-fraszki-ksiegi-pierwsze-epitafium-wysockiemu.vad.npy\n./kucharczyk-jak-modlitwa-ochrania-przed-zlodziejami/jak-modlitwa-ochrania-przed-zlodziejami.spk_emb.npy\n./kucharczyk-jak-modlitwa-ochrania-przed-zlodziejami/jak-modlitwa-ochrania-przed-zlodziejami.subvads.pyd\n./kucharczyk-jak-modlitwa-ochrania-przed-zlodziejami/jak-modlitwa-ochrania-przed-zlodziejami.vad.npy\n./nowakowska-niska-rozdzielczosc-proba-wody/proba-wody.spk_emb.npy\n./nowakowska-niska-rozdzielczosc-proba-wody/proba-wody.subvads.pyd\n./nowakowska-niska-rozdzielczosc-proba-wody/proba-wody.vad.npy\n./slowka-zbior-dziwna-przygoda-rodziny-polanieckich/tadeusz-boy-zelenski-slowka-zbior-dziwna-przygoda-rodziny-polanieckich.spk_emb.npy\n./slowka-zbior-dziwna-przygoda-rodziny-polanieckich/tadeusz-boy-zelenski-slowka-zbior-dziwna-przygoda-rodziny-polanieckich.subvads.pyd\n./slowka-zbior-dziwna-przygoda-rodziny-polanieckich/tadeusz-boy-zelenski-slowka-zbior-dziwna-przygoda-rodziny-polanieckich.vad.npy\n./piesni-ksiegi-wtore-piesn-xii/jan-kochanowski-piesni-ksiegi-wtore-piesn-xii-nie-masz-i-po-drugi-raz-nie-masz-watp.spk_emb.npy\n./piesni-ksiegi-wtore-piesn-xii/jan-kochanowski-piesni-ksiegi-wtore-piesn-xii-nie-masz-i-po-drugi-raz-nie-masz-watp.subvads.pyd\n./piesni-ksiegi-wtore-piesn-xii/jan-kochanowski-piesni-ksiegi-wtore-piesn-xii-nie-masz-i-po-drugi-raz-nie-masz-watp.vad.npy\n./sonety-krymskie-stepy-akermanskie/adam-mickiewicz-sonety-krymskie-stepy-akermanskie.spk_emb.npy\n./sonety-krymskie-stepy-akermanskie/adam-mickiewicz-sonety-krymskie-stepy-akermanskie.subvads.pyd\n./sonety-krymskie-stepy-akermanskie/adam-mickiewicz-sonety-krymskie-stepy-akermanskie.vad.npy\n./napoj-cienisty-balwan-ze-sniegu/boleslaw-lesmian-napoj-cienisty-postacie-cykl-balwan-ze-sniegu.spk_emb.npy\n./napoj-cienisty-balwan-ze-sniegu/boleslaw-lesmian-napoj-cienisty-postacie-cykl-balwan-ze-sniegu.subvads.pyd\n./napoj-cienisty-balwan-ze-sniegu/boleslaw-lesmian-napoj-cienisty-postacie-cykl-balwan-ze-sniegu.vad.npy\n./fraczek-zolw-wiercipieta-prosiaczek/fraczek-zolw-wiercipieta-prosiaczek_001.spk_emb.npy\n./fraczek-zolw-wiercipieta-prosiaczek/fraczek-zolw-wiercipieta-prosiaczek_001.subvads.pyd\n./fraczek-zolw-wiercipieta-prosiaczek/fraczek-zolw-wiercipieta-prosiaczek_001.vad.npy\n./grabinski-nietykalny/grabinski-nietykalny.spk_emb.npy\n./grabinski-nietykalny/grabinski-nietykalny.subvads.pyd\n./grabinski-nietykalny/grabinski-nietykalny.vad.npy\n./wol-i-mrowki/ignacy-krasicki-bajki-i-przypowiesci-wol-i-mrowki.spk_emb.npy\n./wol-i-mrowki/ignacy-krasicki-bajki-i-przypowiesci-wol-i-mrowki.subvads.pyd\n./wol-i-mrowki/ignacy-krasicki-bajki-i-przypowiesci-wol-i-mrowki.vad.npy\n./pszczola-w-bursztynie/jan-andrzej-morsztyn-pszczola-w-bursztynie.spk_emb.npy\n./pszczola-w-bursztynie/jan-andrzej-morsztyn-pszczola-w-bursztynie.subvads.pyd\n./pszczola-w-bursztynie/jan-andrzej-morsztyn-pszczola-w-bursztynie.vad.npy\n./jastrzab-i-sokol/ignacy-krasicki-bajki-i-przypowiesci-jastrzab-i-sokol.spk_emb.npy\n./jastrzab-i-sokol/ignacy-krasicki-bajki-i-przypowiesci-jastrzab-i-sokol.subvads.pyd\n./jastrzab-i-sokol/ignacy-krasicki-bajki-i-przypowiesci-jastrzab-i-sokol.vad.npy\n./fraszki-ksiegi-pierwsze-o-doktorze-hiszpanie/jan-kochanowski-fraszki-ksiegi-pierwsze-o-doktorze-hiszpanie.spk_emb.npy\n./fraszki-ksiegi-pierwsze-o-doktorze-hiszpanie/jan-kochanowski-fraszki-ksiegi-pierwsze-o-doktorze-hiszpanie.subvads.pyd\n./fraszki-ksiegi-pierwsze-o-doktorze-hiszpanie/jan-kochanowski-fraszki-ksiegi-pierwsze-o-doktorze-hiszpanie.vad.npy\n./perrault-kopciuszek/perrault-kopciuszek.spk_emb.npy\n./perrault-kopciuszek/perrault-kopciuszek.subvads.pyd\n./perrault-kopciuszek/perrault-kopciuszek.vad.npy\n./napoj-cienisty-wieczor/boleslaw-lesmian-napoj-cienisty-w-chmur-odbiciu-cykl-wieczor.spk_emb.npy\n./napoj-cienisty-wieczor/boleslaw-lesmian-napoj-cienisty-w-chmur-odbiciu-cykl-wieczor.subvads.pyd\n./napoj-cienisty-wieczor/boleslaw-lesmian-napoj-cienisty-w-chmur-odbiciu-cykl-wieczor.vad.npy\n./satyry-czesc-druga-malzenstwo/satyry-czesc-druga-malzenstwo.spk_emb.npy\n./satyry-czesc-druga-malzenstwo/satyry-czesc-druga-malzenstwo.subvads.pyd\n./satyry-czesc-druga-malzenstwo/satyry-czesc-druga-malzenstwo.vad.npy\n./slowka-zbior-spleen/tadeusz-boy-zelenski-slowka-zbior-spleen.spk_emb.npy\n./slowka-zbior-spleen/tadeusz-boy-zelenski-slowka-zbior-spleen.subvads.pyd\n./slowka-zbior-spleen/tadeusz-boy-zelenski-slowka-zbior-spleen.vad.npy\n./fraczek-zolw-wiercipieta-strus/fraczek-zolw-wiercipieta-strus_001.spk_emb.npy\n./fraczek-zolw-wiercipieta-strus/fraczek-zolw-wiercipieta-strus_001.subvads.pyd\n./fraczek-zolw-wiercipieta-strus/fraczek-zolw-wiercipieta-strus_001.vad.npy\n./janko-muzykant/janko-muzykant.spk_emb.npy\n./janko-muzykant/janko-muzykant.subvads.pyd\n./janko-muzykant/janko-muzykant.vad.npy\n./slowka-zbior-piosenki-zb-dobra-mama/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-dobra-mama.spk_emb.npy\n./slowka-zbior-piosenki-zb-dobra-mama/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-dobra-mama.subvads.pyd\n./slowka-zbior-piosenki-zb-dobra-mama/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-dobra-mama.vad.npy\n./lange-nowy-tarzan/antoni-lange-nowy-tarzan.spk_emb.npy\n./lange-nowy-tarzan/antoni-lange-nowy-tarzan.subvads.pyd\n./lange-nowy-tarzan/antoni-lange-nowy-tarzan.vad.npy\n./w-pamietniku-zofii-bobrowny/w-pamietniku-zofii-bobrowny.spk_emb.npy\n./w-pamietniku-zofii-bobrowny/w-pamietniku-zofii-bobrowny.subvads.pyd\n./w-pamietniku-zofii-bobrowny/w-pamietniku-zofii-bobrowny.vad.npy\n./fraczek-zolw-wiercipieta-zebra/fraczek-zolw-wiercipieta-zebra_001.spk_emb.npy\n./fraczek-zolw-wiercipieta-zebra/fraczek-zolw-wiercipieta-zebra_001.subvads.pyd\n./fraczek-zolw-wiercipieta-zebra/fraczek-zolw-wiercipieta-zebra_001.vad.npy\n./but-w-butonierce-milosc-na-aucie/bruno-jasienski-but-w-butonierce-tomik-milosc-na-aucie.spk_emb.npy\n./but-w-butonierce-milosc-na-aucie/bruno-jasienski-but-w-butonierce-tomik-milosc-na-aucie.subvads.pyd\n./but-w-butonierce-milosc-na-aucie/bruno-jasienski-but-w-butonierce-tomik-milosc-na-aucie.vad.npy\n./sonety-krymskie-grob-potockiej/adam-mickiewicz-sonety-krymskie-grob-potockiej.spk_emb.npy\n./sonety-krymskie-grob-potockiej/adam-mickiewicz-sonety-krymskie-grob-potockiej.subvads.pyd\n./sonety-krymskie-grob-potockiej/adam-mickiewicz-sonety-krymskie-grob-potockiej.vad.npy\n./do-matki/juliusz-slowacki-do-matki-zadrzy-ci-nieraz-serce-mila-matko-moja.spk_emb.npy\n./do-matki/juliusz-slowacki-do-matki-zadrzy-ci-nieraz-serce-mila-matko-moja.subvads.pyd\n./do-matki/juliusz-slowacki-do-matki-zadrzy-ci-nieraz-serce-mila-matko-moja.vad.npy\n./slowka-zbior-piosenki-zb-z-niewydanej-szopki-krakowskiej-na-rok-1908/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-z-niewydanej-szopki-krakowskiej-na-rok-1908.spk_emb.npy\n./slowka-zbior-piosenki-zb-z-niewydanej-szopki-krakowskiej-na-rok-1908/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-z-niewydanej-szopki-krakowskiej-na-rok-1908.subvads.pyd\n./slowka-zbior-piosenki-zb-z-niewydanej-szopki-krakowskiej-na-rok-1908/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-z-niewydanej-szopki-krakowskiej-na-rok-1908.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_030_tom-ii-rozdzial-lx.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_030_tom-ii-rozdzial-lx.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_030_tom-ii-rozdzial-lx.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_006_tom-ii-rozdzial-xxxv.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_006_tom-ii-rozdzial-xxxv.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_006_tom-ii-rozdzial-xxxv.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_017_tom-ii-rozdzial-xlvi.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_017_tom-ii-rozdzial-xlvi.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_017_tom-ii-rozdzial-xlvi.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_022_tom-ii-rozdzial-li.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_022_tom-ii-rozdzial-li.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_022_tom-ii-rozdzial-li.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_023_tom-ii-rozdzial-liii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_023_tom-ii-rozdzial-liii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_023_tom-ii-rozdzial-liii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_012_tom-ii-rozdzial-xli.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_012_tom-ii-rozdzial-xli.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_012_tom-ii-rozdzial-xli.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_019_tom-ii-rozdzial-xlviii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_019_tom-ii-rozdzial-xlviii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_019_tom-ii-rozdzial-xlviii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_010_tom-ii-rozdzial-xxxix.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_010_tom-ii-rozdzial-xxxix.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_010_tom-ii-rozdzial-xxxix.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_035_tom-ii-rozdzial-lii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_035_tom-ii-rozdzial-lii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_035_tom-ii-rozdzial-lii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_002_tom-ii-rozdzial-xxxi.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_002_tom-ii-rozdzial-xxxi.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_002_tom-ii-rozdzial-xxxi.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_025_tom-ii-rozdzial-lv.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_025_tom-ii-rozdzial-lv.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_025_tom-ii-rozdzial-lv.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_003_tom-ii-rozdzial-xxxii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_003_tom-ii-rozdzial-xxxii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_003_tom-ii-rozdzial-xxxii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_009_tom-ii-rozdzial-xxxviii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_009_tom-ii-rozdzial-xxxviii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_009_tom-ii-rozdzial-xxxviii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_008_tom-ii-rozdzial-xxxvii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_008_tom-ii-rozdzial-xxxvii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_008_tom-ii-rozdzial-xxxvii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_016_tom-ii-rozdzial-xlv.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_016_tom-ii-rozdzial-xlv.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_016_tom-ii-rozdzial-xlv.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_004_tom-ii-rozdzial-xxxiii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_004_tom-ii-rozdzial-xxxiii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_004_tom-ii-rozdzial-xxxiii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_032_tom-ii-rozdzial-lxii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_032_tom-ii-rozdzial-lxii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_032_tom-ii-rozdzial-lxii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_021_tom-ii-rozdzial-l.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_021_tom-ii-rozdzial-l.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_021_tom-ii-rozdzial-l.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_018_tom-ii-rozdzial-xlvii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_018_tom-ii-rozdzial-xlvii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_018_tom-ii-rozdzial-xlvii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_033_tom-ii-rozdzial-lxiii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_033_tom-ii-rozdzial-lxiii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_033_tom-ii-rozdzial-lxiii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_014_tom-ii-rozdzial-xliii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_014_tom-ii-rozdzial-xliii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_014_tom-ii-rozdzial-xliii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_005_tom-ii-rozdzial-xxxiv.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_005_tom-ii-rozdzial-xxxiv.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_005_tom-ii-rozdzial-xxxiv.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_034_tom-ii-rozdzial-lxiv.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_034_tom-ii-rozdzial-lxiv.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_034_tom-ii-rozdzial-lxiv.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_001_tom-ii-rozdzial-xxx.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_001_tom-ii-rozdzial-xxx.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_001_tom-ii-rozdzial-xxx.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_029_tom-ii-rozdzial-lix.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_029_tom-ii-rozdzial-lix.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_029_tom-ii-rozdzial-lix.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_007_tom-ii-rozdzial-xxxvi.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_007_tom-ii-rozdzial-xxxvi.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_007_tom-ii-rozdzial-xxxvi.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_024_tom-ii-rozdzial-liv.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_024_tom-ii-rozdzial-liv.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_024_tom-ii-rozdzial-liv.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_028_tom-ii-rozdzial-lviii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_028_tom-ii-rozdzial-lviii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_028_tom-ii-rozdzial-lviii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_026_tom-ii-rozdzial-lvi.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_026_tom-ii-rozdzial-lvi.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_026_tom-ii-rozdzial-lvi.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_011_tom-ii-rozdzial-xl.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_011_tom-ii-rozdzial-xl.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_011_tom-ii-rozdzial-xl.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_031_tom-ii-rozdzial-lxi.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_031_tom-ii-rozdzial-lxi.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_031_tom-ii-rozdzial-lxi.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_015_tom-ii-rozdzial-xliv.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_015_tom-ii-rozdzial-xliv.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_015_tom-ii-rozdzial-xliv.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_013_tom-ii-rozdzial-xlii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_013_tom-ii-rozdzial-xlii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_013_tom-ii-rozdzial-xlii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_027_tom-ii-rozdzial-lvii.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_027_tom-ii-rozdzial-lvii.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_027_tom-ii-rozdzial-lvii.vad.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_020_tom-ii-rozdzial-xlix.spk_emb.npy\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_020_tom-ii-rozdzial-xlix.subvads.pyd\n./dickens-dawid-copperfield-t2/dickens-dawid-copperfield-t2_020_tom-ii-rozdzial-xlix.vad.npy\n./historia-zoltej-cizemki/09-antonina-domanska-historia-zoltej-cizemki-tajemnica-dworku-pod-cmentarzem.spk_emb.npy\n./historia-zoltej-cizemki/09-antonina-domanska-historia-zoltej-cizemki-tajemnica-dworku-pod-cmentarzem.subvads.pyd\n./historia-zoltej-cizemki/09-antonina-domanska-historia-zoltej-cizemki-tajemnica-dworku-pod-cmentarzem.vad.npy\n./historia-zoltej-cizemki/11-antonina-domanska-historia-zoltej-cizemki-zakonczenie.spk_emb.npy\n./historia-zoltej-cizemki/11-antonina-domanska-historia-zoltej-cizemki-zakonczenie.subvads.pyd\n./historia-zoltej-cizemki/11-antonina-domanska-historia-zoltej-cizemki-zakonczenie.vad.npy\n./historia-zoltej-cizemki/08-antonina-domanska-historia-zoltej-cizemki-swiety-kazimierz.spk_emb.npy\n./historia-zoltej-cizemki/08-antonina-domanska-historia-zoltej-cizemki-swiety-kazimierz.subvads.pyd\n./historia-zoltej-cizemki/08-antonina-domanska-historia-zoltej-cizemki-swiety-kazimierz.vad.npy\n./historia-zoltej-cizemki/07-antonina-domanska-historia-zoltej-cizemki-jasiek.spk_emb.npy\n./historia-zoltej-cizemki/07-antonina-domanska-historia-zoltej-cizemki-jasiek.subvads.pyd\n./historia-zoltej-cizemki/07-antonina-domanska-historia-zoltej-cizemki-jasiek.vad.npy\n./historia-zoltej-cizemki/05-antonina-domanska-historia-zoltej-cizemki-u-jana-dlugosza.spk_emb.npy\n./historia-zoltej-cizemki/05-antonina-domanska-historia-zoltej-cizemki-u-jana-dlugosza.subvads.pyd\n./historia-zoltej-cizemki/05-antonina-domanska-historia-zoltej-cizemki-u-jana-dlugosza.vad.npy\n./historia-zoltej-cizemki/03-antonina-domanska-historia-zoltej-cizemki-u-wilow.spk_emb.npy\n./historia-zoltej-cizemki/03-antonina-domanska-historia-zoltej-cizemki-u-wilow.subvads.pyd\n./historia-zoltej-cizemki/03-antonina-domanska-historia-zoltej-cizemki-u-wilow.vad.npy\n./historia-zoltej-cizemki/10-antonina-domanska-historia-zoltej-cizemki-poreba.spk_emb.npy\n./historia-zoltej-cizemki/10-antonina-domanska-historia-zoltej-cizemki-poreba.subvads.pyd\n./historia-zoltej-cizemki/10-antonina-domanska-historia-zoltej-cizemki-poreba.vad.npy\n./historia-zoltej-cizemki/04-antonina-domanska-historia-zoltej-cizemki-konik-zwierzyniecki.spk_emb.npy\n./historia-zoltej-cizemki/04-antonina-domanska-historia-zoltej-cizemki-konik-zwierzyniecki.subvads.pyd\n./historia-zoltej-cizemki/04-antonina-domanska-historia-zoltej-cizemki-konik-zwierzyniecki.vad.npy\n./historia-zoltej-cizemki/02-antonina-domanska-historia-zoltej-cizemki-dziwny-pielgrzym.spk_emb.npy\n./historia-zoltej-cizemki/02-antonina-domanska-historia-zoltej-cizemki-dziwny-pielgrzym.subvads.pyd\n./historia-zoltej-cizemki/02-antonina-domanska-historia-zoltej-cizemki-dziwny-pielgrzym.vad.npy\n./historia-zoltej-cizemki/01-antonina-domanska-historia-zoltej-cizemki-w-domu-i-w-puszczy.spk_emb.npy\n./historia-zoltej-cizemki/01-antonina-domanska-historia-zoltej-cizemki-w-domu-i-w-puszczy.subvads.pyd\n./historia-zoltej-cizemki/01-antonina-domanska-historia-zoltej-cizemki-w-domu-i-w-puszczy.vad.npy\n./historia-zoltej-cizemki/06-antonina-domanska-historia-zoltej-cizemki-uczen-mistrza-wita.spk_emb.npy\n./historia-zoltej-cizemki/06-antonina-domanska-historia-zoltej-cizemki-uczen-mistrza-wita.subvads.pyd\n./historia-zoltej-cizemki/06-antonina-domanska-historia-zoltej-cizemki-uczen-mistrza-wita.vad.npy\n./slowka-zbior-piosenki-zb-piosenka-wzruszajaca/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-piosenka-wzruszajaca.spk_emb.npy\n./slowka-zbior-piosenki-zb-piosenka-wzruszajaca/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-piosenka-wzruszajaca.subvads.pyd\n./slowka-zbior-piosenki-zb-piosenka-wzruszajaca/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-piosenka-wzruszajaca.vad.npy\n./do-justyny/franciszek-karpinski-do-justyny.spk_emb.npy\n./do-justyny/franciszek-karpinski-do-justyny.subvads.pyd\n./do-justyny/franciszek-karpinski-do-justyny.vad.npy\n./pan-i-pies/ignacy-krasicki-bajki-i-przypowiesci-pan-i-pies.spk_emb.npy\n./pan-i-pies/ignacy-krasicki-bajki-i-przypowiesci-pan-i-pies.subvads.pyd\n./pan-i-pies/ignacy-krasicki-bajki-i-przypowiesci-pan-i-pies.vad.npy\n./pasterz-i-owce-bajki-nowe/pasterz-i-owce-bajki-nowe.spk_emb.npy\n./pasterz-i-owce-bajki-nowe/pasterz-i-owce-bajki-nowe.subvads.pyd\n./pasterz-i-owce-bajki-nowe/pasterz-i-owce-bajki-nowe.vad.npy\n./prawdziwy-opis-wypadku-z-p-waldemarem/prawdziwy-opis-wypadku-z-p-waldemarem.spk_emb.npy\n./prawdziwy-opis-wypadku-z-p-waldemarem/prawdziwy-opis-wypadku-z-p-waldemarem.subvads.pyd\n./prawdziwy-opis-wypadku-z-p-waldemarem/prawdziwy-opis-wypadku-z-p-waldemarem.vad.npy\n./grabinski-przypadek/grabinski-przypadek.spk_emb.npy\n./grabinski-przypadek/grabinski-przypadek.subvads.pyd\n./grabinski-przypadek/grabinski-przypadek.vad.npy\n./rozmowa-mistrza-polikarpa-ze-smiercia/rozmowa-mistrza-polikarpa-ze-smiercia.spk_emb.npy\n./rozmowa-mistrza-polikarpa-ze-smiercia/rozmowa-mistrza-polikarpa-ze-smiercia.subvads.pyd\n./rozmowa-mistrza-polikarpa-ze-smiercia/rozmowa-mistrza-polikarpa-ze-smiercia.vad.npy\n./hej-w-dzien-narodzenia/autor-nieznany-hej-w-dzien-narodzenia.spk_emb.npy\n./hej-w-dzien-narodzenia/autor-nieznany-hej-w-dzien-narodzenia.subvads.pyd\n./hej-w-dzien-narodzenia/autor-nieznany-hej-w-dzien-narodzenia.vad.npy\n./napoj-cienisty-dziewczyna/boleslaw-lesmian-napoj-cienisty-powiesc-o-rozumnej-dziewczynie-cykl-dziewczyna.spk_emb.npy\n./napoj-cienisty-dziewczyna/boleslaw-lesmian-napoj-cienisty-powiesc-o-rozumnej-dziewczynie-cykl-dziewczyna.subvads.pyd\n./napoj-cienisty-dziewczyna/boleslaw-lesmian-napoj-cienisty-powiesc-o-rozumnej-dziewczynie-cykl-dziewczyna.vad.npy\n./grabinski-niesamowita-opowiesc-na-tropie/grabinski-niesamowita-opowiesc-na-tropie.spk_emb.npy\n./grabinski-niesamowita-opowiesc-na-tropie/grabinski-niesamowita-opowiesc-na-tropie.subvads.pyd\n./grabinski-niesamowita-opowiesc-na-tropie/grabinski-niesamowita-opowiesc-na-tropie.vad.npy\n./sroczynska-lasowiackie-serce/lasowiackie-serce.spk_emb.npy\n./sroczynska-lasowiackie-serce/lasowiackie-serce.subvads.pyd\n./sroczynska-lasowiackie-serce/lasowiackie-serce.vad.npy\n./slowka-zbior-piosenki-zb-wiersz-inauguracyjny/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-wiersz-inauguracyjny-na-otwarcie-piatego-sezonu-zielonego-balonika.spk_emb.npy\n./slowka-zbior-piosenki-zb-wiersz-inauguracyjny/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-wiersz-inauguracyjny-na-otwarcie-piatego-sezonu-zielonego-balonika.subvads.pyd\n./slowka-zbior-piosenki-zb-wiersz-inauguracyjny/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-wiersz-inauguracyjny-na-otwarcie-piatego-sezonu-zielonego-balonika.vad.npy\n./nasza-czarna-jaskoleczka/nasza-czarna-jaskoleczka.spk_emb.npy\n./nasza-czarna-jaskoleczka/nasza-czarna-jaskoleczka.subvads.pyd\n./nasza-czarna-jaskoleczka/nasza-czarna-jaskoleczka.vad.npy\n./puszkin-bajka-o-rybaku-i-rybce/puszkin-bajka-o-rybaku-i-rybce.spk_emb.npy\n./puszkin-bajka-o-rybaku-i-rybce/puszkin-bajka-o-rybaku-i-rybce.subvads.pyd\n./puszkin-bajka-o-rybaku-i-rybce/puszkin-bajka-o-rybaku-i-rybce.vad.npy\n./nowakowska-niska-rozdzielczosc-daj-mi/daj-mi.spk_emb.npy\n./nowakowska-niska-rozdzielczosc-daj-mi/daj-mi.subvads.pyd\n./nowakowska-niska-rozdzielczosc-daj-mi/daj-mi.vad.npy\n./slowka-zbior-replika-kobiety-polskiej/tadeusz-boy-zelenski-slowka-zbior-replika-kobiety-polskiej.spk_emb.npy\n./slowka-zbior-replika-kobiety-polskiej/tadeusz-boy-zelenski-slowka-zbior-replika-kobiety-polskiej.subvads.pyd\n./slowka-zbior-replika-kobiety-polskiej/tadeusz-boy-zelenski-slowka-zbior-replika-kobiety-polskiej.vad.npy\n./but-w-butonierce-ipecacuana/bruno-jasienski-but-w-butonierce-tomik-ipecacuana.spk_emb.npy\n./but-w-butonierce-ipecacuana/bruno-jasienski-but-w-butonierce-tomik-ipecacuana.subvads.pyd\n./but-w-butonierce-ipecacuana/bruno-jasienski-but-w-butonierce-tomik-ipecacuana.vad.npy\n./madry-i-glupi/ignacy-krasicki-bajki-i-przypowiesci-madry-i-glupi.spk_emb.npy\n./madry-i-glupi/ignacy-krasicki-bajki-i-przypowiesci-madry-i-glupi.subvads.pyd\n./madry-i-glupi/ignacy-krasicki-bajki-i-przypowiesci-madry-i-glupi.vad.npy\n./sonety-krymskie-bakczysaraj-w-nocy/adam-mickiewicz-sonety-krymskie-bakczysaraj-w-nocy.spk_emb.npy\n./sonety-krymskie-bakczysaraj-w-nocy/adam-mickiewicz-sonety-krymskie-bakczysaraj-w-nocy.subvads.pyd\n./sonety-krymskie-bakczysaraj-w-nocy/adam-mickiewicz-sonety-krymskie-bakczysaraj-w-nocy.vad.npy\n./sklepy-cynamonowe-traktat-o-manekinach-dokonczenie/sklepy-cynamonowe-traktat-o-manekinach-dokonczenie.spk_emb.npy\n./sklepy-cynamonowe-traktat-o-manekinach-dokonczenie/sklepy-cynamonowe-traktat-o-manekinach-dokonczenie.subvads.pyd\n./sklepy-cynamonowe-traktat-o-manekinach-dokonczenie/sklepy-cynamonowe-traktat-o-manekinach-dokonczenie.vad.npy\n./schulz-sanatorium-pod-klepsydra-druga-jesien/schulz-sanatorium-pod-klepsydra-druga-jesien_001_druga-jesien.spk_emb.npy\n./schulz-sanatorium-pod-klepsydra-druga-jesien/schulz-sanatorium-pod-klepsydra-druga-jesien_001_druga-jesien.subvads.pyd\n./schulz-sanatorium-pod-klepsydra-druga-jesien/schulz-sanatorium-pod-klepsydra-druga-jesien_001_druga-jesien.vad.npy\n./medrcy-swiata/autor-nieznany-medrcy-swiata.spk_emb.npy\n./medrcy-swiata/autor-nieznany-medrcy-swiata.subvads.pyd\n./medrcy-swiata/autor-nieznany-medrcy-swiata.vad.npy\n./aniol/aniol.spk_emb.npy\n./aniol/aniol.subvads.pyd\n./aniol/aniol.vad.npy\n./do-motyla/jan-andrzej-morsztyn-do-motyla.spk_emb.npy\n./do-motyla/jan-andrzej-morsztyn-do-motyla.subvads.pyd\n./do-motyla/jan-andrzej-morsztyn-do-motyla.vad.npy\n./napoj-cienisty-dokola-klombu/boleslaw-lesmian-napoj-cienisty-postacie-cykl-dokola-klombu.spk_emb.npy\n./napoj-cienisty-dokola-klombu/boleslaw-lesmian-napoj-cienisty-postacie-cykl-dokola-klombu.subvads.pyd\n./napoj-cienisty-dokola-klombu/boleslaw-lesmian-napoj-cienisty-postacie-cykl-dokola-klombu.vad.npy\n./dabrowska-boze-narodzenie/maria-dabrowska-boze-narodzenie.spk_emb.npy\n./dabrowska-boze-narodzenie/maria-dabrowska-boze-narodzenie.subvads.pyd\n./dabrowska-boze-narodzenie/maria-dabrowska-boze-narodzenie.vad.npy\n./spiewak-spod-strzechy/spiewak-spod-strzechy.spk_emb.npy\n./spiewak-spod-strzechy/spiewak-spod-strzechy.subvads.pyd\n./spiewak-spod-strzechy/spiewak-spod-strzechy.vad.npy\n./ziemia-i-potok-bajki-nowe/ziemia-i-potok-bajki-nowe.spk_emb.npy\n./ziemia-i-potok-bajki-nowe/ziemia-i-potok-bajki-nowe.subvads.pyd\n./ziemia-i-potok-bajki-nowe/ziemia-i-potok-bajki-nowe.vad.npy\n./sonety-krymskie-aluszta-w-dzien/adam-mickiewicz-sonety-krymskie-aluszta-w-dzien.spk_emb.npy\n./sonety-krymskie-aluszta-w-dzien/adam-mickiewicz-sonety-krymskie-aluszta-w-dzien.subvads.pyd\n./sonety-krymskie-aluszta-w-dzien/adam-mickiewicz-sonety-krymskie-aluszta-w-dzien.vad.npy\n./napoj-cienisty-wiosna/boleslaw-lesmian-napoj-cienisty-postacie-cykl-wiosna.spk_emb.npy\n./napoj-cienisty-wiosna/boleslaw-lesmian-napoj-cienisty-postacie-cykl-wiosna.subvads.pyd\n./napoj-cienisty-wiosna/boleslaw-lesmian-napoj-cienisty-postacie-cykl-wiosna.vad.npy\n./berenice/edgar-allan-poe-berenice.spk_emb.npy\n./berenice/edgar-allan-poe-berenice.subvads.pyd\n./berenice/edgar-allan-poe-berenice.vad.npy\n./chlop-i-jowisz-bajki-nowe/chlop-i-jowisz-bajki-nowe.spk_emb.npy\n./chlop-i-jowisz-bajki-nowe/chlop-i-jowisz-bajki-nowe.subvads.pyd\n./chlop-i-jowisz-bajki-nowe/chlop-i-jowisz-bajki-nowe.vad.npy\n./podrozny-i-kaleka/ignacy-krasicki-bajki-i-przypowiesci-podrozny-i-kaleka.spk_emb.npy\n./podrozny-i-kaleka/ignacy-krasicki-bajki-i-przypowiesci-podrozny-i-kaleka.subvads.pyd\n./podrozny-i-kaleka/ignacy-krasicki-bajki-i-przypowiesci-podrozny-i-kaleka.vad.npy\n./grabinski-ksiega-ognia-czerwona-magda/grabinski-ksiega-ognia-czerwona-magda.spk_emb.npy\n./grabinski-ksiega-ognia-czerwona-magda/grabinski-ksiega-ognia-czerwona-magda.subvads.pyd\n./grabinski-ksiega-ognia-czerwona-magda/grabinski-ksiega-ognia-czerwona-magda.vad.npy\n./wino-i-woda/ignacy-krasicki-bajki-i-przypowiesci-wino-i-woda.spk_emb.npy\n./wino-i-woda/ignacy-krasicki-bajki-i-przypowiesci-wino-i-woda.subvads.pyd\n./wino-i-woda/ignacy-krasicki-bajki-i-przypowiesci-wino-i-woda.vad.npy\n./grabinski-z-wyjatkow-w-pomrokach-wiary-klatwa/grabinski-z-wyjatkow-w-pomrokach-wiary-klatwa.spk_emb.npy\n./grabinski-z-wyjatkow-w-pomrokach-wiary-klatwa/grabinski-z-wyjatkow-w-pomrokach-wiary-klatwa.subvads.pyd\n./grabinski-z-wyjatkow-w-pomrokach-wiary-klatwa/grabinski-z-wyjatkow-w-pomrokach-wiary-klatwa.vad.npy\n./blumengraber-do-profesorow/blumengraber-do-profesorow.spk_emb.npy\n./blumengraber-do-profesorow/blumengraber-do-profesorow.subvads.pyd\n./blumengraber-do-profesorow/blumengraber-do-profesorow.vad.npy\n./wyzel-i-brytan-ii-bajki-nowe/wyzel-i-brytan-ii-bajki-nowe.spk_emb.npy\n./wyzel-i-brytan-ii-bajki-nowe/wyzel-i-brytan-ii-bajki-nowe.subvads.pyd\n./wyzel-i-brytan-ii-bajki-nowe/wyzel-i-brytan-ii-bajki-nowe.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_017_17-oto-jest-to-dziecko.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_017_17-oto-jest-to-dziecko.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_017_17-oto-jest-to-dziecko.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_006_6-kopalnia-diamentow.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_006_6-kopalnia-diamentow.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_006_6-kopalnia-diamentow.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_011_11-ram-dass.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_011_11-ram-dass.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_011_11-ram-dass.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_012_12-po-drugiej-stronie-sciany.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_012_12-po-drugiej-stronie-sciany.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_012_12-po-drugiej-stronie-sciany.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_004_4-lottie.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_004_4-lottie.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_004_4-lottie.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_008_8-na-poddaszu.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_008_8-na-poddaszu.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_008_8-na-poddaszu.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_015_15-czarnoksieznik.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_015_15-czarnoksieznik.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_015_15-czarnoksieznik.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_010_10-przybysz-z-indii.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_010_10-przybysz-z-indii.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_010_10-przybysz-z-indii.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_018_18-staralam-sie-byc-nia-zawsze.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_018_18-staralam-sie-byc-nia-zawsze.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_018_18-staralam-sie-byc-nia-zawsze.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_019_19-anna.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_019_19-anna.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_019_19-anna.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_003_3-ermenegarda.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_003_3-ermenegarda.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_003_3-ermenegarda.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_013_13-jedna-z-szarego-tlumu.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_013_13-jedna-z-szarego-tlumu.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_013_13-jedna-z-szarego-tlumu.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_014_14-co-slyszal-i-widzial-melchizedech.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_014_14-co-slyszal-i-widzial-melchizedech.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_014_14-co-slyszal-i-widzial-melchizedech.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_005_5-becky.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_005_5-becky.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_005_5-becky.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_009_9-melchizedech.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_009_9-melchizedech.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_009_9-melchizedech.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_016_16-odwiedziny.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_016_16-odwiedziny.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_016_16-odwiedziny.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_001_1-sara.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_001_1-sara.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_001_1-sara.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_002_2-lekcja-francuskiego.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_002_2-lekcja-francuskiego.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_002_2-lekcja-francuskiego.vad.npy\n./mala-ksiezniczka/mala-ksiezniczka_007_7-jeszcze-o-kopalniach-diamentow.spk_emb.npy\n./mala-ksiezniczka/mala-ksiezniczka_007_7-jeszcze-o-kopalniach-diamentow.subvads.pyd\n./mala-ksiezniczka/mala-ksiezniczka_007_7-jeszcze-o-kopalniach-diamentow.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_014_czesc-14.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_014_czesc-14.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_014_czesc-14.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_006_czesc-6.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_006_czesc-6.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_006_czesc-6.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_002_czesc-2.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_002_czesc-2.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_002_czesc-2.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_013_czesc-13.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_013_czesc-13.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_013_czesc-13.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_011_czesc-11.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_011_czesc-11.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_011_czesc-11.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_001_czesc-1.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_001_czesc-1.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_001_czesc-1.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_005_czesc-5.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_005_czesc-5.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_005_czesc-5.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_003_czesc-3.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_003_czesc-3.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_003_czesc-3.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_009_czesc-9.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_009_czesc-9.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_009_czesc-9.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_012_czesc-12.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_012_czesc-12.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_012_czesc-12.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_016_czesc-16.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_016_czesc-16.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_016_czesc-16.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_008_czesc-8.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_008_czesc-8.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_008_czesc-8.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_015_czesc-15.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_015_czesc-15.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_015_czesc-15.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_010_czesc-10.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_010_czesc-10.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_010_czesc-10.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_007_czesc-7.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_007_czesc-7.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_007_czesc-7.vad.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_004_czesc-4.spk_emb.npy\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_004_czesc-4.subvads.pyd\n./balzac-komedia-ludzka-eugenia-grandet/balzac-komedia-ludzka-eugenia-grandet_004_czesc-4.vad.npy\n./kornhauser-tyle-rzeczy-niezwyklych-tyle-rzeczy-niezwyklych/kornhauser-tyle-rzeczy-niezwyklych-tyle-rzeczy-niezwyklych_001.spk_emb.npy\n./kornhauser-tyle-rzeczy-niezwyklych-tyle-rzeczy-niezwyklych/kornhauser-tyle-rzeczy-niezwyklych-tyle-rzeczy-niezwyklych_001.subvads.pyd\n./kornhauser-tyle-rzeczy-niezwyklych-tyle-rzeczy-niezwyklych/kornhauser-tyle-rzeczy-niezwyklych-tyle-rzeczy-niezwyklych_001.vad.npy\n./slowka-zbior-dziadzio/tadeusz-boy-zelenski-slowka-zbior-dziadzio.spk_emb.npy\n./slowka-zbior-dziadzio/tadeusz-boy-zelenski-slowka-zbior-dziadzio.subvads.pyd\n./slowka-zbior-dziadzio/tadeusz-boy-zelenski-slowka-zbior-dziadzio.vad.npy\n./osiel-i-wol/ignacy-krasicki-bajki-i-przypowiesci-osiel-i-wol.spk_emb.npy\n./osiel-i-wol/ignacy-krasicki-bajki-i-przypowiesci-osiel-i-wol.subvads.pyd\n./osiel-i-wol/ignacy-krasicki-bajki-i-przypowiesci-osiel-i-wol.vad.npy\n./hop-frog/edgar-allan-poe-hop-frog.spk_emb.npy\n./hop-frog/edgar-allan-poe-hop-frog.subvads.pyd\n./hop-frog/edgar-allan-poe-hop-frog.vad.npy\n./slowka-zbior-ernestynka/tadeusz-boy-zelenski-slowka-zbior-ernestynka.spk_emb.npy\n./slowka-zbior-ernestynka/tadeusz-boy-zelenski-slowka-zbior-ernestynka.subvads.pyd\n./slowka-zbior-ernestynka/tadeusz-boy-zelenski-slowka-zbior-ernestynka.vad.npy\n./maska-smierci-szkarlatnej/edgar-allan-poe-maska-smierci-szkarlatnej.spk_emb.npy\n./maska-smierci-szkarlatnej/edgar-allan-poe-maska-smierci-szkarlatnej.subvads.pyd\n./maska-smierci-szkarlatnej/edgar-allan-poe-maska-smierci-szkarlatnej.vad.npy\n./tulli-sny-i-kamienie/sny-i-kamienie.spk_emb.npy\n./tulli-sny-i-kamienie/sny-i-kamienie.subvads.pyd\n./tulli-sny-i-kamienie/sny-i-kamienie.vad.npy\n./beresewicz-czy-pisarzom-czy-lubil-pan-chodzic-do-szkoly/beresewicz-czy-pisarzom-czy-lubil-pan-chodzic-do-szkoly_001.spk_emb.npy\n./beresewicz-czy-pisarzom-czy-lubil-pan-chodzic-do-szkoly/beresewicz-czy-pisarzom-czy-lubil-pan-chodzic-do-szkoly_001.subvads.pyd\n./beresewicz-czy-pisarzom-czy-lubil-pan-chodzic-do-szkoly/beresewicz-czy-pisarzom-czy-lubil-pan-chodzic-do-szkoly_001.vad.npy\n./skrucha-jozi/skrucha-jozi.spk_emb.npy\n./skrucha-jozi/skrucha-jozi.subvads.pyd\n./skrucha-jozi/skrucha-jozi.vad.npy\n./piesni-ksiegi-pierwsze-piesn-xx/jan-kochanowski-piesni-ksiegi-pierwsze-piesn-xx-milo-szalec-kiedy-czas-po-temu.spk_emb.npy\n./piesni-ksiegi-pierwsze-piesn-xx/jan-kochanowski-piesni-ksiegi-pierwsze-piesn-xx-milo-szalec-kiedy-czas-po-temu.subvads.pyd\n./piesni-ksiegi-pierwsze-piesn-xx/jan-kochanowski-piesni-ksiegi-pierwsze-piesn-xx-milo-szalec-kiedy-czas-po-temu.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_005_rozdzial-v-o-wierszach-wergilego.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_005_rozdzial-v-o-wierszach-wergilego.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_005_rozdzial-v-o-wierszach-wergilego.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_012_rozdzial-xii-o-fizjonomii.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_012_rozdzial-xii-o-fizjonomii.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_012_rozdzial-xii-o-fizjonomii.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_007_rozdzial-vii-o-ciezarach-wielkosci.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_007_rozdzial-vii-o-ciezarach-wielkosci.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_007_rozdzial-vii-o-ciezarach-wielkosci.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_001_rozdzial-i-o-pozytecznym-i-poczciwym.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_001_rozdzial-i-o-pozytecznym-i-poczciwym.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_001_rozdzial-i-o-pozytecznym-i-poczciwym.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_013_rozdzial-xiii-o-doswiadczeniu.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_013_rozdzial-xiii-o-doswiadczeniu.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_013_rozdzial-xiii-o-doswiadczeniu.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_006_rozdzial-vi-o-pojazdach.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_006_rozdzial-vi-o-pojazdach.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_006_rozdzial-vi-o-pojazdach.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_010_rozdzial-x-o-oszczedzaniu-woli.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_010_rozdzial-x-o-oszczedzaniu-woli.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_010_rozdzial-x-o-oszczedzaniu-woli.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_003_rozdzial-iii-o-trzech-rodzajach-obcowania.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_003_rozdzial-iii-o-trzech-rodzajach-obcowania.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_003_rozdzial-iii-o-trzech-rodzajach-obcowania.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_004_rozdzial-iv-o-dywersji.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_004_rozdzial-iv-o-dywersji.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_004_rozdzial-iv-o-dywersji.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_011_rozdzial-xi-o-kulawych.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_011_rozdzial-xi-o-kulawych.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_011_rozdzial-xi-o-kulawych.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_008_rozdzial-viii-o-sztuce-rozmawiania.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_008_rozdzial-viii-o-sztuce-rozmawiania.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_008_rozdzial-viii-o-sztuce-rozmawiania.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_009_rozdzial-ix-o-proznosci.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_009_rozdzial-ix-o-proznosci.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_009_rozdzial-ix-o-proznosci.vad.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_002_rozdzial-ii-o-zalu.spk_emb.npy\n./proby-ksiega-trzecia/proby-ksiega-trzecia_002_rozdzial-ii-o-zalu.subvads.pyd\n./proby-ksiega-trzecia/proby-ksiega-trzecia_002_rozdzial-ii-o-zalu.vad.npy\n./slonce-i-zaby-bajki-nowe/slonce-i-zaby-bajki-nowe.spk_emb.npy\n./slonce-i-zaby-bajki-nowe/slonce-i-zaby-bajki-nowe.subvads.pyd\n./slonce-i-zaby-bajki-nowe/slonce-i-zaby-bajki-nowe.vad.npy\n./slowka-zbior-nowa-wiara/tadeusz-boy-zelenski-slowka-zbior-nowa-wiara.spk_emb.npy\n./slowka-zbior-nowa-wiara/tadeusz-boy-zelenski-slowka-zbior-nowa-wiara.subvads.pyd\n./slowka-zbior-nowa-wiara/tadeusz-boy-zelenski-slowka-zbior-nowa-wiara.vad.npy\n./sonety-krymskie-bakczysaraj/adam-mickiewicz-sonety-krymskie-bakczysaraj.spk_emb.npy\n./sonety-krymskie-bakczysaraj/adam-mickiewicz-sonety-krymskie-bakczysaraj.subvads.pyd\n./sonety-krymskie-bakczysaraj/adam-mickiewicz-sonety-krymskie-bakczysaraj.vad.npy\n./wolny-hamkalo-nikt-nic/nikt-nic.spk_emb.npy\n./wolny-hamkalo-nikt-nic/nikt-nic.subvads.pyd\n./wolny-hamkalo-nikt-nic/nikt-nic.vad.npy\n./wabik-bajki-nowe/wabik-bajki-nowe.spk_emb.npy\n./wabik-bajki-nowe/wabik-bajki-nowe.subvads.pyd\n./wabik-bajki-nowe/wabik-bajki-nowe.vad.npy\n./wilk-i-baran-bajki-nowe/wilk-i-baran-bajki-nowe.spk_emb.npy\n./wilk-i-baran-bajki-nowe/wilk-i-baran-bajki-nowe.subvads.pyd\n./wilk-i-baran-bajki-nowe/wilk-i-baran-bajki-nowe.vad.npy\n./grabinski-ksiega-ognia-bialy-wyrak/grabinski-ksiega-ognia-bialy-wyrak.spk_emb.npy\n./grabinski-ksiega-ognia-bialy-wyrak/grabinski-ksiega-ognia-bialy-wyrak.subvads.pyd\n./grabinski-ksiega-ognia-bialy-wyrak/grabinski-ksiega-ognia-bialy-wyrak.vad.npy\n./konopnicka-w-polu/w-polu-pojdziemy-w-pole-w-ranny-czas.spk_emb.npy\n./konopnicka-w-polu/w-polu-pojdziemy-w-pole-w-ranny-czas.subvads.pyd\n./konopnicka-w-polu/w-polu-pojdziemy-w-pole-w-ranny-czas.vad.npy\n./lis-i-osiel/ignacy-krasicki-bajki-i-przypowiesci-lis-i-osiel.spk_emb.npy\n./lis-i-osiel/ignacy-krasicki-bajki-i-przypowiesci-lis-i-osiel.subvads.pyd\n./lis-i-osiel/ignacy-krasicki-bajki-i-przypowiesci-lis-i-osiel.vad.npy\n./do-delljusa/do-delljusa.spk_emb.npy\n./do-delljusa/do-delljusa.subvads.pyd\n./do-delljusa/do-delljusa.vad.npy\n./satyry-czesc-pierwsza-zona-modna/satyry-czesc-pierwsza-zona-modna.spk_emb.npy\n./satyry-czesc-pierwsza-zona-modna/satyry-czesc-pierwsza-zona-modna.subvads.pyd\n./satyry-czesc-pierwsza-zona-modna/satyry-czesc-pierwsza-zona-modna.vad.npy\n./janicki-i-nas-wybawi/i-nas-wybawi.spk_emb.npy\n./janicki-i-nas-wybawi/i-nas-wybawi.subvads.pyd\n./janicki-i-nas-wybawi/i-nas-wybawi.vad.npy\n./napoj-cienisty-cmentarz/boleslaw-lesmian-napoj-cienisty-w-nicosc-sniaca-sie-droga-cykl-cmentarz.spk_emb.npy\n./napoj-cienisty-cmentarz/boleslaw-lesmian-napoj-cienisty-w-nicosc-sniaca-sie-droga-cykl-cmentarz.subvads.pyd\n./napoj-cienisty-cmentarz/boleslaw-lesmian-napoj-cienisty-w-nicosc-sniaca-sie-droga-cykl-cmentarz.vad.npy\n./przyjaciel/ignacy-krasicki-bajki-i-przypowiesci-przyjaciel.spk_emb.npy\n./przyjaciel/ignacy-krasicki-bajki-i-przypowiesci-przyjaciel.subvads.pyd\n./przyjaciel/ignacy-krasicki-bajki-i-przypowiesci-przyjaciel.vad.npy\n./but-w-butonierce-jak-introdukcja/bruno-jasienski-but-w-butonierce-tomik-jak-introdukcja.spk_emb.npy\n./but-w-butonierce-jak-introdukcja/bruno-jasienski-but-w-butonierce-tomik-jak-introdukcja.subvads.pyd\n./but-w-butonierce-jak-introdukcja/bruno-jasienski-but-w-butonierce-tomik-jak-introdukcja.vad.npy\n./czlowiek-i-zwierciadla/ignacy-krasicki-bajki-i-przypowiesci-czlowiek-i-zwierciadla.spk_emb.npy\n./czlowiek-i-zwierciadla/ignacy-krasicki-bajki-i-przypowiesci-czlowiek-i-zwierciadla.subvads.pyd\n./czlowiek-i-zwierciadla/ignacy-krasicki-bajki-i-przypowiesci-czlowiek-i-zwierciadla.vad.npy\n./dobroczynnosc/ignacy-krasicki-bajki-i-przypowiesci-dobroczynnosc.spk_emb.npy\n./dobroczynnosc/ignacy-krasicki-bajki-i-przypowiesci-dobroczynnosc.subvads.pyd\n./dobroczynnosc/ignacy-krasicki-bajki-i-przypowiesci-dobroczynnosc.vad.npy\n./krol-i-pisarze/ignacy-krasicki-bajki-i-przypowiesci-krol-i-pisarze.spk_emb.npy\n./krol-i-pisarze/ignacy-krasicki-bajki-i-przypowiesci-krol-i-pisarze.subvads.pyd\n./krol-i-pisarze/ignacy-krasicki-bajki-i-przypowiesci-krol-i-pisarze.vad.npy\n./grzegorzewska-wszystkie-wieczory-swiata/wszystkie-wieczory-swiata.spk_emb.npy\n./grzegorzewska-wszystkie-wieczory-swiata/wszystkie-wieczory-swiata.subvads.pyd\n./grzegorzewska-wszystkie-wieczory-swiata/wszystkie-wieczory-swiata.vad.npy\n./do-leukonoe/do-leukonoe.spk_emb.npy\n./do-leukonoe/do-leukonoe.subvads.pyd\n./do-leukonoe/do-leukonoe.vad.npy\n./slonecznik-i-fialek-bajki-nowe/slonecznik-i-fialek-bajki-nowe.spk_emb.npy\n./slonecznik-i-fialek-bajki-nowe/slonecznik-i-fialek-bajki-nowe.subvads.pyd\n./slonecznik-i-fialek-bajki-nowe/slonecznik-i-fialek-bajki-nowe.vad.npy\n./slowka-zbior-odsiecz-wiednia/tadeusz-boy-zelenski-slowka-zbior-odsiecz-wiednia.spk_emb.npy\n./slowka-zbior-odsiecz-wiednia/tadeusz-boy-zelenski-slowka-zbior-odsiecz-wiednia.subvads.pyd\n./slowka-zbior-odsiecz-wiednia/tadeusz-boy-zelenski-slowka-zbior-odsiecz-wiednia.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_022_rozdzial-22.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_022_rozdzial-22.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_022_rozdzial-22.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_010_rozdzial-10.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_010_rozdzial-10.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_010_rozdzial-10.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_027_rozdzial-27.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_027_rozdzial-27.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_027_rozdzial-27.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_001_rozdzial-1.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_001_rozdzial-1.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_001_rozdzial-1.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_025_rozdzial-25.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_025_rozdzial-25.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_025_rozdzial-25.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_020_rozdzial-20.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_020_rozdzial-20.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_020_rozdzial-20.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_003_rozdzial-3.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_003_rozdzial-3.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_003_rozdzial-3.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_011_rozdzial-11.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_011_rozdzial-11.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_011_rozdzial-11.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_007_rozdzial-7.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_007_rozdzial-7.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_007_rozdzial-7.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_019_rozdzial-19.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_019_rozdzial-19.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_019_rozdzial-19.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_018_rozdzial-18.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_018_rozdzial-18.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_018_rozdzial-18.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_013_rozdzial-13.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_013_rozdzial-13.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_013_rozdzial-13.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_016_rozdzial-16.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_016_rozdzial-16.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_016_rozdzial-16.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_008_rozdzial-8.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_008_rozdzial-8.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_008_rozdzial-8.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_023_rozdzial-23.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_023_rozdzial-23.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_023_rozdzial-23.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_021_rozdzial-21.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_021_rozdzial-21.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_021_rozdzial-21.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_002_rozdzial-2.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_002_rozdzial-2.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_002_rozdzial-2.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_014_rozdzial-14.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_014_rozdzial-14.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_014_rozdzial-14.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_024_rozdzial-24.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_024_rozdzial-24.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_024_rozdzial-24.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_009_rozdzial-9.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_009_rozdzial-9.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_009_rozdzial-9.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_017_rozdzial-17.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_017_rozdzial-17.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_017_rozdzial-17.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_005_rozdzial-5.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_005_rozdzial-5.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_005_rozdzial-5.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_012_rozdzial-12.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_012_rozdzial-12.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_012_rozdzial-12.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_028_rozdzial-28.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_028_rozdzial-28.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_028_rozdzial-28.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_006_rozdzial-6.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_006_rozdzial-6.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_006_rozdzial-6.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_026_rozdzial-26.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_026_rozdzial-26.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_026_rozdzial-26.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_015_rozdzial-15.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_015_rozdzial-15.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_015_rozdzial-15.vad.npy\n./arystoteles-poetyka/arystoteles-poetyka_004_rozdzial-4.spk_emb.npy\n./arystoteles-poetyka/arystoteles-poetyka_004_rozdzial-4.subvads.pyd\n./arystoteles-poetyka/arystoteles-poetyka_004_rozdzial-4.vad.npy\n./slowka-zbior-piosenki-zb-piosenka-w-stylu-klasycznym/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-piosenka-w-stylu-klasycznym.spk_emb.npy\n./slowka-zbior-piosenki-zb-piosenka-w-stylu-klasycznym/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-piosenka-w-stylu-klasycznym.subvads.pyd\n./slowka-zbior-piosenki-zb-piosenka-w-stylu-klasycznym/tadeusz-boy-zelenski-slowka-zbior-piosenki-zielonego-balonika-piosenka-w-stylu-klasycznym.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_010_w-noska.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_010_w-noska.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_010_w-noska.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_007_bijatyka.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_007_bijatyka.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_007_bijatyka.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_004_rozstanie.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_004_rozstanie.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_004_rozstanie.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_009_list.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_009_list.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_009_list.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_011_pimpus-smialek.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_011_pimpus-smialek.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_011_pimpus-smialek.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_012_postanowienie.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_012_postanowienie.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_012_postanowienie.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_005_marsz-z-kuchni.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_005_marsz-z-kuchni.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_005_marsz-z-kuchni.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_003_lekcja-tanca.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_003_lekcja-tanca.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_003_lekcja-tanca.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_001_szkola.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_001_szkola.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_001_szkola.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_008_pimpus-buty-czysci.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_008_pimpus-buty-czysci.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_008_pimpus-buty-czysci.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_002_szczescie-rodzinne.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_002_szczescie-rodzinne.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_002_szczescie-rodzinne.vad.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_006_katastrofa.spk_emb.npy\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_006_katastrofa.subvads.pyd\n./konopnicka-szkolne-przygody-pimpusia-sadelko/konopnicka-szkolne-przygody-pimpusia-sadelko_006_katastrofa.vad.npy\n./but-w-butonierce-przejechali/bruno-jasienski-but-w-butonierce-tomik-przejechali.spk_emb.npy\n./but-w-butonierce-przejechali/bruno-jasienski-but-w-butonierce-tomik-przejechali.subvads.pyd\n./but-w-butonierce-przejechali/bruno-jasienski-but-w-butonierce-tomik-przejechali.vad.npy\n./fiedorczuk-kazdy-snil-swoj-sen/fiedorczuk-kazdy-snil-swoj-sen.spk_emb.npy\n./fiedorczuk-kazdy-snil-swoj-sen/fiedorczuk-kazdy-snil-swoj-sen.subvads.pyd\n./fiedorczuk-kazdy-snil-swoj-sen/fiedorczuk-kazdy-snil-swoj-sen.vad.npy\n\n\n\nds = chunked_audio_dataset(['../wolnelektury-wds2/wolnelektury-audio-000000.tar'])\nprev = None\nfor s in progress_bar(ds, total=6):\n    sim = F.cosine_similarity(torch.tensor(s['spk_emb.npy']), torch.tensor((prev if prev is not None else s)['spk_emb.npy']), dim=0)\n    if sim &lt; 0.5: print(\"new\")\n    print(s['__key__'], sim, s['tend'] - s['tstart'], sum([e-s for s,e in s['orig_s']['subvads.pyd'][s['i']]]))\n    display(IPython.display.Audio(s['samples'], rate=s['sample_rate']))\n    time.sleep(.5)\n    prev = s"
  },
  {
    "objectID": "D. Common inference utilities.html",
    "href": "D. Common inference utilities.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\nget_compute_device\n\n get_compute_device ()"
  },
  {
    "objectID": "7. Pipeline.html",
    "href": "7. Pipeline.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "source\n\nPipeline\n\n Pipeline (t2s_ref=None, s2a_ref=None, optimize=True, torch_compile=False,\n           device=None)\n\nInitialize self. See help(type(self)) for accurate signature."
  },
  {
    "objectID": "3c. s2a acoustic tokens preparation.html",
    "href": "3c. s2a acoustic tokens preparation.html",
    "title": "S2A dataset preparation",
    "section": "",
    "text": "Automatic pdb calling has been turned ON\n\n\n\nprepare_atoks('../wolnelektury-wds2/wolnelektury-audio-000000.tar', n_samples=1024, batch_size=8)\n\n/opt/conda/lib/python3.10/site-packages/torch/nn/utils/weight_norm.py:30: UserWarning: torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\n  warnings.warn(\"torch.nn.utils.weight_norm is deprecated in favor of torch.nn.utils.parametrizations.weight_norm.\")\n\n\nBenchmarking run of 1024 samples (128 batches)\n\n\n\n\n\n\n\n    \n      \n      100.00% [128/128 00:22&lt;00:00]\n    \n    \n\n\n\nprepare_atoks('../wolnelektury-wds2/wolnelektury-audio-000000.tar', n_samples=1024, batch_size=4)\n\nBenchmarking run of 1024 samples (256 batches)\n\n\n\n\n\n\n\n    \n      \n      100.00% [256/256 00:23&lt;00:00]\n    \n    \n\n\n\nprepare_atoks('../wolnelektury-wds2/wolnelektury-audio-000000.tar', batch_size=4)\n\n\n\n\n\n\n    \n      \n      100.00% [2769/2769 04:09&lt;00:00]\n    \n    \n\n\n\nprepare_atoks('../wolnelektury-wds2/wolnelektury-audio-000000.tar', n_samples=1024, batch_size=2)\n\nBenchmarking run of 1024 samples (512 batches)\n\n\n\n\n\n\n\n    \n      \n      100.00% [512/512 00:31&lt;00:00]"
  },
  {
    "objectID": "3b. semantic token extraction.html",
    "href": "3b. semantic token extraction.html",
    "title": "Semantic token extraction",
    "section": "",
    "text": "vq_model = vq_stoks.RQBottleneckTransformer.load_model(\"vqmodel-medium-en+pl-512c-dim64.model\").cuda()\n\n\nvq_model.ensure_whisper('cuda')\n\n\nvq_model.whmodel[0].encoder\n\nAudioEncoder(\n  (conv1): Conv1d(80, 1024, kernel_size=(3,), stride=(1,), padding=(1,))\n  (conv2): Conv1d(1024, 1024, kernel_size=(3,), stride=(2,), padding=(1,))\n  (blocks): ModuleList(\n    (0-23): 24 x ResidualAttentionBlock(\n      (attn): MultiHeadAttention(\n        (query): Linear(in_features=1024, out_features=1024, bias=True)\n        (key): Linear(in_features=1024, out_features=1024, bias=False)\n        (value): Linear(in_features=1024, out_features=1024, bias=True)\n        (out): Linear(in_features=1024, out_features=1024, bias=True)\n      )\n      (attn_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (mlp): Sequential(\n        (0): Linear(in_features=1024, out_features=4096, bias=True)\n        (1): GELU(approximate='none')\n        (2): Linear(in_features=4096, out_features=1024, bias=True)\n      )\n      (mlp_ln): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n    )\n  )\n  (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n)\n\n\n\n\n\nAutomatic pdb calling has been turned ON\n\n\n\nprepare_stoks('../wolnelektury-wds2/wolnelektury-audio-000000.tar', n_samples=1024, batch_size=16)\n\n\n\n\n\n\n    \n      \n      100.00% [64/64 00:23&lt;00:00]\n    \n    \n\n\n\nprepare_stoks('../wolnelektury-wds2/wolnelektury-audio-000000.tar', n_samples=1024, batch_size=16)\n\n\n\n\n\n\n    \n      \n      100.00% [64/64 00:21&lt;00:00]\n    \n    \n\n\n\nprepare_stoks('../wolnelektury-wds2/wolnelektury-audio-000000.tar', n_samples=1024, batch_size=32)\n\n\n\n\n\n\n    \n      \n      100.00% [32/32 00:21&lt;00:00]\n    \n    \n\n\n\nprepare_stoks('../wolnelektury-wds2/wolnelektury-audio-000000.tar', n_samples=1024, batch_size=64)\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:20&lt;00:00]\n    \n    \n\n\n\nprepare_stoks('../wolnelektury-wds2/wolnelektury-audio-000000.tar', n_samples=1024, batch_size=64)\n\n\n\n\n\n\n    \n      \n      100.00% [16/16 00:23&lt;00:00]\n    \n    \n\n\n\n!ls -lh ../wolnelektury-wds2/wolnelektury-maxvad-stoks-000000.tar\n!tar -tf ../wolnelektury-wds2/wolnelektury-maxvad-stoks-000000.tar"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "If you have questions or you want to help you can find us in the #audio-generation channel on the LAION Discord server.\nAn Open Source text-to-speech system built by inverting Whisper. Previously known as spear-tts-pytorch.\nWe want this model to be like Stable Diffusion but for speech – both powerful and easily customizable.\nWe are working only with properly licensed speech recordings and all the code is Open Source so the model will be always safe to use for commercial applications.\nCurrently the models are trained on the English LibreLight dataset. In the next release we want to target multiple languages (Whisper and EnCodec are both multilanguage).\nSample of the synthesized voice:\nhttps://github.com/collabora/WhisperSpeech/assets/107984/aa5a1e7e-dc94-481f-8863-b022c7fd7434",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#progress-update-2024-01-29",
    "href": "index.html#progress-update-2024-01-29",
    "title": "WhisperSpeech",
    "section": "Progress update [2024-01-29]",
    "text": "Progress update [2024-01-29]\nWe successfully trained a tiny S2A model on an en+pl+fr dataset and it can do voice cloning in French:\nhttps://github.com/collabora/WhisperSpeech/assets/107984/267f2602-7eec-4646-a43b-059ff91b574e\nhttps://github.com/collabora/WhisperSpeech/assets/107984/fbf08e8e-0f9a-4b0d-ab5e-747ffba2ccb9\nWe were able to do this with frozen semantic tokens that were only trained on English and Polish. This supports the idea that we will be able to train a single semantic token model to support all the languages in the world. Quite likely even ones that are not currently well supported by the Whisper model. Stay tuned for more updates on this front. :)",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#progress-update-2024-01-18",
    "href": "index.html#progress-update-2024-01-18",
    "title": "WhisperSpeech",
    "section": "Progress update [2024-01-18]",
    "text": "Progress update [2024-01-18]\nWe spend the last week optimizing inference performance. We integrated torch.compile, added kv-caching and tuned some of the layers – we are now working over 12x faster than real-time on a consumer 4090!\nWe can mix languages in a single sentence (here the highlighted English project names are seamlessly mixed into Polish speech):\n\nTo jest pierwszy test wielojęzycznego Whisper Speech modelu zamieniającego tekst na mowę, który Collabora i Laion nauczyli na superkomputerze Jewels.\n\nhttps://github.com/collabora/WhisperSpeech/assets/107984/d7092ef1-9df7-40e3-a07e-fdc7a090ae9e\nWe also added an easy way to test voice-cloning. Here is a sample voice cloned from a famous speech by Winston Churchill (the radio static is a feature, not a bug ;) – it is part of the reference recording):\nhttps://github.com/collabora/WhisperSpeech/assets/107984/bd28110b-31fb-4d61-83f6-c997f560bc26\nYou can test all of these on Colab (we optimized the dependencies so now it takes less than 30 seconds to install). A Huggingface Space is coming soon.",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#progress-update-2024-01-10",
    "href": "index.html#progress-update-2024-01-10",
    "title": "WhisperSpeech",
    "section": "Progress update [2024-01-10]",
    "text": "Progress update [2024-01-10]\nWe’ve pushed a new SD S2A model that is a lot faster while still generating high-quality speech. We’ve also added an example of voice cloning based on a reference audio file.\nAs always, you can check out our Colab to try it yourself!",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#progress-update-2023-12-10",
    "href": "index.html#progress-update-2023-12-10",
    "title": "WhisperSpeech",
    "section": "Progress update [2023-12-10]",
    "text": "Progress update [2023-12-10]\nAnother trio of models, this time they support multiple languages (English and Polish). Here are two new samples for a sneak peek. You can check out our Colab to try it yourself!\nEnglish speech, female voice (transferred from a Polish language dataset):\nhttps://github.com/collabora/WhisperSpeech/assets/107984/aa5a1e7e-dc94-481f-8863-b022c7fd7434\nA Polish sample, male voice:\nhttps://github.com/collabora/WhisperSpeech/assets/107984/4da14b03-33f9-4e2d-be42-f0fcf1d4a6ec\nOlder progress updates are archived here",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#downloads",
    "href": "index.html#downloads",
    "title": "WhisperSpeech",
    "section": "Downloads",
    "text": "Downloads\nWe encourage you to start with the Google Colab link above or run the provided notebook locally. If you want to download manually or train the models from scratch then both the WhisperSpeech pre-trained models as well as the converted datasets are available on HuggingFace.",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#roadmap",
    "href": "index.html#roadmap",
    "title": "WhisperSpeech",
    "section": "Roadmap",
    "text": "Roadmap\n\nGather a bigger emotive speech dataset\nFigure out a way to condition the generation on emotions and prosody\nCreate a community effort to gather freely licensed speech in multiple languages\nTrain final multi-language models",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#architecture",
    "href": "index.html#architecture",
    "title": "WhisperSpeech",
    "section": "Architecture",
    "text": "Architecture\nThe general architecture is similar to AudioLM, SPEAR TTS from Google and MusicGen from Meta. We avoided the NIH syndrome and built it on top of powerful Open Source models: Whisper from OpenAI to generate semantic tokens and perform transcription, EnCodec from Meta for acoustic modeling and Vocos from Charactr Inc as the high-quality vocoder.\nWe gave two presentation diving deeper into WhisperSpeech. The first one talks about the challenges of large scale training:\n\n\n\nTricks Learned from Scaling WhisperSpeech Models to 80k+ Hours of Speech - video recording by Jakub Cłapa, Collabora\n\n\nThe other one goes a bit more into the architectural choices we made:\n\n\n\nOpen Source Text-To-Speech Projects: WhisperSpeech - In Depth Discussion\n\n\n\nWhisper for modeling semantic tokens\nWe utilize the OpenAI Whisper encoder block to generate embeddings which we then quantize to get semantic tokens.\nIf the language is already supported by Whisper then this process requires only audio files (without ground truth transcriptions).\n\n\n\nUsing Whisper for semantic token extraction diagram",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#encodec-for-modeling-acoustic-tokens",
    "href": "index.html#encodec-for-modeling-acoustic-tokens",
    "title": "WhisperSpeech",
    "section": "EnCodec for modeling acoustic tokens",
    "text": "EnCodec for modeling acoustic tokens\nWe use EnCodec to model the audio waveform. Out of the box it delivers reasonable quality at 1.5kbps and we can bring this to high-quality by using Vocos – a vocoder pretrained on EnCodec tokens.\n\n\n\nEnCodec block diagram",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#appreciation",
    "href": "index.html#appreciation",
    "title": "WhisperSpeech",
    "section": "Appreciation",
    "text": "Appreciation\n      \nThis work would not be possible without the generous sponsorships from:\n\nCollabora – code development and model training\nLAION – community building and datasets (special thanks to\nJülich Supercomputing Centre - JUWELS Booster supercomputer\n\nWe gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) for funding part of this work by providing computing time through the John von Neumann Institute for Computing (NIC) on the GCS Supercomputer JUWELS Booster at Jülich Supercomputing Centre (JSC), with access to compute provided via LAION cooperation on foundation models research.\nWe’d like to also thank individual contributors for their great help in building this model:\n\ninevitable-2031 (qwerty_qwer on Discord) for dataset curation",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#consulting",
    "href": "index.html#consulting",
    "title": "WhisperSpeech",
    "section": "Consulting",
    "text": "Consulting\nWe are available to help you with both Open Source and proprietary AI projects. You can reach us via the Collabora website or on Discord ( and )",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "index.html#citations",
    "href": "index.html#citations",
    "title": "WhisperSpeech",
    "section": "Citations",
    "text": "Citations\nWe rely on many amazing Open Source projects and research papers:\n@article{SpearTTS,\n  title = {Speak, Read and Prompt: High-Fidelity Text-to-Speech with Minimal Supervision},\n  url = {https://arxiv.org/abs/2302.03540},\n  author = {Kharitonov, Eugene and Vincent, Damien and Borsos, Zalán and Marinier, Raphaël and Girgin, Sertan and Pietquin, Olivier and Sharifi, Matt and Tagliasacchi, Marco and Zeghidour, Neil},\n  publisher = {arXiv},\n  year = {2023},\n}\n@article{MusicGen,\n  title={Simple and Controllable Music Generation}, \n  url = {https://arxiv.org/abs/2306.05284},\n  author={Jade Copet and Felix Kreuk and Itai Gat and Tal Remez and David Kant and Gabriel Synnaeve and Yossi Adi and Alexandre Défossez},\n  publisher={arXiv},\n  year={2023},\n}\n@article{Whisper\n  title = {Robust Speech Recognition via Large-Scale Weak Supervision},\n  url = {https://arxiv.org/abs/2212.04356},\n  author = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},\n  publisher = {arXiv},\n  year = {2022},\n}\n@article{EnCodec\n  title = {High Fidelity Neural Audio Compression},\n  url = {https://arxiv.org/abs/2210.13438},\n  author = {Défossez, Alexandre and Copet, Jade and Synnaeve, Gabriel and Adi, Yossi},\n  publisher = {arXiv},\n  year = {2022},\n}\n@article{Vocos\n  title={Vocos: Closing the gap between time-domain and Fourier-based neural vocoders for high-quality audio synthesis}, \n  url = {https://arxiv.org/abs/2306.00814},\n  author={Hubert Siuzdak},\n  publisher={arXiv},\n  year={2023},\n}",
    "crumbs": [
      "WhisperSpeech"
    ]
  },
  {
    "objectID": "5b. multi-lang text to semantic token modeling.html",
    "href": "5b. multi-lang text to semantic token modeling.html",
    "title": "Text to semantic tokens model",
    "section": "",
    "text": "from whisperspeech.wer_metrics import *\nfrom whisperspeech.train import *\n\nfrom fastprogress import master_bar\nimport webdataset as wds\n\n\nDataset\n\nsource\n\nload_dataset\n\n load_dataset (txt_shard_spec:str, stoks_shard_dir:str, samples:int,\n               txt_kind:str='small.en-txt', vq_codes:int=4096,\n               language:str='en', weight:float=1, validation:bool=False,\n               exclude_files:str=None, cwd:pathlib.Path=None)\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ntxt_shard_spec\nstr\n\ntranscription webdataset shards\n\n\nstoks_shard_dir\nstr\n\nstoks webdataset base dir\n\n\nsamples\nint\n\nsamples per epoch\n\n\ntxt_kind\nstr\nsmall.en-txt\n\n\n\nvq_codes\nint\n4096\n\n\n\nlanguage\nstr\nen\n\n\n\nweight\nfloat\n1\n\n\n\nvalidation\nbool\nFalse\n\n\n\nexclude_files\nstr\nNone\n\n\n\ncwd\nPath\nNone\n\n\n\n\n\n\n\nAutomatic pdb calling has been turned ON\n\n\n\ntrain_ds = load_dataset('../wolnelektury-wds2/wolnelektury-medium-txt-*.tar.gz', '../wolnelektury-vqv2/', 190000,\n                        txt_kind='medium-txt', vq_codes=513, language='pl',\n                        exclude_files='../wolnelektury-wds2/validation-samples')\nval_ds = load_dataset('../wolnelektury-wds2/validation-eqvad.tar.gz', '../wolnelektury-vqv2/', 520,\n                      txt_kind='medium-txt', vq_codes=513, language='pl', validation=True)\n\n\nfor x in progress_bar(train_ds, total=100): pass\nx\n\n\n\n\n\n\n    \n      \n      100.00% [100/100 00:06&lt;00:00]\n    \n    \n\n\n[tensor([[  0,  80, 114,  ...,   0,   0,   0],\n         [  0,  74,  97,  ...,   0,   0,   0],\n         [  0,  80, 114,  ...,   0,   0,   0],\n         ...,\n         [  0,  90,  32,  ...,   0,   0,   0],\n         [  0,  78, 105,  ...,   0,   0,   0],\n         [  0,  74,  97,  ...,   0,   0,   0]]),\n tensor([[ 80, 114, 111,  ...,   0,   0,   0],\n         [ 74,  97,  99,  ...,   0,   0,   0],\n         [ 80, 114, 111,  ...,   0,   0,   0],\n         ...,\n         [ 90,  32, 100,  ...,   0,   0,   0],\n         [ 78, 105, 101,  ...,   0,   0,   0],\n         [ 74,  97,  32,  ...,   0,   0,   0]]),\n array([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n        10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10]),\n array([16.87227866, 13.26666667, 10.44474394, 12.30366492, 17.85714286,\n        12.91291291, 17.57731959, 12.59044863, 13.08701657,  9.05923345,\n         6.64893617, 16.04938272, 13.57664234, 16.6958042 , 12.89986092,\n        12.30385164, 13.0044843 , 11.58280922,  6.55940594, 14.94444444,\n        14.01639344, 11.34085213, 14.24632353, 13.95348837, 13.08219178,\n        14.08382066, 17.42424242, 13.91006098, 12.85425101, 14.37296417,\n        13.3640553 , 12.09103841, 12.54098361, 11.59711075, 14.07380608,\n        13.40388007, 14.59537572, 11.70212766, 12.1559633 , 14.36781609,\n        13.86138614, 12.27272727, 14.36915888, 13.57388316, 12.84059946,\n        13.21478382, 11.01123596, 15.40041068, 14.14473684, 10.51401869,\n        11.55172414, 14.90990991, 16.0130719 , 12.80959752, 14.18511066,\n         6.04448743, 11.36      , 15.35087719, 15.41155867, 14.49880668,\n        12.47892074, 12.34375   , 14.04612159, 16.55629139]),\n tensor([[512, 460,  66,  ..., 512, 512, 512],\n         [512, 336, 452,  ..., 116, 116, 116],\n         [512,  66, 309,  ..., 512, 512, 512],\n         ...,\n         [512, 336, 253,  ..., 512, 512, 512],\n         [512, 336, 141,  ..., 512, 512, 512],\n         [512, 336, 261,  ..., 512, 512, 512]]),\n tensor([[460,  66, 337,  ..., 512, 512, 512],\n         [336, 452, 417,  ..., 116, 116, 460],\n         [ 66, 309,  58,  ..., 512, 512, 512],\n         ...,\n         [336, 253, 253,  ..., 512, 512, 512],\n         [336, 141, 248,  ..., 512, 512, 512],\n         [336, 261, 197,  ..., 512, 512, 512]])]\n\n\n\n\n\nModeling\n\nsource\n\nTunables\n\n Tunables (init_std:float=1, embeddings_std:float=0.01,\n           embeddings_lr_scale:float=5,\n           embedding_projector_lr_scale:float=2.5, output_mult:float=0.35,\n           query_mult:float=1, encoder_depth_ratio:float=0.25,\n           causal_encoder:bool=True, eot_dropout_p:float=0.5,\n           cps_input:bool=True, cps_bins:int=32, lr0:float=0.0015,\n           clip_gradient_norm:float=0.2, weight_decay:float=0.1,\n           warmup_steps:float=4000, random:bool=False)\n\n\nsource\n\n\nrand\n\n rand (start, end)\n\n\nsource\n\n\nT2SEmbedding\n\n T2SEmbedding (length=1500, codes=1024, width=384, pos_embs=None,\n               stoks_width=384)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nEncoder\n\n Encoder (depth=6, width=384, n_head=6, length=1500, codes=1024,\n          emb_width=384, ffn_mult=4, pos_embs=None,\n          tunables=Tunables(init_std=1, embeddings_std=0.01,\n          embeddings_lr_scale=5, embedding_projector_lr_scale=2.5,\n          output_mult=0.35, query_mult=1, encoder_depth_ratio=0.25,\n          causal_encoder=True, eot_dropout_p=0.5, cps_input=True,\n          cps_bins=32, lr0=0.0015, clip_gradient_norm=0.2,\n          weight_decay=0.1, warmup_steps=4000, random=False))\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nTSARTransformer\n\n TSARTransformer (depth=6, n_head=6, head_width=64, ffn_mult=4,\n                  ttoks_len=200, ttoks_codes=256, ttoks_width=None,\n                  stoks_len=1500, stoks_codes=1024, stoks_width=None,\n                  tunables=Tunables(init_std=1, embeddings_std=0.01,\n                  embeddings_lr_scale=5, embedding_projector_lr_scale=2.5,\n                  output_mult=0.35, query_mult=1,\n                  encoder_depth_ratio=0.25, causal_encoder=True,\n                  eot_dropout_p=0.5, cps_input=True, cps_bins=32,\n                  lr0=0.0015, clip_gradient_norm=0.2, weight_decay=0.1,\n                  warmup_steps=4000, random=False))\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\nsource\n\n\nmake_model\n\n make_model (size:str, frozen_embeddings_model:str=None,\n             tunables:__main__.Tunables=Tunables(init_std=1,\n             embeddings_std=0.01, embeddings_lr_scale=5,\n             embedding_projector_lr_scale=2.5, output_mult=0.35,\n             query_mult=1, encoder_depth_ratio=0.25, causal_encoder=True,\n             eot_dropout_p=0.5, cps_input=True, cps_bins=32, lr0=0.0015,\n             clip_gradient_norm=0.2, weight_decay=0.1, warmup_steps=4000,\n             random=False), dataset:torch.utils.data.dataset.Dataset=None)\n\n\n# baseline\nmodel = make_model('micro', dataset=train_ds, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model',\n                   tunables=Tunables()).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=4,\n      warmup_steps=model.tunables.warmup_steps, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 07:59&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.62064\n2.48393\n01:06\n\n\n200000\n1.79376\n1.77248\n02:11\n\n\n300000\n1.69666\n1.66202\n03:11\n\n\n400000\n1.73755\n1.60740\n04:17\n\n\n500000\n1.75108\n1.56827\n05:17\n\n\n600000\n1.59873\n1.53394\n06:23\n\n\n700000\n1.50289\n1.49515\n07:23\n\n\n759936\n1.52261\n1.47473\n08:00\n\n\n\n\n\n    \n      \n      100.00% [5937/5937 02:00&lt;00:00 #189984/189984 loss: 1.523 / 1.475]\n    \n    \n\n\n\n\n\n\n\n\n\n\n\nmodel.save_model('t2s-micro.model')\n\n\n# no encoder LM loss, trains visibly slower\nmodel = make_model('micro', dataset=train_ds, frozen_embeddings_model='vqmodel-medium-en+pl-512c-dim64.model',\n                   tunables=Tunables(causal_encoder=False)).cuda()\ntrain(\"tsar-wx\", model, train_ds, val_ds, half=True, bs=32, lr=model.tunables.lr0, epochs=4,\n      warmup_steps=1500, weight_decay=model.tunables.weight_decay, clip_gradient_norm=model.tunables.clip_gradient_norm,\n      table_row_every_iters=100000, run_valid_every_iters=10000)\n\n\n\n\n\n\n\n\n\n\n    \n      \n      100.00% [4/4 07:57&lt;00:00]\n    \n    \n\n\n\n\nsamples\ntrain\nval\ntime\n\n\n\n\n100000\n2.44452\n2.38181\n01:04\n\n\n200000\n2.33279\n2.19010\n02:11\n\n\n300000\n1.83019\n1.82918\n03:12\n\n\n400000\n1.74988\n1.73074\n04:16\n\n\n500000\n1.58686\n1.67560\n05:15\n\n\n600000\n1.54544\n1.62922\n06:21\n\n\n700000\n1.68379\n1.59513\n07:21\n\n\n759936\n1.61915\n1.57619\n07:57\n\n\n\n\n\n    \n      \n      100.00% [5937/5937 01:59&lt;00:00 #189984/189984 loss: 1.619 / 1.576]\n    \n    \n\n\n\n/opt/conda/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)"
  },
  {
    "objectID": "B2. Training (Lightning).html",
    "href": "B2. Training (Lightning).html",
    "title": "WhisperSpeech",
    "section": "",
    "text": "def test_fun(a:str=None, to:int = 2, toggle:bool=True):\n    assert(a is not None)\n    print(a, to, toggle)\nparse_and_call(\"test\", test_fun, [\"--to\", \"4\"], dict(a=[]), log_to_wandb=False)\n\n[] 4 False\n\n\n\nfrom fastcore.script import anno_parser\ndef test_fun(a:str=None, to:int = 2, toggle:bool=True):\n    assert(a is not None)\n    print(a, to, toggle)\ntest_fun(\"a\")\nanno_parser(test_fun).parse_args([])\n\na 2 True\n\n\nNamespace(a=None, to=2, toggle=False, pdb=False, xtra=None)\n\n\n\ndef test_fun2(a:str, to:int = 2):\n    assert(a is not None)\n    print(a, to)\n\nparse_and_call(\"test\", test_fun2, [\"qwe\"], log_to_wandb=False)\n\nqwe 2"
  },
  {
    "objectID": "1. acoustic token extraction.html",
    "href": "1. acoustic token extraction.html",
    "title": "Acoustic token extraction",
    "section": "",
    "text": "# unpacked small.tar should go here:\ndatadir = Path('/mnt/')\n# you can download it downloaded from\n# https://github.com/facebookresearch/libri-light/blob/main/data_preparation/README.md\n\n\nsource\n\nload\n\n load (fname, newsr=24000)\n\nLoad an audio file to the GPU and resample to newsr.\n\nsource\n\n\nload_model\n\n load_model ()\n\nLoad the pretrained EnCodec model\n\nsource\n\n\nextract_Atoks\n\n extract_Atoks (model, audio)\n\nExtract EnCodec tokens for the given audio tensor (or file path) using the given model (see load_model).\n\nsource\n\n\nextract_acoustic\n\n extract_acoustic (srcdir:pathlib.Path, outdir:pathlib.Path)\n\nConvert audio files to .encodec files with tensors of tokens\n\n\n\n\nType\nDetails\n\n\n\n\nsrcdir\nPath\nsource dir, should contain *.flac files\n\n\noutdir\nPath\noutput dir, will get the *.encodec files\n\n\n\n\n# process all files for speaker 1401\nmodel = load_model()\nextract_acoustic(model, datadir/'small/1401', datadir/'acoustic-1401')\n\n\n\n\n\n\n    \n      \n      100.00% [131/131 05:38&lt;00:00]\n    \n    \n\n\n\n!du -hs {datadir}/acoustic-1401/\n\n78M /mnt/acoustic-1401/"
  }
]